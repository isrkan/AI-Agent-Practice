{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-based retrieval\n",
    "\n",
    "Information retrieval for AI agents is rarely a simple matter of finding documents that match keywords. Production knowledge bases contain thousands or millions of documents with varying levels of relevance, authority, and freshness. A naive search might return technically matching documents that are outdated, from untrusted sources, or semantically off-target. The challenge is not just finding potential matches, but systematically filtering through multiple dimensions to identify the small subset of documents that truly deserve a place in the agent's context.\n",
    "\n",
    "Query-based retrieval addresses this challenge through multi-stage filtering pipelines that progressively narrow the candidate set using different criteria. Rather than relying on a single similarity score, we combine semantic filters, metadata constraints, recency requirements, source authority ratings, and hybrid search strategies to select documents that are both semantically relevant and contextually appropriate. Each filter removes documents that fail specific quality or relevance criteria, ensuring that only the highest-quality, most pertinent information reaches the agent.\n",
    "\n",
    "In this notebook, we explore how to build sophisticated query-based retrieval systems as a select strategy for context engineering. We will examine how to implement semantic similarity filters that capture meaning beyond keywords, how to apply metadata filters for attributes like document type or category, how to enforce recency requirements for time-sensitive domains, how to score and prioritize based on source authority, and how to combine multiple retrieval strategies through hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by initializing the language model and embedding model that will power our retrieval pipeline. Consistent models ensure reproducible similarity measurements across all filtering stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model for generating responses\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(),\n",
    "    temperature=0  # Set to 0 for more deterministic outputs\n",
    ")\n",
    "\n",
    "# Initialize embedding model for semantic search\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip())\n",
    "\n",
    "print(\"Models initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a rich knowledge base with metadata\n",
    "\n",
    "Effective multi-stage filtering requires rich document metadata that captures various dimensions of relevance and quality. Unlike simple text collections, production knowledge bases should include attributes like document type, publication date, source authority, category tags, and version information. This metadata enables sophisticated filtering that goes beyond semantic similarity to consider recency, trustworthiness and topical alignment.\n",
    "\n",
    "We will create a realistic knowledge base for a technology company's customer support system, with documents spanning product documentation, troubleshooting guides, policy statements and announcements. Each document includes comprehensive metadata that our filtering stages will leverage to make intelligent selection decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created knowledge base with 10 documents\n",
      "\n",
      "Sample document metadata:\n",
      "Content: Laptop Pro X1 overheating: Ensure ventilation is clear. Update thermal management drivers to v2.3.1. Use cooling pad for intensive gaming.\n",
      "Metadata: {'doc_type': 'troubleshooting', 'category': 'laptops', 'source_authority': 5, 'publish_date': '2025-11-26T15:55:39.674713', 'version': '2.3', 'author': 'Technical Support Team'}\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DocumentMetadata:\n",
    "    \"\"\"Rich metadata for knowledge base documents.\"\"\"\n",
    "    doc_type: str  # Type: 'documentation', 'policy', 'troubleshooting', 'announcement'\n",
    "    category: str  # Product category or topic area\n",
    "    source_authority: int  # Authority score 1-5 (5 = official, 1 = community)\n",
    "    publish_date: datetime  # When document was created/updated\n",
    "    version: str  # Document version\n",
    "    author: str  # Document author or team\n",
    "\n",
    "# Create comprehensive knowledge base with rich metadata\n",
    "def create_knowledge_base() -> List[Document]:\n",
    "    \"\"\"Create knowledge base documents with comprehensive metadata.\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects with content and metadata\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    \n",
    "    documents = [\n",
    "        # Recent official documentation\n",
    "        Document(\n",
    "            page_content=\"Laptop Pro X1 overheating: Ensure ventilation is clear. Update thermal management drivers to v2.3.1. Use cooling pad for intensive gaming.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"troubleshooting\",\n",
    "                \"category\": \"laptops\",\n",
    "                \"source_authority\": 5,\n",
    "                \"publish_date\": (now - timedelta(days=10)).isoformat(),\n",
    "                \"version\": \"2.3\",\n",
    "                \"author\": \"Technical Support Team\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Older documentation (outdated)\n",
    "        Document(\n",
    "            page_content=\"Laptop overheating issues: Clean fans and reapply thermal paste. Update to driver v1.8 for better cooling.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"troubleshooting\",\n",
    "                \"category\": \"laptops\",\n",
    "                \"source_authority\": 5,\n",
    "                \"publish_date\": (now - timedelta(days=400)).isoformat(),\n",
    "                \"version\": \"1.8\",\n",
    "                \"author\": \"Technical Support Team\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Community contribution (lower authority)\n",
    "        Document(\n",
    "            page_content=\"My laptop runs hot too. I just put it in the fridge for a few minutes when it overheats. Works great!\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"troubleshooting\",\n",
    "                \"category\": \"laptops\",\n",
    "                \"source_authority\": 1,  # Community post, not official\n",
    "                \"publish_date\": (now - timedelta(days=5)).isoformat(),\n",
    "                \"version\": \"1.0\",\n",
    "                \"author\": \"CommunityUser123\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Official policy documents\n",
    "        Document(\n",
    "            page_content=\"Return Policy 2024: Products may be returned within 30 days of purchase with original packaging. Refunds processed within 5-7 business days.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"policy\",\n",
    "                \"category\": \"returns\",\n",
    "                \"source_authority\": 5,\n",
    "                \"publish_date\": (now - timedelta(days=60)).isoformat(),\n",
    "                \"version\": \"2024.1\",\n",
    "                \"author\": \"Legal Team\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Old policy (superseded)\n",
    "        Document(\n",
    "            page_content=\"Return Policy 2023: Products may be returned within 14 days. Refunds take 10-14 business days.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"policy\",\n",
    "                \"category\": \"returns\",\n",
    "                \"source_authority\": 5,\n",
    "                \"publish_date\": (now - timedelta(days=450)).isoformat(),\n",
    "                \"version\": \"2023.1\",\n",
    "                \"author\": \"Legal Team\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Product documentation\n",
    "        Document(\n",
    "            page_content=\"Laptop Pro X1 specifications: 16GB RAM, 512GB SSD, Intel i7 processor, 15.6 inch display. Battery life up to 10 hours.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"documentation\",\n",
    "                \"category\": \"laptops\",\n",
    "                \"source_authority\": 5,\n",
    "                \"publish_date\": (now - timedelta(days=30)).isoformat(),\n",
    "                \"version\": \"1.0\",\n",
    "                \"author\": \"Product Team\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Shipping information\n",
    "        Document(\n",
    "            page_content=\"Shipping: Standard delivery 5-7 business days, Express 2-3 days. Free shipping on orders over $50.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"policy\",\n",
    "                \"category\": \"shipping\",\n",
    "                \"source_authority\": 4,\n",
    "                \"publish_date\": (now - timedelta(days=90)).isoformat(),\n",
    "                \"version\": \"1.5\",\n",
    "                \"author\": \"Operations Team\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Recent announcement\n",
    "        Document(\n",
    "            page_content=\"NEW: Extended warranty now available for all laptop models. 2-year coverage for $199.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"announcement\",\n",
    "                \"category\": \"laptops\",\n",
    "                \"source_authority\": 5,\n",
    "                \"publish_date\": (now - timedelta(days=3)).isoformat(),\n",
    "                \"version\": \"1.0\",\n",
    "                \"author\": \"Marketing Team\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Tablet troubleshooting (different category)\n",
    "        Document(\n",
    "            page_content=\"Tablet Mini screen issues: Calibrate touchscreen in settings. Reset display settings if unresponsive.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"troubleshooting\",\n",
    "                \"category\": \"tablets\",\n",
    "                \"source_authority\": 5,\n",
    "                \"publish_date\": (now - timedelta(days=20)).isoformat(),\n",
    "                \"version\": \"1.2\",\n",
    "                \"author\": \"Technical Support Team\"\n",
    "            }\n",
    "        ),\n",
    "        \n",
    "        # Battery troubleshooting\n",
    "        Document(\n",
    "            page_content=\"Laptop battery draining fast: Check for background processes. Reduce screen brightness. Update power management drivers.\",\n",
    "            metadata={\n",
    "                \"doc_type\": \"troubleshooting\",\n",
    "                \"category\": \"laptops\",\n",
    "                \"source_authority\": 5,\n",
    "                \"publish_date\": (now - timedelta(days=15)).isoformat(),\n",
    "                \"version\": \"2.1\",\n",
    "                \"author\": \"Technical Support Team\"\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Create knowledge base\n",
    "kb_documents = create_knowledge_base()\n",
    "\n",
    "print(f\"Created knowledge base with {len(kb_documents)} documents\")\n",
    "print(f\"\\nSample document metadata:\")\n",
    "print(f\"Content: {kb_documents[0].page_content}\")\n",
    "print(f\"Metadata: {kb_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our knowledge base now contains documents with diverse characteristics:\n",
    "1. Creates documents with rich metadata including type, category, authority scores, publication dates, versions and authors.\n",
    "2. Includes intentional variety to demonstrate filtering effectivenessâ€”outdated documents, low-authority sources, different categories and varying recency.\n",
    "3. Uses realistic content for a tech support scenario with troubleshooting guides, policies, documentation and announcements.\n",
    "4. Provides the foundation for demonstrating how multi-stage filtering progressively narrows results to the most relevant, trustworthy, current information.\n",
    "\n",
    "This diverse collection makes an ideal testbed for advanced retrieval strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - Semantic similarity filtering\n",
    "\n",
    "The first filtering stage uses semantic similarity to identify documents that are conceptually related to the query. This casts a wide net, retrieving all documents that might be relevant based on meaning rather than exact keyword matches. We typically retrieve more candidates than needed at this stage, knowing that subsequent filters will narrow the results based on other criteria.\n",
    "\n",
    "Semantic filtering forms the foundation of the pipeline by ensuring we start with documents that are at least topically related. Without this initial semantic filter, metadata-based filters would have no way to distinguish between documents that are completely irrelevant versus those that are relevant but perhaps outdated or from lower-authority sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'My laptop is getting too hot when I use it'\n",
      "\n",
      "Stage 1: Semantic Similarity Filter\n",
      "======================================================================\n",
      "Retrieved 5 semantically similar documents:\n",
      "\n",
      "1. [Similarity: 0.2439]\n",
      "   Content: My laptop runs hot too. I just put it in the fridge for a few minutes when it ov...\n",
      "   Type: troubleshooting | Category: laptops\n",
      "   Authority: 1/5 | Published: 2025-12-01\n",
      "\n",
      "2. [Similarity: 0.2789]\n",
      "   Content: Laptop overheating issues: Clean fans and reapply thermal paste. Update to drive...\n",
      "   Type: troubleshooting | Category: laptops\n",
      "   Authority: 5/5 | Published: 2024-11-01\n",
      "\n",
      "3. [Similarity: 0.3006]\n",
      "   Content: Laptop Pro X1 overheating: Ensure ventilation is clear. Update thermal managemen...\n",
      "   Type: troubleshooting | Category: laptops\n",
      "   Authority: 5/5 | Published: 2025-11-26\n",
      "\n",
      "4. [Similarity: 0.3461]\n",
      "   Content: Laptop battery draining fast: Check for background processes. Reduce screen brig...\n",
      "   Type: troubleshooting | Category: laptops\n",
      "   Authority: 5/5 | Published: 2025-11-21\n",
      "\n",
      "5. [Similarity: 0.4145]\n",
      "   Content: Laptop Pro X1 specifications: 16GB RAM, 512GB SSD, Intel i7 processor, 15.6 inch...\n",
      "   Type: documentation | Category: laptops\n",
      "   Authority: 5/5 | Published: 2025-11-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create vector store from knowledge base for semantic search\n",
    "vector_store = FAISS.from_documents(kb_documents, embeddings)\n",
    "\n",
    "def semantic_filter(query: str, \n",
    "                   vector_store: FAISS,\n",
    "                   similarity_threshold: float = 0.5,\n",
    "                   top_k: int = 10) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"Stage 1: Filter by semantic similarity to query.\n",
    "    \n",
    "    Args:\n",
    "        query: User's search query\n",
    "        vector_store: Vector store containing knowledge base\n",
    "        similarity_threshold: Minimum similarity score (0-1)\n",
    "        top_k: Maximum candidates to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of (document, similarity_score) tuples above threshold\n",
    "    \"\"\"\n",
    "    # Retrieve semantically similar documents with scores\n",
    "    results = vector_store.similarity_search_with_score(query, k=top_k)\n",
    "    \n",
    "    # Return all results (threshold filtering can happen in later stages)\n",
    "    return results\n",
    "\n",
    "# Test semantic filtering\n",
    "query = \"My laptop is getting too hot when I use it\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nStage 1: Semantic Similarity Filter\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "semantic_results = semantic_filter(query, vector_store, top_k=5)\n",
    "\n",
    "print(f\"Retrieved {len(semantic_results)} semantically similar documents:\\n\")\n",
    "\n",
    "for i, (doc, score) in enumerate(semantic_results, 1):\n",
    "    print(f\"{i}. [Similarity: {score:.4f}]\")\n",
    "    print(f\"   Content: {doc.page_content[:80]}...\")\n",
    "    print(f\"   Type: {doc.metadata['doc_type']} | Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Authority: {doc.metadata['source_authority']}/5 | Published: {doc.metadata['publish_date'][:10]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic filter successfully identifies relevant documents:\n",
    "1. Implements vector similarity search to retrieve documents semantically related to the query about laptop overheating.\n",
    "2. Returns top candidates with similarity scores, capturing both highly relevant troubleshooting guides and tangentially related content.\n",
    "3. Demonstrates that semantic similarity alone may surface outdated documents, low-authority sources and varying quality content.\n",
    "4. Sets the stage for additional filtering layers that will refine this candidate set based on metadata criteria.\n",
    "\n",
    "Semantic filtering ensures we start with topically relevant documents before applying quality filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - Metadata filtering\n",
    "\n",
    "The second filtering stage applies metadata constraints to narrow the candidate set based on document attributes. This might include filtering by document type to include only troubleshooting guides and exclude announcements, filtering by category to ensure topical alignment, or filtering by other custom attributes that indicate relevance to the current context.\n",
    "\n",
    "Metadata filters are precise and efficient because they operate on structured attributes rather than semantic similarity scores. They allow us to enforce hard requirements - for example, when handling a refund inquiry, we might require documents of type policy, or when troubleshooting a laptop issue, we might exclude documents categorized under tablets. These filters dramatically improve precision by removing semantically similar but contextually inappropriate documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: Metadata Filter\n",
      "======================================================================\n",
      "Applied filters:\n",
      "  - Document types: ['troubleshooting']\n",
      "  - Categories: ['laptops']\n",
      "  - Minimum authority: 4/5\n",
      "\n",
      "Filtered from 5 to 3 documents:\n",
      "\n",
      "1. [Similarity: 0.2789]\n",
      "   Content: Laptop overheating issues: Clean fans and reapply thermal paste. Update to drive...\n",
      "   Type: troubleshooting | Category: laptops\n",
      "   Authority: 5/5\n",
      "\n",
      "2. [Similarity: 0.3006]\n",
      "   Content: Laptop Pro X1 overheating: Ensure ventilation is clear. Update thermal managemen...\n",
      "   Type: troubleshooting | Category: laptops\n",
      "   Authority: 5/5\n",
      "\n",
      "3. [Similarity: 0.3461]\n",
      "   Content: Laptop battery draining fast: Check for background processes. Reduce screen brig...\n",
      "   Type: troubleshooting | Category: laptops\n",
      "   Authority: 5/5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def metadata_filter(candidates: List[Tuple[Document, float]],\n",
    "                   doc_types: Optional[List[str]] = None,\n",
    "                   categories: Optional[List[str]] = None,\n",
    "                   min_authority: Optional[int] = None) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"Stage 2: Filter by document metadata attributes.\n",
    "    \n",
    "    Args:\n",
    "        candidates: Results from semantic filtering\n",
    "        doc_types: Allowed document types (None = all types)\n",
    "        categories: Allowed categories (None = all categories)\n",
    "        min_authority: Minimum source authority score (None = no filter)\n",
    "        \n",
    "    Returns:\n",
    "        Filtered list of (document, score) tuples\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    \n",
    "    for doc, score in candidates:\n",
    "        # Check document type filter\n",
    "        if doc_types and doc.metadata.get('doc_type') not in doc_types:\n",
    "            continue\n",
    "        \n",
    "        # Check category filter\n",
    "        if categories and doc.metadata.get('category') not in categories:\n",
    "            continue\n",
    "        \n",
    "        # Check authority filter\n",
    "        if min_authority and doc.metadata.get('source_authority', 0) < min_authority:\n",
    "            continue\n",
    "        \n",
    "        # Document passes all filters\n",
    "        filtered.append((doc, score))\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Apply metadata filters to semantic results\n",
    "print(f\"Stage 2: Metadata Filter\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Filter for troubleshooting docs in laptops category with high authority\n",
    "metadata_results = metadata_filter(\n",
    "    semantic_results,\n",
    "    doc_types=['troubleshooting'],  # Only troubleshooting guides\n",
    "    categories=['laptops'],  # Only laptop-related\n",
    "    min_authority=4  # Minimum authority of 4/5\n",
    ")\n",
    "\n",
    "print(f\"Applied filters:\")\n",
    "print(f\"  - Document types: ['troubleshooting']\")\n",
    "print(f\"  - Categories: ['laptops']\")\n",
    "print(f\"  - Minimum authority: 4/5\")\n",
    "print(f\"\\nFiltered from {len(semantic_results)} to {len(metadata_results)} documents:\\n\")\n",
    "\n",
    "for i, (doc, score) in enumerate(metadata_results, 1):\n",
    "    print(f\"{i}. [Similarity: {score:.4f}]\")\n",
    "    print(f\"   Content: {doc.page_content[:80]}...\")\n",
    "    print(f\"   Type: {doc.metadata['doc_type']} | Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Authority: {doc.metadata['source_authority']}/5\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata filtering significantly refines the candidate set:\n",
    "1. Implements flexible metadata filtering that can constrain by document type, category and authority level based on use case requirements.\n",
    "2. Applies filters to remove documents that are semantically similar but contextually inappropriate (e.g., announcements when troubleshooting is needed).\n",
    "3. Enforces minimum authority requirements to exclude low-quality community contributions when official guidance is needed.\n",
    "4. Demonstrates how metadata filters work in conjunction with semantic similarity to balance breadth and precision.\n",
    "\n",
    "The combination of semantic and metadata filtering produces focused, high-quality results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - Recency filtering\n",
    "\n",
    "For many domains, information freshness is critical. Technical documentation becomes outdated as products evolve, policies change over time, and troubleshooting procedures improve with new discoveries. The third filtering stage applies recency requirements to ensure we surface current information and suppress superseded content.\n",
    "\n",
    "Recency filtering can be implemented in multiple ways. Hard cutoffs exclude any document older than a specific age, useful when we know information becomes invalid after a certain period. Recency boosting adjusts similarity scores based on document age, allowing recent documents to rank higher without completely excluding older content. The right approach depends on whether outdated information is merely less useful or actively harmful to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3: Recency Filter\n",
      "======================================================================\n",
      "Applied filters:\n",
      "  - Maximum age: 180 days\n",
      "  - Recency boosting: enabled\n",
      "\n",
      "Filtered from 3 to 2 documents:\n",
      "\n",
      "1. [Adjusted Score: 0.6211]\n",
      "   Content: Laptop battery draining fast: Check for background processes. Reduce screen brig...\n",
      "   Published: 2025-11-21 (15 days ago)\n",
      "   Version: 2.1\n",
      "\n",
      "2. [Adjusted Score: 0.5840]\n",
      "   Content: Laptop Pro X1 overheating: Ensure ventilation is clear. Update thermal managemen...\n",
      "   Published: 2025-11-26 (10 days ago)\n",
      "   Version: 2.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def recency_filter(candidates: List[Tuple[Document, float]],\n",
    "                  max_age_days: Optional[int] = None,\n",
    "                  boost_recent: bool = False,\n",
    "                  boost_factor: float = 0.1) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"Stage 3: Filter or boost based on document recency.\n",
    "    \n",
    "    Args:\n",
    "        candidates: Results from previous filtering stages\n",
    "        max_age_days: Maximum document age in days (None = no hard cutoff)\n",
    "        boost_recent: Whether to boost scores of recent documents\n",
    "        boost_factor: Score boost per 30 days of recency (if boosting enabled)\n",
    "        \n",
    "    Returns:\n",
    "        Filtered/boosted list of (document, score) tuples\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    results = []\n",
    "    \n",
    "    for doc, score in candidates:\n",
    "        # Parse publication date\n",
    "        pub_date_str = doc.metadata.get('publish_date', '')\n",
    "        try:\n",
    "            pub_date = datetime.fromisoformat(pub_date_str)\n",
    "        except:\n",
    "            # If date parsing fails, exclude document\n",
    "            continue\n",
    "        \n",
    "        # Calculate age in days\n",
    "        age_days = (now - pub_date).days\n",
    "        \n",
    "        # Apply hard cutoff if specified\n",
    "        if max_age_days and age_days > max_age_days:\n",
    "            continue\n",
    "        \n",
    "        # Apply recency boost if enabled\n",
    "        adjusted_score = score\n",
    "        if boost_recent:\n",
    "            # Boost score based on recency (more recent = higher boost)\n",
    "            recency_boost = (max_age_days - age_days) / 30 * boost_factor if max_age_days else 0\n",
    "            adjusted_score = score + recency_boost\n",
    "        \n",
    "        results.append((doc, adjusted_score))\n",
    "    \n",
    "    # Re-sort by adjusted scores\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(f\"Stage 3: Recency Filter\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Apply recency filter with 180-day maximum age\n",
    "recency_results = recency_filter(\n",
    "    metadata_results,\n",
    "    max_age_days=180,  # Only documents from last 6 months\n",
    "    boost_recent=True,  # Boost recent documents\n",
    "    boost_factor=0.05\n",
    ")\n",
    "\n",
    "print(f\"Applied filters:\")\n",
    "print(f\"  - Maximum age: 180 days\")\n",
    "print(f\"  - Recency boosting: enabled\")\n",
    "print(f\"\\nFiltered from {len(metadata_results)} to {len(recency_results)} documents:\\n\")\n",
    "\n",
    "for i, (doc, score) in enumerate(recency_results, 1):\n",
    "    pub_date = datetime.fromisoformat(doc.metadata['publish_date'])\n",
    "    age_days = (datetime.now() - pub_date).days\n",
    "    \n",
    "    print(f\"{i}. [Adjusted Score: {score:.4f}]\")\n",
    "    print(f\"   Content: {doc.page_content[:80]}...\")\n",
    "    print(f\"   Published: {doc.metadata['publish_date'][:10]} ({age_days} days ago)\")\n",
    "    print(f\"   Version: {doc.metadata['version']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recency filtering ensures we surface current, relevant information:\n",
    "1. Implements both hard cutoff filtering (excluding documents older than specified age) and recency boosting (increasing scores for newer documents).\n",
    "2. Calculates document age from publication date metadata and applies age-based filters and adjustments.\n",
    "3. Removes outdated content that may contain superseded information (older driver versions, previous policies, etc.).\n",
    "4. Re-ranks results to prioritize recent documents, ensuring users see the most current guidance first.\n",
    "\n",
    "For technical support and policy domains, recency filtering is essential for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - Source authority scoring\n",
    "\n",
    "Not all documents are equally trustworthy. Official documentation from product teams should be weighted more heavily than community forum posts, legal policy documents should take precedence over blog summaries, and verified expert content should outrank unverified contributions. The fourth filtering stage uses source authority scores to prioritize high-quality, trustworthy information.\n",
    "\n",
    "Authority scoring can be implemented as hard filtering that excludes low-authority sources entirely, or as score boosting that increases the ranking of high-authority documents while still allowing lower-authority content to appear if it is highly semantically relevant. The approach should match our quality requirements and the diversity of our source base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 4: Source Authority Scoring\n",
      "======================================================================\n",
      "Applied authority-based score boosting\n",
      "  - Boost factor: 0.1 per authority point\n",
      "  - Official sources (5/5) get maximum boost\n",
      "\n",
      "Re-ranked 2 documents by authority:\n",
      "\n",
      "1. [Final Score: 1.0211]\n",
      "   Content: Laptop battery draining fast: Check for background processes. Reduce screen brig...\n",
      "   Authority: 5/5 (Technical Support Team)\n",
      "   Version: 2.1\n",
      "\n",
      "2. [Final Score: 0.9840]\n",
      "   Content: Laptop Pro X1 overheating: Ensure ventilation is clear. Update thermal managemen...\n",
      "   Authority: 5/5 (Technical Support Team)\n",
      "   Version: 2.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def authority_scoring(candidates: List[Tuple[Document, float]],\n",
    "                     boost_factor: float = 0.1) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"Stage 4: Boost scores based on source authority.\n",
    "    \n",
    "    Args:\n",
    "        candidates: Results from previous filtering stages\n",
    "        boost_factor: Score boost per authority point\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list of (document, score) tuples\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for doc, score in candidates:\n",
    "        # Get authority score (1-5)\n",
    "        authority = doc.metadata.get('source_authority', 1)\n",
    "        \n",
    "        # Boost score based on authority\n",
    "        # Authority 5 gets highest boost, authority 1 gets minimal boost\n",
    "        authority_boost = (authority - 1) * boost_factor\n",
    "        adjusted_score = score + authority_boost\n",
    "        \n",
    "        results.append((doc, adjusted_score))\n",
    "    \n",
    "    # Re-sort by adjusted scores\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(f\"Stage 4: Source Authority Scoring\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Apply authority-based boosting\n",
    "authority_results = authority_scoring(\n",
    "    recency_results,\n",
    "    boost_factor=0.1  # Boost score by 0.1 per authority point\n",
    ")\n",
    "\n",
    "print(f\"Applied authority-based score boosting\")\n",
    "print(f\"  - Boost factor: 0.1 per authority point\")\n",
    "print(f\"  - Official sources (5/5) get maximum boost\")\n",
    "print(f\"\\nRe-ranked {len(authority_results)} documents by authority:\\n\")\n",
    "\n",
    "for i, (doc, score) in enumerate(authority_results, 1):\n",
    "    authority = doc.metadata['source_authority']\n",
    "    \n",
    "    print(f\"{i}. [Final Score: {score:.4f}]\")\n",
    "    print(f\"   Content: {doc.page_content[:80]}...\")\n",
    "    print(f\"   Authority: {authority}/5 ({doc.metadata['author']})\")\n",
    "    print(f\"   Version: {doc.metadata['version']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authority scoring ensures trustworthy sources rise to the top:\n",
    "1. Implements score boosting based on source authority ratings, rewarding official documentation and penalizing low-quality sources.\n",
    "2. Applies graduated boosting where maximum-authority sources (5/5) receive the highest adjustment while minimum-authority sources (1/5) receive minimal or no boost.\n",
    "3. Re-ranks the filtered result set so that among similarly relevant documents, higher-authority sources appear first.\n",
    "4. Demonstrates that authority scoring works in concert with semantic similarity - both relevance and trustworthiness contribute to final ranking.\n",
    "\n",
    "This ensures agents rely on authoritative information rather than questionable sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete multi-stage retrieval pipeline\n",
    "\n",
    "Now we bring together all filtering stages into a unified pipeline that processes queries through semantic similarity, metadata constraints, recency requirements and authority scoring in sequence. This production-ready system demonstrates how multiple filtering dimensions combine to produce highly relevant, trustworthy, current results from large, diverse knowledge bases.\n",
    "\n",
    "The multi-stage approach is powerful because each filter addresses a different dimension of quality and relevance. Semantic similarity ensures topical relevance, metadata filters enforce contextual appropriateness, recency filters maintain currency, and authority scoring prioritizes trustworthiness. Together, they create a robust selection mechanism that consistently surfaces the best available information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Stage Retrieval Pipeline - Complete Examples\n",
      "======================================================================\n",
      "\n",
      "Query: 'My laptop overheats when gaming'\n",
      "\n",
      "Pipeline progression:\n",
      "  Stage 1 (Semantic): 10 candidates\n",
      "  Stage 2 (Metadata): 3 candidates\n",
      "  Stage 3 (Recency):  2 candidates\n",
      "  Stage 4 (Authority): 2 candidates\n",
      "  Final results: 2 documents\n",
      "\n",
      "Top 2 results:\n",
      "\n",
      "  1. [Score: 1.0424]\n",
      "     Laptop battery draining fast: Check for background processes. Reduce screen brightness. Update power management drivers.\n",
      "     Type: troubleshooting | Authority: 5/5\n",
      "     Published: 2025-11-21 | Version: 2.1\n",
      "\n",
      "  2. [Score: 0.9380]\n",
      "     Laptop Pro X1 overheating: Ensure ventilation is clear. Update thermal management drivers to v2.3.1. Use cooling pad for intensive gaming.\n",
      "     Type: troubleshooting | Authority: 5/5\n",
      "     Published: 2025-11-26 | Version: 2.3\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Query: 'What is your return policy?'\n",
      "\n",
      "Pipeline progression:\n",
      "  Stage 1 (Semantic): 10 candidates\n",
      "  Stage 2 (Metadata): 2 candidates\n",
      "  Stage 3 (Recency):  1 candidates\n",
      "  Stage 4 (Authority): 1 candidates\n",
      "  Final results: 1 documents\n",
      "\n",
      "Top 1 results:\n",
      "\n",
      "  1. [Score: 1.2000]\n",
      "     Return Policy 2024: Products may be returned within 30 days of purchase with original packaging. Refunds processed within 5-7 business days.\n",
      "     Type: policy | Authority: 5/5\n",
      "     Published: 2025-10-07 | Version: 2024.1\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class MultiStageRetriever:\n",
    "    \"\"\"Production multi-stage retrieval pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: FAISS):\n",
    "        \"\"\"Initialize retriever with vector store.\n",
    "        \n",
    "        Args:\n",
    "            vector_store: FAISS vector store containing knowledge base\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "    \n",
    "    def retrieve(self,\n",
    "                query: str,\n",
    "                doc_types: Optional[List[str]] = None,\n",
    "                categories: Optional[List[str]] = None,\n",
    "                min_authority: int = 3,\n",
    "                max_age_days: int = 365,\n",
    "                top_k: int = 3) -> Dict:\n",
    "        \"\"\"Execute multi-stage retrieval pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query: User's search query\n",
    "            doc_types: Allowed document types\n",
    "            categories: Allowed categories\n",
    "            min_authority: Minimum source authority (1-5)\n",
    "            max_age_days: Maximum document age in days\n",
    "            top_k: Number of final results to return\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing results and pipeline metadata\n",
    "        \"\"\"\n",
    "        # Stage 1: Semantic similarity\n",
    "        stage1_results = semantic_filter(query, self.vector_store, top_k=10)\n",
    "        \n",
    "        # Stage 2: Metadata filtering\n",
    "        stage2_results = metadata_filter(\n",
    "            stage1_results,\n",
    "            doc_types=doc_types,\n",
    "            categories=categories,\n",
    "            min_authority=min_authority\n",
    "        )\n",
    "        \n",
    "        # Stage 3: Recency filtering and boosting\n",
    "        stage3_results = recency_filter(\n",
    "            stage2_results,\n",
    "            max_age_days=max_age_days,\n",
    "            boost_recent=True,\n",
    "            boost_factor=0.05\n",
    "        )\n",
    "        \n",
    "        # Stage 4: Authority scoring\n",
    "        stage4_results = authority_scoring(\n",
    "            stage3_results,\n",
    "            boost_factor=0.1\n",
    "        )\n",
    "        \n",
    "        # Take top K results\n",
    "        final_results = stage4_results[:top_k]\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"results\": final_results,\n",
    "            \"pipeline_stats\": {\n",
    "                \"stage1_semantic\": len(stage1_results),\n",
    "                \"stage2_metadata\": len(stage2_results),\n",
    "                \"stage3_recency\": len(stage3_results),\n",
    "                \"stage4_authority\": len(stage4_results),\n",
    "                \"final_count\": len(final_results)\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create retriever and test with multiple queries\n",
    "retriever = MultiStageRetriever(vector_store)\n",
    "\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"My laptop overheats when gaming\",\n",
    "        \"doc_types\": [\"troubleshooting\"],\n",
    "        \"categories\": [\"laptops\"],\n",
    "        \"min_authority\": 4,\n",
    "        \"max_age_days\": 180\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is your return policy?\",\n",
    "        \"doc_types\": [\"policy\"],\n",
    "        \"categories\": [\"returns\"],\n",
    "        \"min_authority\": 5,\n",
    "        \"max_age_days\": 365\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Multi-Stage Retrieval Pipeline - Complete Examples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for test_case in test_queries:\n",
    "    result = retriever.retrieve(**test_case)\n",
    "    \n",
    "    print(f\"\\nQuery: '{result['query']}'\")\n",
    "    print(f\"\\nPipeline progression:\")\n",
    "    stats = result['pipeline_stats']\n",
    "    print(f\"  Stage 1 (Semantic): {stats['stage1_semantic']} candidates\")\n",
    "    print(f\"  Stage 2 (Metadata): {stats['stage2_metadata']} candidates\")\n",
    "    print(f\"  Stage 3 (Recency):  {stats['stage3_recency']} candidates\")\n",
    "    print(f\"  Stage 4 (Authority): {stats['stage4_authority']} candidates\")\n",
    "    print(f\"  Final results: {stats['final_count']} documents\")\n",
    "    \n",
    "    print(f\"\\nTop {len(result['results'])} results:\")\n",
    "    for i, (doc, score) in enumerate(result['results'], 1):\n",
    "        print(f\"\\n  {i}. [Score: {score:.4f}]\")\n",
    "        print(f\"     {doc.page_content}\")\n",
    "        print(f\"     Type: {doc.metadata['doc_type']} | Authority: {doc.metadata['source_authority']}/5\")\n",
    "        print(f\"     Published: {doc.metadata['publish_date'][:10]} | Version: {doc.metadata['version']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete pipeline demonstrates production-quality retrieval:\n",
    "1. Implements a unified retriever class that orchestrates all four filtering stages in sequence, providing a clean interface for complex retrieval.\n",
    "2. Tracks pipeline statistics showing how many candidates survive each stage, providing visibility into filtering effectiveness.\n",
    "3. Tests realistic scenarios with different filtering requirements, demonstrating flexibility to handle diverse query types.\n",
    "4. Returns only the highest-quality, most relevant documents after progressive refinement through multiple quality dimensions.\n",
    "\n",
    "This architecture scales to large knowledge bases while maintaining precision and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
