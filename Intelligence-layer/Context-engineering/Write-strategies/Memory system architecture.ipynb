{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Memory system architecture\n",
    "\n",
    "AI agents that learn from interactions and maintain context across conversations need memory systems. Without memory, every interaction starts from scratch - agents cannot remember user preferences, recall past conversations or build on previous exchanges. With well-designed memory, agents provide personalized experiences, reference historical context and improve over time. However, poorly designed memory systems create their own challenges: unbounded growth that eventually overwhelms context windows, difficulty surfacing relevant memories when needed, and storage of trivial information that dilutes signal from important facts.\n",
    "\n",
    "Memory system architecture is about building sustainable, efficient systems for storing, organizing and retrieving information across different timescales. This involves understanding the types of memory—episodic versus semantic, short-term versus long-term—and applying appropriate strategies for each. It requires structured storage that enables efficient querying, semantic tagging that supports similarity-based retrieval, importance scoring that prioritizes valuable information, and consolidation rules that compress memories as they age. The goal is preserving the signal that matters while discarding or compressing noise.\n",
    "\n",
    "In this notebook, we explore systematic techniques for designing memory systems that scale effectively and retrieve intelligently. We will examine different memory types and their uses, structured storage formats that enable querying, semantic tagging and metadata systems, importance and relevance scoring, consolidation rules for memory compression, and full lifecycle management from storage through retrieval to eventual discard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### Initialize the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(),\n",
    "    temperature=0  # Set to 0 for more deterministic outputs\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 1: Memory types and their purposes\n",
    "\n",
    "Not all information should be stored and retrieved the same way. A fact about user preferences requires different handling than a specific conversation turn, and information needed for the current session requires different management than facts that persist across months. Understanding memory types and their appropriate uses is the foundation of effective memory system architecture.\n",
    "\n",
    "Memory classification happens along multiple dimensions: timespan distinguishes short-term session memory from long-term persistent memory, content type separates episodic events from semantic facts, and scope differentiates single-session data from cross-session information. Each combination of these dimensions calls for different storage strategies, retrieval approaches and lifecycle management. Choosing the right memory type for each piece of information determines whether our system can find what it needs when it needs it.\n",
    "\n",
    "When building memory systems for AI agents, we need to think about how human memory works. Our brains distinguish between remembering specific events (like \"I had coffee this morning\") versus general knowledge (like \"I prefer coffee over tea\"). Similarly, we separate what we need to remember right now in a conversation from facts we carry with us for years. This natural categorization translates directly into AI agent memory architecture. Short-term episodic memory handles conversation flow and context - those references to \"it\" and \"that\" which need immediate context. Short-term semantic memory holds session-specific facts like a current shopping budget. Long-term episodic memory preserves the timeline of interactions, while long-term semantic memory stores enduring preferences and user profiles. Each type serves a distinct purpose, and mixing them inappropriately leads to either losing critical context or drowning in irrelevant details.\n",
    "\n",
    "### Defining memory type categories\n",
    "Let's start by defining the four fundamental memory types that our system will use. These categories form the backbone of our memory architecture and determine how we handle different kinds of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c455b89-fddf-4b7f-9a1b-78fca8c46b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryType(Enum):\n",
    "    \"\"\"Types of memory in AI agent systems.\"\"\"\n",
    "    SHORT_TERM_EPISODIC = \"short_term_episodic\"  # Memory that holds recent conversation turns within the current session - Cleared when the session ends\n",
    "    SHORT_TERM_SEMANTIC = \"short_term_semantic\"  # Facts and information extracted from the current session - Cleared when the session ends\n",
    "    LONG_TERM_EPISODIC = \"long_term_episodic\"    # Historical conversation events that persist across sessions - Stored permanently for context and history\n",
    "    LONG_TERM_SEMANTIC = \"long_term_semantic\"    # Learned facts, preferences, and user profile information - Stored permanently and used for personalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c9ab8-b3bf-4786-8850-4249c583fa13",
   "metadata": {},
   "source": [
    "- Enum class: Python's `Enum` provides type-safe memory classification, preventing invalid memory types from being used.\n",
    "\n",
    "Now let's create concrete examples for each memory type to understand what kind of information belongs in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Types and Examples:\n",
      "============================================================\n",
      "\n",
      "SHORT TERM EPISODIC:\n",
      "  • User asked about return policy 2 messages ago\n",
      "  • Agent suggested product X in the last response\n",
      "  • Current conversation is about laptop purchases\n",
      "\n",
      "SHORT TERM SEMANTIC:\n",
      "  • User's budget is $1500\n",
      "  • User prefers Windows over Mac\n",
      "  • User needs laptop for video editing\n",
      "\n",
      "LONG TERM EPISODIC:\n",
      "  • User purchased Laptop Pro X1 on Jan 15, 2024\n",
      "  • User contacted support about shipping delay on Jan 20\n",
      "  • User left 5-star review on Jan 25\n",
      "\n",
      "LONG TERM SEMANTIC:\n",
      "  • User prefers email over phone contact\n",
      "  • User is a software developer\n",
      "  • User lives in California (PST timezone)\n"
     ]
    }
   ],
   "source": [
    "# Create examples for each memory type to illustrate their use cases\n",
    "memory_examples = {\n",
    "    # Short-term episodic: Recent conversation turns and contextual references\n",
    "    MemoryType.SHORT_TERM_EPISODIC: [\n",
    "        \"User asked about return policy 2 messages ago\",\n",
    "        \"Agent suggested product X in the last response\",\n",
    "        \"Current conversation is about laptop purchases\"\n",
    "    ],\n",
    "    # Short-term semantic: Session-specific facts and temporary information\n",
    "    MemoryType.SHORT_TERM_SEMANTIC: [\n",
    "        \"User's budget is $1500\",\n",
    "        \"User prefers Windows over Mac\",\n",
    "        \"User needs laptop for video editing\"\n",
    "    ],\n",
    "    # Long-term episodic: Historical events and interaction timeline\n",
    "    MemoryType.LONG_TERM_EPISODIC: [\n",
    "        \"User purchased Laptop Pro X1 on Jan 15, 2024\",\n",
    "        \"User contacted support about shipping delay on Jan 20\",\n",
    "        \"User left 5-star review on Jan 25\"\n",
    "    ],\n",
    "    # Long-term semantic: Persistent user profile and learned preferences\n",
    "    MemoryType.LONG_TERM_SEMANTIC: [\n",
    "        \"User prefers email over phone contact\",\n",
    "        \"User is a software developer\",\n",
    "        \"User lives in California (PST timezone)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the examples in a readable format\n",
    "print(\"Memory Types and Examples:\")\n",
    "print(\"=\"*60)\n",
    "# Iterate through each memory type and display its examples\n",
    "for mem_type, examples in memory_examples.items():\n",
    "    # Format the memory type name for display\n",
    "    print(f\"\\n{mem_type.value.upper().replace('_', ' ')}:\")\n",
    "    for example in examples:\n",
    "        print(f\"  • {example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "- The dictionary structure makes it easy to extend with more examples or modify existing ones.\n",
    "\n",
    "Key Differences:\n",
    "- Short-term: Cleared after session ends\n",
    "- Long-term: Persists across sessions\n",
    "- Episodic: Specific events with context\n",
    "- Semantic: General facts without temporal context\n",
    "  \n",
    "### When to use each memory type\n",
    "Knowing the memory types is only half the battle - the real skill lies in deciding which type to use for each piece of information. When a user says \"I prefer email contact,\" should that go into short-term semantic memory for this session, or long-term semantic memory to persist across all future interactions? When they mention \"I bought a laptop last month,\" is that a long-term episodic event to record, or just conversational noise to ignore? The decision framework for memory type selection depends on answering a few key questions: Is this information session-specific or should it persist? Is it a specific event or a general fact? How important is it for future personalization? Let's build a decision matrix that maps common use cases to appropriate memory types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Type Selection Guide:\n",
      "============================================================\n",
      "\n",
      "Maintaining conversation flow:\n",
      "  → Use short_term_episodic\n",
      "\n",
      "Tracking user preferences in session:\n",
      "  → Use short_term_semantic\n",
      "\n",
      "Referencing past purchases:\n",
      "  → Use long_term_episodic\n",
      "\n",
      "Personalizing future interactions:\n",
      "  → Use long_term_semantic\n",
      "\n",
      "Understanding context of 'it' or 'that':\n",
      "  → Use short_term_episodic\n",
      "\n",
      "Remembering customer communication preferences:\n",
      "  → Use long_term_semantic\n"
     ]
    }
   ],
   "source": [
    "# Decision matrix for memory type selection - This helps determine which memory type to use in different scenarios\n",
    "memory_use_cases = {\n",
    "    \"Maintaining conversation flow\": MemoryType.SHORT_TERM_EPISODIC,  # Conversation flow requires short-term episodic for immediate context\n",
    "    \"Tracking user preferences in session\": MemoryType.SHORT_TERM_SEMANTIC,  # Session preferences are short-term semantic (cleared after session)\n",
    "    \"Referencing past purchases\": MemoryType.LONG_TERM_EPISODIC,  # Historical transactions are long-term episodic events\n",
    "    \"Personalizing future interactions\": MemoryType.LONG_TERM_SEMANTIC,  # Persistent preferences are long-term semantic facts\n",
    "    \"Understanding context of 'it' or 'that'\": MemoryType.SHORT_TERM_EPISODIC,  # Pronoun resolution needs recent conversation context\n",
    "    \"Remembering customer communication preferences\": MemoryType.LONG_TERM_SEMANTIC,  # Communication preferences persist across all sessions\n",
    "}\n",
    "\n",
    "print(\"Memory Type Selection Guide:\")\n",
    "print(\"=\"*60)\n",
    "for use_case, mem_type in memory_use_cases.items():\n",
    "    print(f\"\\n{use_case}:\")\n",
    "    print(f\"  → Use {mem_type.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "This creates a decision matrix that maps common agent use cases to their appropriate memory types.\n",
    "- Dictionary structure: Keys are use case descriptions, values are the recommended `MemoryType`.\n",
    "- Categorization logic: Each use case is analyzed for persistence needs (short vs long-term) and information type (episodic vs semantic).\n",
    "\n",
    "The dictionary acts as a lookup table for memory type selection during runtime. This pattern can be extended to include scoring logic or priority weighting for cases where multiple memory types might apply. In production systems, this matrix could be used by an automated classifier that determines memory type based on content analysis. The simple key-value structure makes it easy to add new use cases or modify existing mappings without changing code logic.\n",
    "\n",
    "## Part 2: Structured storage formats\n",
    "Memory must be stored in structured formats that facilitate efficient retrieval and updates. The difference between effective and ineffective memory systems often comes down to how the information is stored. Structured storage means treating each memory as a distinct entity with properties rather than cramming everything into freeform text. Each memory needs an identifier for updates, a timestamp for recency tracking, importance scores for prioritization, tags for categorization and metadata for rich filtering. This structure enables operations that would be impossible with unstructured storage: \"retrieve all memories about laptops from the last week with importance above 0.8\" becomes a simple query rather than text parsing nightmare. Let's see the stark difference between these approaches.\n",
    "\n",
    "### Bad approach: Unstructured text storage\n",
    "Let's first look at what happens when we store memory as unstructured text. This approach might seem simple initially, but quickly becomes unmaintainable as the system scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Unstructured memory storage:\n",
      "\n",
      "User asked about laptops. They want something for video editing. Budget is around $1500.\n",
      "They previously bought a tablet from us. They prefer email contact. They mentioned they're\n",
      "a software developer. They asked about the return policy. They live in California.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of unstructured memory storage - everything in one text blob\n",
    "# This demonstrates the problematic approach where all memories are concatenated as plain text\n",
    "unstructured_memory = \"\"\"\n",
    "User asked about laptops. They want something for video editing. Budget is around $1500.\n",
    "They previously bought a tablet from us. They prefer email contact. They mentioned they're\n",
    "a software developer. They asked about the return policy. They live in California.\n",
    "\"\"\"\n",
    "\n",
    "print(\"❌ Unstructured memory storage:\")\n",
    "print(unstructured_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "Problems:\n",
    "- Can't easily update specific facts.\n",
    "- Hard to search for specific information.\n",
    "- No way to prioritize important vs trivial facts.\n",
    "- Can't filter by recency or relevance.\n",
    "- Difficult to remove outdated information.\n",
    "\n",
    "### Good approach: Structured memory with schemas\n",
    "The solution is to define a structured schema for memory entries using Pydantic models. Each memory becomes a first-class object with well-defined fields: a unique identifier, the memory type classification, the actual content, timestamp for temporal tracking, importance score for prioritization, tags for categorization and arbitrary metadata for extensibility. This structure transforms memory from opaque text into queryable, updateable data. Let's first define our memory entry schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9518955-79a8-4182-acd4-c22fae02b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEntry(BaseModel):\n",
    "    \"\"\"Structured memory entry with metadata and validation.\"\"\"\n",
    "    \n",
    "    id: str  # Unique identifier for this memory entry, used for updates and retrieval\n",
    "    memory_type: MemoryType  # Type of memory (short/long-term, episodic/semantic)\n",
    "    content: str  # The actual content/text of the memory\n",
    "    timestamp: datetime  # When this memory was created or last updated\n",
    "    importance: float = Field(ge=0.0, le=1.0, description=\"Importance score 0-1\")  # Importance score between 0.0 (trivial) and 1.0 (critical)\n",
    "    tags: List[str] = Field(default_factory=list)  # List of tags for categorization and retrieval\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)  # Arbitrary metadata for additional context - Can store source, category, verification status, etc.\n",
    "    \n",
    "    class ConfigDict:\n",
    "        # Convert enum values to their string representation when serializing\n",
    "        use_enum_values = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d4478-864e-4802-8dfd-a453231b720a",
   "metadata": {},
   "source": [
    "The schema makes memories database-ready - this structure could map directly to database columns or document fields.\n",
    "- Pydantic BaseModel: Provides automatic validation, type checking, and serialization for memory entries.\n",
    "- Field constraints: The importance field uses Pydantic's `Field` with `ge` (greater-than-or-equal) and `le` (less-than-or-equal) validators to enforce 0.0-1.0 range.\n",
    "- Default factories: `default_factory=list` and `default_factory=dict` ensure each instance gets its own mutable default values.\n",
    "- The `ConfigDict` class with `use_enum_values=True` automatically converts MemoryType enums to strings during serialization (useful for JSON export).\n",
    "\n",
    "Now let's create some structured memory entries using this schema. We will create memories representing the same information from the unstructured example, but this time with proper structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Structured memory entries:\n",
      "============================================================\n",
      "\n",
      "ID: mem_001\n",
      "Type: MemoryType.SHORT_TERM_SEMANTIC\n",
      "Content: User's budget is $1500 for laptop purchase\n",
      "Importance: 0.9\n",
      "Tags: budget, purchase, laptop\n",
      "Metadata: {'category': 'preference', 'context': 'current_session'}\n",
      "\n",
      "ID: mem_002\n",
      "Type: MemoryType.LONG_TERM_SEMANTIC\n",
      "Content: User prefers email contact over phone\n",
      "Importance: 0.7\n",
      "Tags: communication, preference\n",
      "Metadata: {'category': 'user_preference', 'verified': True}\n",
      "\n",
      "ID: mem_003\n",
      "Type: MemoryType.LONG_TERM_EPISODIC\n",
      "Content: User purchased Tablet Mini on 2024-01-15 for $499\n",
      "Importance: 0.8\n",
      "Tags: purchase, tablet, transaction\n",
      "Metadata: {'order_id': 'ORD-12345', 'amount': 499.0}\n"
     ]
    }
   ],
   "source": [
    "# Create structured memory entries with all required fields\n",
    "structured_memories = [\n",
    "    MemoryEntry(\n",
    "        id=\"mem_001\",\n",
    "        memory_type=MemoryType.SHORT_TERM_SEMANTIC,\n",
    "        content=\"User's budget is $1500 for laptop purchase\",\n",
    "        timestamp=datetime.now(),\n",
    "        importance=0.9,\n",
    "        tags=[\"budget\", \"purchase\", \"laptop\"],\n",
    "        metadata={\"category\": \"preference\", \"context\": \"current_session\"}\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_002\",\n",
    "        memory_type=MemoryType.LONG_TERM_SEMANTIC,\n",
    "        content=\"User prefers email contact over phone\",\n",
    "        timestamp=datetime.now() - timedelta(days=30),\n",
    "        importance=0.7,\n",
    "        tags=[\"communication\", \"preference\"],\n",
    "        metadata={\"category\": \"user_preference\", \"verified\": True}\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_003\",\n",
    "        memory_type=MemoryType.LONG_TERM_EPISODIC,\n",
    "        content=\"User purchased Tablet Mini on 2024-01-15 for $499\",\n",
    "        timestamp=datetime(2024, 1, 15),\n",
    "        importance=0.8,\n",
    "        tags=[\"purchase\", \"tablet\", \"transaction\"],\n",
    "        metadata={\"order_id\": \"ORD-12345\", \"amount\": 499.00}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"✅ Structured memory entries:\")\n",
    "print(\"=\"*60)\n",
    "for mem in structured_memories:\n",
    "    print(f\"\\nID: {mem.id}\")\n",
    "    print(f\"Type: {mem.memory_type}\")\n",
    "    print(f\"Content: {mem.content}\")\n",
    "    print(f\"Importance: {mem.importance}\")\n",
    "    print(f\"Tags: {', '.join(mem.tags)}\")\n",
    "    print(f\"Metadata: {mem.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "Benefits:\n",
    "- Easy to query by type, tags, or importance\n",
    "- Can update individual entries\n",
    "- Metadata enables rich filtering\n",
    "- Importance scores for prioritization\n",
    "\n",
    "Compared to the unstructured approach, we can now: update specific memories by ID, query by importance threshold, filter by tags, sort by timestamp and aggregate by metadata categories.\n",
    "\n",
    "## Part 3: Semantic tagging and metadata systems\n",
    "Tags and metadata enable efficient retrieval and organization of memories. Having structured memory entries is valuable, but without effective tagging and indexing, retrieval still requires scanning through every single memory. Imagine having thousands of structured memories but needing to iterate through all of them to find those related to \"laptop purchases\" - the structure helps, but we are still doing linear search. Semantic tagging solves this by creating an inverted index: instead of going from memory to tags, we go from tags to memories.\n",
    "\n",
    "Good tagging systems operate on multiple levels. Primary tags capture the main topic - \"laptop\", \"budget\", \"purchase\". Secondary tags might capture sentiment, context or relationships. Metadata provides a catch-all for additional structure that doesn't fit neatly into tags: verification status, source system, related entities, confidence scores. Together, tags and metadata transform memory retrieval from \"find a needle in a haystack\" to \"open the drawer labeled 'needles'.\" Let's build a memory store with tag-based indexing.\n",
    "\n",
    "### Building a memory store with semantic indexing\n",
    "We will create a `MemoryStore` class that maintains both the memories themselves and inverted indices for fast tag-based lookup. Let's start by defining the class structure and initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b56aa80b-2e26-489b-be0d-2debc7b96db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryStore:\n",
    "    \"\"\"In-memory storage with semantic tagging.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memories: List[MemoryEntry] = []  # Main storage: list of all memory entries\n",
    "        # Inverted index: maps each tag to list of memory IDs that have that tag\n",
    "        self.tag_index: Dict[str, List[str]] = {}  # tag -> [memory_ids]\n",
    "    \n",
    "    def add_memory(self, memory: MemoryEntry):\n",
    "        \"\"\"Add memory and update tag index.\"\"\"\n",
    "        # Store the memory in our main list\n",
    "        self.memories.append(memory)\n",
    "        \n",
    "        # Update the inverted index for each tag - This allows fast retrieval by tag without scanning all memories\n",
    "        for tag in memory.tags:\n",
    "            # Create a new list for this tag if it doesn't exist\n",
    "            if tag not in self.tag_index:\n",
    "                self.tag_index[tag] = []\n",
    "             # Add this memory's ID to the tag's index\n",
    "            self.tag_index[tag].append(memory.id)\n",
    "    \n",
    "    def get_by_tags(self, tags: List[str], match_all: bool = False) -> List[MemoryEntry]:\n",
    "        \"\"\"Retrieve memories by tags.\"\"\"\n",
    "        if match_all:\n",
    "            # AND logic: memory must have ALL specified tags\n",
    "            # Start with memories that have the first tag\n",
    "            matching_ids = set(self.tag_index.get(tags[0], []))\n",
    "            # Intersect with memories that have each subsequent tag\n",
    "            for tag in tags[1:]:\n",
    "                matching_ids &= set(self.tag_index.get(tag, []))\n",
    "        else:\n",
    "            # OR logic: memory must have AT LEAST ONE tag\n",
    "            matching_ids = set()\n",
    "            # Union all memory IDs across all specified tags\n",
    "            for tag in tags:\n",
    "                matching_ids |= set(self.tag_index.get(tag, []))\n",
    "\n",
    "        # Convert memory IDs back to actual memory objects\n",
    "        return [m for m in self.memories if m.id in matching_ids]\n",
    "    \n",
    "    def get_by_type(self, memory_type: MemoryType) -> List[MemoryEntry]:\n",
    "        \"\"\"Retrieve memories by type.\"\"\"\n",
    "        # Filter memories by their type attribute\n",
    "        return [m for m in self.memories if m.memory_type == memory_type]\n",
    "    \n",
    "    def get_by_importance(self, min_importance: float) -> List[MemoryEntry]:\n",
    "        \"\"\"Retrieve high-importance memories.\"\"\"\n",
    "        # Filter memories with importance >= threshold\n",
    "        high_importance = [m for m in self.memories if m.importance >= min_importance]\n",
    "        \n",
    "        # Sort by importance in descending order (most important first)\n",
    "        return sorted(high_importance, key=lambda x: x.importance, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57815a0-d4e6-4cc2-b25a-9eb4f8faf41a",
   "metadata": {},
   "source": [
    "Now let's test our MemoryStore by adding the structured memories we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7eea5d-04ae-467e-b351-608c45c7f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 memories to the store\n",
      "Tag index now contains 7 unique tags\n"
     ]
    }
   ],
   "source": [
    "# Create a new memory store instance\n",
    "store = MemoryStore()\n",
    "\n",
    "# Add our previously created structured memories to the store\n",
    "for mem in structured_memories:\n",
    "    store.add_memory(mem)\n",
    "\n",
    "print(f\"Added {len(structured_memories)} memories to the store\")\n",
    "print(f\"Tag index now contains {len(store.tag_index)} unique tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa69bfc-e71d-4c09-bb3a-12cc2744052b",
   "metadata": {},
   "source": [
    "Let's add a few more memories to make our testing more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24dee8c1-ccde-4804-baa0-7e0af72b2668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memories in store: 5\n",
      "Total unique tags: 11\n",
      "Tags: budget, communication, laptop, occupation, preference, profile, purchase, requirement, tablet, transaction, video_editing\n"
     ]
    }
   ],
   "source": [
    "# Create additional memories with different tags and attributes\n",
    "additional_memories = [\n",
    "    MemoryEntry(\n",
    "        id=\"mem_004\",\n",
    "        memory_type=MemoryType.SHORT_TERM_SEMANTIC,\n",
    "        content=\"User needs laptop for video editing\",\n",
    "        timestamp=datetime.now(),\n",
    "        importance=0.95,\n",
    "        tags=[\"requirement\", \"laptop\", \"video_editing\"],\n",
    "        metadata={\"category\": \"use_case\"}\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_005\",\n",
    "        memory_type=MemoryType.LONG_TERM_SEMANTIC,\n",
    "        content=\"User is a software developer\",\n",
    "        timestamp=datetime.now() - timedelta(days=60),\n",
    "        importance=0.6,\n",
    "        tags=[\"occupation\", \"profile\"],\n",
    "        metadata={\"category\": \"user_profile\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Add the new memories to our store\n",
    "for mem in additional_memories:\n",
    "    store.add_memory(mem)\n",
    "\n",
    "print(f\"Total memories in store: {len(store.memories)}\")\n",
    "print(f\"Total unique tags: {len(store.tag_index)}\")\n",
    "print(f\"Tags: {', '.join(sorted(store.tag_index.keys()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a46d8-35a4-45bd-a2d4-399690d44e22",
   "metadata": {},
   "source": [
    "### Testing tag-based retrieval\n",
    "Now let's test our tag-based retrieval system to see how efficiently we can find memories by their tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1236844-fca9-4215-8bfc-cc7b97af4f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval by tags:\n",
      "============================================================\n",
      "\n",
      "Memories tagged 'laptop' (2):\n",
      "  • User's budget is $1500 for laptop purchase\n",
      "  • User needs laptop for video editing\n",
      "\n",
      "Memories tagged 'purchase' (2):\n",
      "  • User's budget is $1500 for laptop purchase\n",
      "  • User purchased Tablet Mini on 2024-01-15 for $499\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval by tags\n",
    "print(\"Retrieval by tags:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find all memories tagged with \"laptop\"\n",
    "laptop_memories = store.get_by_tags([\"laptop\"])\n",
    "print(f\"\\nMemories tagged 'laptop' ({len(laptop_memories)}):\")\n",
    "for mem in laptop_memories:\n",
    "    print(f\"  • {mem.content}\")\n",
    "\n",
    "# Find all memories tagged with \"purchase\"\n",
    "purchase_memories = store.get_by_tags([\"purchase\"])\n",
    "print(f\"\\nMemories tagged 'purchase' ({len(purchase_memories)}):\")\n",
    "for mem in purchase_memories:\n",
    "    print(f\"  • {mem.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a681f3c-b527-4078-9b3a-a7954049a578",
   "metadata": {},
   "source": [
    "### Testing importance-based retrieval\n",
    "We can also filter memories by importance to prioritize critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d75e26d-f5e7-4d68-843f-c6cfad5c25d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-importance memories (≥0.8) (3):\n",
      "  • [0.95] User needs laptop for video editing\n",
      "  • [0.9] User's budget is $1500 for laptop purchase\n",
      "  • [0.8] User purchased Tablet Mini on 2024-01-15 for $499\n"
     ]
    }
   ],
   "source": [
    "# Retrieve memories above importance threshold\n",
    "# This is useful when context window is limited and we need only critical info\n",
    "high_importance = store.get_by_importance(0.8)\n",
    "\n",
    "print(f\"High-importance memories (≥0.8) ({len(high_importance)}):\")\n",
    "for mem in high_importance:\n",
    "    # Display with importance score for context\n",
    "    print(f\"  • [{mem.importance}] {mem.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf3b9c-e440-4e59-9394-9414e85559ca",
   "metadata": {},
   "source": [
    "### Testing type-based retrieval\n",
    "Finally, let's retrieve memories by their type classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-term semantic memories (2):\n",
      "  • User prefers email contact over phone\n",
      "  • User is a software developer\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all long-term semantic memories (persistent user facts/preferences)\n",
    "semantic_memories = store.get_by_type(MemoryType.LONG_TERM_SEMANTIC)\n",
    "\n",
    "print(f\"Long-term semantic memories ({len(semantic_memories)}):\")\n",
    "for mem in semantic_memories:\n",
    "    print(f\"  • {mem.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "In production, this pattern could be extended with vector similarity search using embeddings for semantic retrieval beyond exact tag matching.\n",
    "\n",
    "## Part 4: Importance and relevance scoring\n",
    "\n",
    "Not all memories are equally important. Scoring helps prioritize what to retrieve and what to discard. Imagine having hundreds of memories - some critical (\"user's budget is $1500\"), some useful (\"user prefers email\"), and some trivial (\"user said thanks\"). Without importance scoring, retrieval treats them equally, wasting context window on low-value information. The solution is multi-dimensional scoring that considers intrinsic importance (how critical is this fact?), recency (when was this created?), and relevance (how related is this to the current query?).\n",
    "\n",
    "Good scoring systems combine multiple signals. Base importance comes from content analysis - keywords like \"budget\", \"need\", \"must\" signal high importance. Recency scoring applies exponential decay - recent memories score higher, old ones fade. Relevance scoring matches memory tags against query context. The composite score weights these factors appropriately: maybe 40% importance, 30% recency, 30% relevance. This multi-factor approach ensures we surface the right memories at the right time. Let's start by seeing the problem without scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ All memories (no prioritization):\n",
      "User's budget is $1500 for laptop purchase\n",
      "User prefers email contact over phone\n",
      "User purchased Tablet Mini on 2024-01-15 for $499\n",
      "User needs laptop for video editing\n",
      "User is a software developer\n",
      "\n",
      "Total characters: 195\n"
     ]
    }
   ],
   "source": [
    "# Example of retrieving ALL memories without any prioritization\n",
    "# This demonstrates the problem: important and trivial facts mixed together\n",
    "def retrieve_all_memories(store: MemoryStore) -> str:\n",
    "    \"\"\"Retrieve all memories without prioritization.\"\"\"\n",
    "    # Simply concatenate all memory contents\n",
    "    return \"\\n\".join([m.content for m in store.memories])\n",
    "\n",
    "# Get all memories from our store\n",
    "all_memories = retrieve_all_memories(store)\n",
    "\n",
    "print(\"❌ All memories (no prioritization):\")\n",
    "print(all_memories)\n",
    "print(f\"\\nTotal characters: {len(all_memories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "Problems:\n",
    "- Trivial facts mixed with critical ones.\n",
    "- Wastes context window on low-value info.\n",
    "- No way to prioritize when space is limited.\n",
    "\n",
    "### Implementing importance scoring\n",
    "Now we will build a comprehensive scoring system that evaluates memories across multiple dimensions: intrinsic importance based on content keywords, recency using exponential time decay, relevance through tag matching and a composite score that combines all factors. This multi-dimensional approach ensures we retrieve the most valuable memories for any given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "627b2858-588d-44c0-a001-a6ace69beda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportanceScorer:\n",
    "    \"\"\"Calculate importance scores for memories using multiple signals.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_base_importance(memory: MemoryEntry) -> float:\n",
    "        \"\"\"Calculate intrinsic importance based on content keywords.\"\"\"\n",
    "        content_lower = memory.content.lower()\n",
    "        \n",
    "        # Keywords indicating high-importance information - These suggest critical user requirements or decisions\n",
    "        high_importance_keywords = [\n",
    "            'purchase', 'order', 'budget', 'requirement', \n",
    "            'need', 'must', 'critical', 'urgent'\n",
    "        ]\n",
    "        \n",
    "        # Keywords indicating medium-importance preferences - These suggest user preferences but not requirements\n",
    "        medium_importance_keywords = [\n",
    "            'prefer', 'like', 'want', 'interested'\n",
    "        ]\n",
    "        \n",
    "        # Start with base score of 0.5 (neutral importance)\n",
    "        score = 0.5\n",
    "\n",
    "        # Add bonus for high-importance keywords\n",
    "        for keyword in high_importance_keywords:\n",
    "            if keyword in content_lower:\n",
    "                score += 0.2\n",
    "\n",
    "        # Add smaller bonus for medium-importance keywords\n",
    "        for keyword in medium_importance_keywords:\n",
    "            if keyword in content_lower:\n",
    "                score += 0.1\n",
    "        \n",
    "        # Long-term semantic memories (user profile facts) are generally more important - These are persistently valuable across sessions\n",
    "        if memory.memory_type == MemoryType.LONG_TERM_SEMANTIC:\n",
    "            score += 0.1\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_recency_score(memory: MemoryEntry) -> float:\n",
    "        \"\"\"Calculate score based on recency using exponential decay.\"\"\"\n",
    "        # Calculate how many days old this memory is\n",
    "        age_days = (datetime.now() - memory.timestamp).days\n",
    "        \n",
    "        # Apply exponential decay formula: score = e^(-age/half_life)\n",
    "        # Half-life of 30 days means score drops to 0.5 after 30 days\n",
    "        half_life = 30  # 30 days\n",
    "        import math\n",
    "        recency_score = math.exp(-age_days / half_life)\n",
    "        \n",
    "        return recency_score\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_relevance_score(memory: MemoryEntry, query_tags: List[str]) -> float:\n",
    "        \"\"\"Calculate relevance to current query using tag overlap.\"\"\"\n",
    "        # If no query tags provided, use neutral relevance\n",
    "        if not query_tags:\n",
    "            return 0.5\n",
    "        \n",
    "        # Convert to sets for efficient set operations\n",
    "        memory_tags = set(memory.tags)\n",
    "        query_tag_set = set(query_tags)\n",
    "\n",
    "        # If either set is empty, use neutral relevance\n",
    "        if not memory_tags or not query_tag_set:\n",
    "            return 0.5\n",
    "\n",
    "        # Calculate Jaccard similarity: intersection / union\n",
    "        intersection = len(memory_tags & query_tag_set)\n",
    "        union = len(memory_tags | query_tag_set)\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_composite_score(memory: MemoryEntry, \n",
    "                                  query_tags: List[str] = None,\n",
    "                                  weights: Dict[str, float] = None) -> float:\n",
    "        \"\"\"Calculate composite score from multiple factors.\"\"\"\n",
    "        # Default weights: balance importance, recency and relevance\n",
    "        if weights is None:\n",
    "            weights = {\n",
    "                'importance': 0.4,  # 40% weight on intrinsic importance\n",
    "                'recency': 0.3,     # 30% weight on how recent\n",
    "                'relevance': 0.3    # 30% weight on query relevance\n",
    "            }\n",
    "        \n",
    "        query_tags = query_tags or []\n",
    "\n",
    "        # Calculate all three score components\n",
    "        scores = {\n",
    "            'importance': memory.importance,  # Use pre-calculated importance\n",
    "            'recency': ImportanceScorer.calculate_recency_score(memory),\n",
    "            'relevance': ImportanceScorer.calculate_relevance_score(memory, query_tags)\n",
    "        }\n",
    "\n",
    "        # Weighted sum of all components\n",
    "        # composite = (0.4 × importance) + (0.3 × recency) + (0.3 × relevance)\n",
    "        composite = sum(scores[k] * weights[k] for k in weights)\n",
    "        \n",
    "        return composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0a55e-d00a-431b-a917-d2ca609053aa",
   "metadata": {},
   "source": [
    "We have built a multi-dimensional scoring system that evaluates memories using:\n",
    "- Base importance: Keyword-based analysis assigns scores based on content criticality.\n",
    "- Recency scoring: Exponential decay (e^(-t/τ)) reduces scores for older memories, with configurable half-life. The exponential decay function creates smooth aging.\n",
    "- Relevance scoring: Jaccard similarity between memory tags and query tags measures contextual relevance. Jaccard similarity handles partial tag matches - memories with some relevant tags score higher than completely unrelated ones.\n",
    "- Composite scoring: Weighted combination of all three factors produces final ranking score.\n",
    "\n",
    "The scoring separates concerns: each method handles one dimension, making it easy to modify or extend individual scoring components. In production, base importance could use ML models instead of keywords, and relevance could use embedding similarity for semantic matching.\n",
    "\n",
    "Now let's test our scoring system by scoring all memories in the context of a specific query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance Scoring:\n",
      "============================================================\n",
      "Query context tags: ['laptop', 'purchase']\n",
      "\n",
      "\n",
      "Memories ranked by composite score:\n",
      "\n",
      "[Score: 0.860] User's budget is $1500 for laptop purchase\n",
      "  Importance: 0.90 | Age: 0 days\n",
      "  Tags: budget, purchase, laptop\n",
      "\n",
      "[Score: 0.755] User needs laptop for video editing\n",
      "  Importance: 0.95 | Age: 0 days\n",
      "  Tags: requirement, laptop, video_editing\n",
      "\n",
      "[Score: 0.395] User purchased Tablet Mini on 2024-01-15 for $499\n",
      "  Importance: 0.80 | Age: 693 days\n",
      "  Tags: purchase, tablet, transaction\n",
      "\n",
      "[Score: 0.390] User prefers email contact over phone\n",
      "  Importance: 0.70 | Age: 30 days\n",
      "  Tags: communication, preference\n",
      "\n",
      "[Score: 0.281] User is a software developer\n",
      "  Importance: 0.60 | Age: 60 days\n",
      "  Tags: occupation, profile\n"
     ]
    }
   ],
   "source": [
    "# Define the current query context using tags\n",
    "# This simulates a user asking about laptop purchases\n",
    "query_tags = [\"laptop\", \"purchase\"]\n",
    "\n",
    "print(\"Importance Scoring:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Query context tags: {query_tags}\\n\")\n",
    "\n",
    "# Calculate composite scores for all memories\n",
    "scored_memories = []\n",
    "for mem in store.memories:\n",
    "    # Get the composite score considering importance, recency, and relevance\n",
    "    score = ImportanceScorer.calculate_composite_score(mem, query_tags)\n",
    "    scored_memories.append((mem, score))\n",
    "\n",
    "# Sort memories by score in descending order (highest scores first)\n",
    "scored_memories.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display memories ranked by their composite scores\n",
    "print(\"\\nMemories ranked by composite score:\")\n",
    "for mem, score in scored_memories:\n",
    "    print(f\"\\n[Score: {score:.3f}] {mem.content}\")\n",
    "    print(f\"  Importance: {mem.importance:.2f} | Age: {(datetime.now() - mem.timestamp).days} days\")\n",
    "    print(f\"  Tags: {', '.join(mem.tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Part 5: Consolidation rules\n",
    "\n",
    "As memories accumulate, consolidation compresses related memories to save space while preserving information. Without consolidation, memory systems grow unbounded. A user might mention their \"$1500 budget\" three times in a conversation, creating three nearly identical memories. Over weeks and months, this redundancy multiplies - thousands of memories saying essentially the same thing, consuming context window and slowing retrieval. The solution is intelligent consolidation that detects similar memories and merges them into single, comprehensive entries.\n",
    "\n",
    "Effective consolidation requires careful design. First, identify similar memories using tag overlap, content similarity or embedding distance. Second, merge them intelligently - don't just delete, but create a new consolidated memory that preserves information from all sources. Third, maintain provenance by tracking which original memories contributed to the consolidated version. Fourth, preserve the most recent timestamp and highest importance score. This approach achieves dramatic space savings (often 50-70% reduction) while actually improving information quality by removing redundancy. Let's see the problem first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Before consolidation (redundant):\n",
      "  • [mem_101] User's budget is $1500\n",
      "  • [mem_102] User can spend up to $1500 on laptop\n",
      "  • [mem_103] User mentioned $1500 budget for new laptop\n",
      "\n",
      "Total memories: 3\n",
      "Total characters: 100\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the problem: redundant memories saying essentially the same thing\n",
    "# These three memories all convey the same information about budget\n",
    "redundant_memories = [\n",
    "    MemoryEntry(\n",
    "        id=\"mem_101\",\n",
    "        memory_type=MemoryType.SHORT_TERM_SEMANTIC,\n",
    "        content=\"User's budget is $1500\",\n",
    "        timestamp=datetime.now() - timedelta(minutes=30),\n",
    "        importance=0.8,\n",
    "        tags=[\"budget\"],\n",
    "        metadata={}\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_102\",\n",
    "        memory_type=MemoryType.SHORT_TERM_SEMANTIC,\n",
    "        content=\"User can spend up to $1500 on laptop\",\n",
    "        timestamp=datetime.now() - timedelta(minutes=20),\n",
    "        importance=0.8,\n",
    "        tags=[\"budget\", \"laptop\"],\n",
    "        metadata={}\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_103\",\n",
    "        memory_type=MemoryType.SHORT_TERM_SEMANTIC,\n",
    "        content=\"User mentioned $1500 budget for new laptop\",\n",
    "        timestamp=datetime.now() - timedelta(minutes=10),\n",
    "        importance=0.8,\n",
    "        tags=[\"budget\", \"laptop\"],\n",
    "        metadata={}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Show the redundancy problem\n",
    "print(\"❌ Before consolidation (redundant):\")\n",
    "for mem in redundant_memories:\n",
    "    print(f\"  • [{mem.id}] {mem.content}\")\n",
    "\n",
    "# Calculate wasted space\n",
    "print(f\"\\nTotal memories: {len(redundant_memories)}\")\n",
    "print(\"Total characters: {}\".format(sum(len(m.content) for m in redundant_memories)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Implementing consolidation rules\n",
    "Now, we will build a consolidation system that intelligently merges similar memories. The system will use tag overlap to identify related memories, then use an LLM to create a consolidated version that preserves the essential information while eliminating redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6232899b-d6d5-4eae-963b-1a82bfa24f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryConsolidator:\n",
    "    \"\"\"Consolidate related memories to reduce redundancy.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        # Store LLM reference for generating consolidated memories\n",
    "        self.llm = llm\n",
    "    \n",
    "    def find_similar_memories(self, memories: List[MemoryEntry], \n",
    "                            similarity_threshold: float = 0.7) -> List[List[MemoryEntry]]:\n",
    "        \"\"\"Group similar memories together based on tag overlap.\"\"\"\n",
    "        # Simple approach: group by overlapping tags\n",
    "        groups = []\n",
    "        processed = set()  # Track which memories we have already grouped\n",
    "\n",
    "        # Compare each memory with all subsequent memories\n",
    "        for i, mem1 in enumerate(memories):\n",
    "            # Skip if this memory is already in a group\n",
    "            if mem1.id in processed:\n",
    "                continue\n",
    "\n",
    "            # Start a new group with this memory\n",
    "            group = [mem1]\n",
    "            processed.add(mem1.id)\n",
    "\n",
    "            # Look for similar memories in the remaining list\n",
    "            for mem2 in memories[i+1:]:\n",
    "                if mem2.id in processed:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate tag overlap using Jaccard similarity\n",
    "                tags1 = set(mem1.tags)\n",
    "                tags2 = set(mem2.tags)\n",
    "                \n",
    "                if tags1 and tags2:\n",
    "                    # Jaccard = intersection / union\n",
    "                    overlap = len(tags1 & tags2) / len(tags1 | tags2)\n",
    "\n",
    "                    # If overlap exceeds threshold, add to group\n",
    "                    if overlap >= similarity_threshold:\n",
    "                        group.append(mem2)\n",
    "                        processed.add(mem2.id)\n",
    "\n",
    "            # Only keep groups with multiple memories (single memories don't need consolidation)\n",
    "            if len(group) > 1:\n",
    "                groups.append(group)\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def consolidate_group(self, group: List[MemoryEntry]) -> MemoryEntry:\n",
    "        \"\"\"Consolidate a group of similar memories into one using LLM.\"\"\"\n",
    "        # Format all memory contents for the LLM\n",
    "        contents = \"\\n\".join([f\"- {m.content}\" for m in group])\n",
    "\n",
    "        # Ask LLM to create consolidated version\n",
    "        prompt = f\"\"\"Consolidate these related memories into a single, concise statement:\n",
    "\n",
    "{contents}\n",
    "\n",
    "Consolidated memory (one sentence):\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        consolidated_content = response.content.strip()\n",
    "        \n",
    "        # Combine all tags from all memories (union)\n",
    "        all_tags = set()\n",
    "        for mem in group:\n",
    "            all_tags.update(mem.tags)\n",
    "        \n",
    "        # Use the most recent timestamp from the group\n",
    "        most_recent = max(group, key=lambda m: m.timestamp)\n",
    "\n",
    "        # Use the highest importance score from the group\n",
    "        highest_importance = max(m.importance for m in group)\n",
    "\n",
    "        # Create consolidated memory with metadata tracking source memories\n",
    "        return MemoryEntry(\n",
    "            id=f\"consolidated_{group[0].id}\",\n",
    "            memory_type=group[0].memory_type,  # Assume same type for group\n",
    "            content=consolidated_content,\n",
    "            timestamp=most_recent.timestamp,\n",
    "            importance=highest_importance,\n",
    "            tags=list(all_tags),  # Convert set back to list\n",
    "            metadata={\n",
    "                \"consolidated_from\": [m.id for m in group],\n",
    "                \"consolidation_date\": datetime.now().isoformat()\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def consolidate_memories(self, memories: List[MemoryEntry]) -> List[MemoryEntry]:\n",
    "        \"\"\"Consolidate all similar memories in the list.\"\"\"\n",
    "        # Find groups of similar memories\n",
    "        groups = self.find_similar_memories(memories)\n",
    "        \n",
    "        # Track which memories have been consolidated\n",
    "        consolidated = []\n",
    "        consolidated_ids = set()\n",
    "\n",
    "        # Consolidate each group\n",
    "        for group in groups:\n",
    "            consolidated_mem = self.consolidate_group(group)\n",
    "            consolidated.append(consolidated_mem)\n",
    "            # Track all memory IDs that went into this consolidation\n",
    "            consolidated_ids.update(m.id for m in group)\n",
    "        \n",
    "        # Keep memories that were not consolidated (singlets)\n",
    "        for mem in memories:\n",
    "            if mem.id not in consolidated_ids:\n",
    "                consolidated.append(mem)\n",
    "        \n",
    "        return consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c12f9-b678-42ba-a743-fb04d75d3ace",
   "metadata": {},
   "source": [
    "The consolidation system implements intelligent memory merging through:\n",
    "- Similarity detection: Jaccard similarity on tag sets identifies related memories above a configurable threshold. The similarity threshold (default 0.7) controls aggressiveness - lower values consolidate more, higher values are more conservative.\n",
    "- LLM-powered merging: Uses the language model to create natural, consolidated summaries that preserve key information. LLM consolidation is more nuanced than simple string concatenation - it can rephrase, remove duplication, and create coherent summaries.\n",
    "- Metadata preservation: Tracks provenance by storing source memory IDs in the consolidated entry.\n",
    "- Property aggregation: Retains the most recent timestamp and highest importance score from the group.\n",
    "\n",
    "In production, we might batch consolidation (run nightly), use embedding similarity instead of tag overlap for better semantic matching, or implement incremental consolidation that only processes new memories.\n",
    "\n",
    "Now let's test consolidation on our redundant memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consolidating redundant memories...\n",
      "\n",
      "✅ After consolidation:\n",
      "  • [consolidated_mem_102] User has a budget of up to $1500 for a new laptop.\n",
      "    (consolidated from: mem_102, mem_103)\n",
      "  • [mem_101] User's budget is $1500\n",
      "\n",
      "Total memories: 3 → 2\n",
      "Total characters: 100 → 72\n",
      "✅ Space saved: 28 characters (28.0%)\n"
     ]
    }
   ],
   "source": [
    "# Create consolidator instance\n",
    "consolidator = MemoryConsolidator(llm)\n",
    "\n",
    "print(\"\\nConsolidating redundant memories...\\n\")\n",
    "# Consolidate the redundant memories\n",
    "consolidated_memories = consolidator.consolidate_memories(redundant_memories)\n",
    "\n",
    "# Display the consolidated results\n",
    "print(\"✅ After consolidation:\")\n",
    "for mem in consolidated_memories:\n",
    "    print(f\"  • [{mem.id}] {mem.content}\")\n",
    "    # Show which memories were consolidated if applicable\n",
    "    if \"consolidated_from\" in mem.metadata:\n",
    "        print(f\"    (consolidated from: {', '.join(mem.metadata['consolidated_from'])})\")\n",
    "\n",
    "# Calculate space savings\n",
    "original_count = len(redundant_memories)\n",
    "consolidated_count = len(consolidated_memories)\n",
    "original_chars = sum(len(m.content) for m in redundant_memories)\n",
    "new_chars = sum(len(m.content) for m in consolidated_memories)\n",
    "char_savings = original_chars - new_chars\n",
    "percent_savings = (char_savings / original_chars * 100) if original_chars > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal memories: {original_count} → {consolidated_count}\")\n",
    "print(f\"Total characters: {original_chars} → {new_chars}\")\n",
    "print(f\"✅ Space saved: {char_savings} characters ({percent_savings:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Part 6: Memory lifecycle management\n",
    "Effective memory systems manage the full lifecycle: what gets stored, how it is encoded, when it is retrieved, and when it is compressed or discarded. Memory lifecycle management is the orchestration layer that brings together all the techniques we have covered. It is not enough to have structured storage, tagging, scoring and consolidation as separate pieces - they need to work together in a coordinated flow that handles memories from birth to death.\n",
    "\n",
    "The memory lifecycle has five critical stages. Storage decides what information deserves to be remembered in the first place - filtering out trivial greetings and acknowledgments while capturing substantive facts. Encoding transforms raw text into structured entries with appropriate types, tags, and importance scores. Retrieval uses scoring to surface the most relevant memories for each query context. Consolidation periodically merges similar memories to prevent unbounded growth. Finally, discard removes memories that are both old and unimportant, making room for new information. Let's build a complete lifecycle manager that coordinates all these stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryLifecycleManager:\n",
    "    \"\"\"Manage the full lifecycle of memories.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_memories: int = 100):\n",
    "        self.llm = llm  # LLM for tag extraction and encoding decisions\n",
    "        self.store = MemoryStore()  # Memory store with inverted indexing\n",
    "        self.consolidator = MemoryConsolidator(llm)  # Consolidator for merging similar memories\n",
    "        self.max_memories = max_memories  # Maximum number of memories before forcing cleanup\n",
    "    \n",
    "    # Stage 1: STORAGE DECISION - What deserves to be remembered?\n",
    "    def should_store(self, content: str, context: Dict) -> bool:\n",
    "        \"\"\"Decide if information should be stored based on content analysis.\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        # Filter out trivial conversational phrases - These add no value and waste memory space\n",
    "        trivial_phrases = ['hello', 'hi', 'thanks', 'ok', 'bye']\n",
    "        if any(phrase in content_lower for phrase in trivial_phrases):\n",
    "            return False\n",
    "        \n",
    "        # Store factual information, preferences and important events - These keywords indicate substantive information worth remembering\n",
    "        important_indicators = [\n",
    "            'prefer', 'need', 'want', 'budget', 'purchase', \n",
    "            'order', 'like', 'dislike', 'always', 'never'\n",
    "        ]\n",
    "        if any(indicator in content_lower for indicator in important_indicators):\n",
    "            return True\n",
    "        \n",
    "        # Store if content is substantial (not too short) - Very short statements are likely acknowledgments, not facts\n",
    "        return len(content.split()) > 5\n",
    "    \n",
    "    # Stage 2: ENCODING - Transform raw text into structured memory\n",
    "    def encode_memory(self, content: str, context: Dict) -> MemoryEntry:\n",
    "        \"\"\"Encode information as a structured memory entry with tags and importance.\"\"\"\n",
    "        # Determine appropriate memory type based on content keywords\n",
    "        if any(word in content.lower() for word in ['purchased', 'ordered', 'on 20']):\n",
    "            # Past events with dates are long-term episodic\n",
    "            memory_type = MemoryType.LONG_TERM_EPISODIC\n",
    "        elif context.get('session_scope', False):\n",
    "            # Session-specific facts are short-term semantic\n",
    "            memory_type = MemoryType.SHORT_TERM_SEMANTIC\n",
    "        else:\n",
    "            # Default to long-term semantic for persistent facts\n",
    "            memory_type = MemoryType.LONG_TERM_SEMANTIC\n",
    "        \n",
    "        # Extract tags using LLM - more sophisticated than keyword matching\n",
    "        tag_prompt = f\"\"\"Extract 2-4 relevant tags from this statement. Return only tags separated by commas.\n",
    "\n",
    "Statement: {content}\n",
    "\n",
    "Tags:\"\"\"\n",
    "        tag_response = self.llm.invoke(tag_prompt)\n",
    "        # Clean and normalize the extracted tags\n",
    "        tags = [tag.strip().lower() for tag in tag_response.content.split(',')]\n",
    "        \n",
    "        # Calculate importance score using our scoring system\n",
    "        # Create temporary memory to use with ImportanceScorer\n",
    "        temp_memory = MemoryEntry(\n",
    "            id=\"temp\",\n",
    "            memory_type=memory_type,\n",
    "            content=content,\n",
    "            timestamp=datetime.now(),\n",
    "            importance=0.5,\n",
    "            tags=tags,\n",
    "            metadata={}\n",
    "        )\n",
    "        importance = ImportanceScorer.calculate_base_importance(temp_memory)\n",
    "        \n",
    "        # Create final memory entry with generated ID\n",
    "        memory_id = f\"mem_{len(self.store.memories) + 1:03d}\"\n",
    "        return MemoryEntry(\n",
    "            id=memory_id,\n",
    "            memory_type=memory_type,\n",
    "            content=content,\n",
    "            timestamp=datetime.now(),\n",
    "            importance=importance,\n",
    "            tags=tags,\n",
    "            metadata=context  # Store original context as metadata\n",
    "        )\n",
    "    \n",
    "    # Stage 3: RETRIEVAL - Find relevant memories for query context\n",
    "    def retrieve_relevant(self, query: str, query_tags: List[str], \n",
    "                         max_results: int = 5) -> List[MemoryEntry]:\n",
    "        \"\"\"Retrieve most relevant memories for query using composite scoring.\"\"\"\n",
    "        # Score all memories against the query context\n",
    "        scored = []\n",
    "        for mem in self.store.memories:\n",
    "            # Use importance, recency, and relevance scoring\n",
    "            score = ImportanceScorer.calculate_composite_score(mem, query_tags)\n",
    "            scored.append((mem, score))\n",
    "        \n",
    "        # Sort by composite score (highest first) and return top N\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [mem for mem, score in scored[:max_results]]\n",
    "    \n",
    "    # Stage 4: CONSOLIDATION - Compress similar memories\n",
    "    def consolidate_if_needed(self):\n",
    "        \"\"\"Consolidate memories if approaching limit.\"\"\"\n",
    "        # Only consolidate when at 80% capacity to avoid frequent consolidation\n",
    "        if len(self.store.memories) > self.max_memories * 0.8:\n",
    "            print(f\"Memory approaching limit ({len(self.store.memories)}/{self.max_memories}). Consolidating...\")\n",
    "            \n",
    "            # Target short-term memories for consolidation - Long-term memories are typically more diverse and important\n",
    "            short_term = [m for m in self.store.memories \n",
    "                         if m.memory_type in [MemoryType.SHORT_TERM_EPISODIC, MemoryType.SHORT_TERM_SEMANTIC]]\n",
    "\n",
    "            # Only consolidate if we have enough short-term memories\n",
    "            if len(short_term) > 5:\n",
    "                consolidated = self.consolidator.consolidate_memories(short_term)\n",
    "                \n",
    "                # Replace short-term memories with consolidated versions - Keep all long-term memories unchanged\n",
    "                self.store.memories = [m for m in self.store.memories \n",
    "                                      if m.memory_type not in [MemoryType.SHORT_TERM_EPISODIC, MemoryType.SHORT_TERM_SEMANTIC]]\n",
    "                self.store.memories.extend(consolidated)\n",
    "                \n",
    "                print(f\"Consolidated {len(short_term)} memories into {len(consolidated)}\")\n",
    "    \n",
    "    # Stage 5: DISCARD - Remove old, unimportant memories\n",
    "    def discard_old_memories(self, max_age_days: int = 90, min_importance: float = 0.3):\n",
    "        \"\"\"Remove old, low-importance memories.\"\"\"\n",
    "        before_count = len(self.store.memories)\n",
    "        \n",
    "        # Calculate the cutoff date for \"old\" memories\n",
    "        cutoff_date = datetime.now() - timedelta(days=max_age_days)\n",
    "\n",
    "        # Keep memories that meet at least one criterion:\n",
    "        # 1. Recent (within max_age_days)\n",
    "        # 2. Important (above min_importance threshold)\n",
    "        # 3. Long-term semantic (always preserve user profile/preferences)\n",
    "        self.store.memories = [\n",
    "            m for m in self.store.memories\n",
    "            if (m.timestamp > cutoff_date or \n",
    "                m.importance >= min_importance or \n",
    "                m.memory_type == MemoryType.LONG_TERM_SEMANTIC)\n",
    "        ]\n",
    "\n",
    "        # Report how many memories were discarded\n",
    "        discarded = before_count - len(self.store.memories)\n",
    "        if discarded > 0:\n",
    "            print(f\"Discarded {discarded} old/low-importance memories\")\n",
    "    \n",
    "    def add_memory(self, content: str, context: Dict = None):\n",
    "        \"\"\"Add memory through the full lifecycle: filter, encode, store, maintain.\"\"\"\n",
    "        context = context or {}\n",
    "        \n",
    "        # Stage 1: STORAGE DECISION - Should we remember this?\n",
    "        if not self.should_store(content, context):\n",
    "            print(f\"Skipped (trivial): {content}\")\n",
    "            return\n",
    "        \n",
    "        # Stage 2: ENCODING - Transform to structured memory\n",
    "        memory = self.encode_memory(content, context)\n",
    "        self.store.add_memory(memory)\n",
    "        print(f\"Stored [{memory.memory_type.value}]: {content}\")\n",
    "        \n",
    "        # Stages 4 & 5: MAINTENANCE - Consolidate and discard as needed\n",
    "        self.consolidate_if_needed()\n",
    "        self.discard_old_memories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf11da-15b4-46eb-a822-73615d2c3d62",
   "metadata": {},
   "source": [
    "The memory lifecycle manager orchestrates all five stages of memory management:\n",
    "- Storage filtering: Uses keyword-based rules to skip trivial content (\"hi\", \"thanks\") while storing substantive information.\n",
    "- Intelligent encoding: LLM extracts tags, determines memory type based on content patterns, and calculates importance scores.\n",
    "- Retrieval with scoring: Combines importance, recency, and relevance to rank memories for any query.\n",
    "- Automatic consolidation: Triggers when approaching 80% capacity, merging similar short-term memories.\n",
    "- Selective discard: Removes old, low-importance memories while always preserving long-term semantic facts.\n",
    "\n",
    "The `add_memory` method implements the template method pattern, ensuring consistent lifecycle processing for all memories. In production, we would add persistence (database storage), async processing for LLM calls and monitoring/alerting for memory usage. The lifecycle stages could be extended: adding verification (fact-checking), enrichment (linking related memories) or archival (moving old memories to cold storage).\n",
    "\n",
    "Let's test the complete lifecycle management system with various types of input to see how it handles filtering, encoding and maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4c7e613-6fa2-4b35-8af1-b5668074dfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Memory Lifecycle Management\n",
      "============================================================\n",
      "\n",
      "Adding memories:\n",
      "\n",
      "Skipped (trivial): Hi there\n",
      "Stored [short_term_semantic]: My budget is $1500 for a laptop\n",
      "Stored [short_term_semantic]: I need it for video editing\n",
      "Stored [long_term_semantic]: I prefer Windows over Mac\n",
      "Skipped (trivial): Thanks\n",
      "Stored [long_term_episodic]: I purchased a tablet last month\n"
     ]
    }
   ],
   "source": [
    "# Create lifecycle manager with small limit for testing\n",
    "manager = MemoryLifecycleManager(llm, max_memories=10)\n",
    "\n",
    "# Prepare test inputs - mix of trivial and substantive content\n",
    "test_inputs = [\n",
    "    (\"Hi there\", {}),  # Should be filtered out (trivial greeting)\n",
    "    (\"My budget is $1500 for a laptop\", {\"session_scope\": True}),  # Should store\n",
    "    (\"I need it for video editing\", {\"session_scope\": True}),  # Should store\n",
    "    (\"I prefer Windows over Mac\", {}),  # Should store\n",
    "    (\"Thanks\", {}),  # Should be filtered out (trivial acknowledgment)\n",
    "    (\"I purchased a tablet last month\", {}),  # Should store (event)\n",
    "]\n",
    "\n",
    "print(\"Testing Memory Lifecycle Management\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAdding memories:\\n\")\n",
    "\n",
    "# Process each input through the full lifecycle\n",
    "for content, context in test_inputs:\n",
    "    manager.add_memory(content, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32f68180-1346-4237-9528-f7256bac33ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "Total memories stored: 4\n",
      "\n",
      "Stored memories:\n",
      "  • [short_term_semantic] My budget is $1500 for a laptop\n",
      "    Tags: budget, laptop, $1500 | Importance: 0.70\n",
      "  • [short_term_semantic] I need it for video editing\n",
      "    Tags: video editing, editing tools, multimedia | Importance: 0.70\n",
      "  • [long_term_semantic] I prefer Windows over Mac\n",
      "    Tags: windows, mac, preference | Importance: 0.70\n",
      "  • [long_term_episodic] I purchased a tablet last month\n",
      "    Tags: tablet, purchase, electronics | Importance: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Display the final state of memories\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nTotal memories stored: {len(manager.store.memories)}\")\n",
    "print(\"\\nStored memories:\")\n",
    "\n",
    "# Show each memory with its details\n",
    "for mem in manager.store.memories:\n",
    "    print(f\"  • [{mem.memory_type.value}] {mem.content}\")\n",
    "    print(f\"    Tags: {', '.join(mem.tags)} | Importance: {mem.importance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Putting it all together: Production memory system\n",
    "Now that we have built all the individual components - memory types, structured storage, semantic tagging, importance scoring, consolidation, and lifecycle management - let's create a production-ready system that ties everything together. This final implementation provides a complete API for memory-enabled AI agents, handling conversation processing, fact extraction, context retrieval and system monitoring. The production system wraps our lifecycle manager with higher-level methods designed for real-world use cases: processing conversation turns, retrieving context for queries and tracking system statistics. This is the layer that application developers would actually interact with when building memory-enabled agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionMemorySystem:\n",
    "    \"\"\"Production-ready memory system with all best practices.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_memories: int = 100):\n",
    "        # Wrap the lifecycle manager for higher-level operations\n",
    "        self.manager = MemoryLifecycleManager(llm, max_memories)\n",
    "    \n",
    "    def process_interaction(self, user_message: str, agent_response: str, \n",
    "                          extract_facts: bool = True):\n",
    "        \"\"\"Process a conversation turn and extract memorable facts.\"\"\"\n",
    "        if extract_facts:\n",
    "            # Use LLM to extract facts from conversation - This is more sophisticated than storing raw messages\n",
    "            extraction_prompt = f\"\"\"Extract factual information and user preferences from this conversation. \n",
    "Return each fact as a separate line, or 'None' if no facts to extract.\n",
    "\n",
    "User: {user_message}\n",
    "Agent: {agent_response}\n",
    "\n",
    "Extracted facts:\"\"\"\n",
    "            \n",
    "            response = self.manager.llm.invoke(extraction_prompt)\n",
    "            # Parse the LLM response into individual facts\n",
    "            facts = [f.strip() for f in response.content.split('\\n') if f.strip() and f.strip().lower() != 'none']\n",
    "\n",
    "            # Store each extracted fact through the lifecycle\n",
    "            for fact in facts:\n",
    "                self.manager.add_memory(fact, {\"source\": \"conversation\"})\n",
    "        else:\n",
    "            # Fallback: store the full user message as a memory - Useful when fact extraction is not needed\n",
    "            self.manager.add_memory(\n",
    "                f\"User said: {user_message}\",\n",
    "                {\"source\": \"conversation\", \"session_scope\": True}\n",
    "            )\n",
    "    \n",
    "    def get_context_for_query(self, query: str, query_tags: List[str] = None) -> str:\n",
    "        \"\"\"Get relevant memory context formatted for inclusion in prompts.\"\"\"\n",
    "        if query_tags is None:\n",
    "            # Extract tags from the query using LLM - This handles queries where tags are not provided\n",
    "            tag_prompt = f\"\"\"Extract 2-3 key topics/tags from this query. Return only tags separated by commas.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Tags:\"\"\"\n",
    "            response = self.manager.llm.invoke(tag_prompt)\n",
    "            query_tags = [tag.strip().lower() for tag in response.content.split(',')]\n",
    "        \n",
    "        # Retrieve top relevant memories using our scoring system\n",
    "        relevant = self.manager.retrieve_relevant(query, query_tags, max_results=5)\n",
    "\n",
    "        # Handle case where no relevant memories found\n",
    "        if not relevant:\n",
    "            return \"No relevant context from memory.\"\n",
    "        \n",
    "        # Build formatted context string for inclusion in prompts\n",
    "        context_parts = [\"Relevant context from memory:\"]\n",
    "        for mem in relevant:\n",
    "            # Include both content and memory type for transparency\n",
    "            context_parts.append(f\"- {mem.content} [{mem.memory_type.value}]\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get memory system statistics for monitoring and debugging.\"\"\"\n",
    "        memories = self.manager.store.memories\n",
    "\n",
    "        # Calculate comprehensive statistics\n",
    "        stats = {\n",
    "            \"total_memories\": len(memories),\n",
    "            \"by_type\": {},  # Breakdown by memory type\n",
    "            \"avg_importance\": sum(m.importance for m in memories) / len(memories) if memories else 0,\n",
    "            \"total_tags\": len(self.manager.store.tag_index),\n",
    "        }\n",
    "\n",
    "        # Count memories by type\n",
    "        for mem_type in MemoryType:\n",
    "            count = len([m for m in memories if m.memory_type == mem_type])\n",
    "            stats[\"by_type\"][mem_type.value] = count\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867500d-301b-4fac-b2c0-50188ade295c",
   "metadata": {},
   "source": [
    "The production memory system provides a complete, application-ready interface for memory-enabled agents:\n",
    "- Conversation processing: Uses LLM to extract facts from dialogue, storing only meaningful information.\n",
    "- Context retrieval: Automatically tags queries, scores memories and formats relevant context for prompt inclusion.\n",
    "- Statistics monitoring: Provides insights into memory usage, type distribution and system health.\n",
    "- Lifecycle automation: All memory management (filtering, encoding, consolidation, discard) happens automatically.\n",
    "\n",
    "In production deployments, we would add: persistent storage (PostgreSQL with pgvector for similarity search), async processing (background workers for LLM calls), caching (Redis for frequently accessed memories), monitoring (Prometheus metrics, alerting on thresholds), API layer (REST/GraphQL endpoints) and multi-tenancy (isolated memory spaces per user).\n",
    "\n",
    "Let's test the complete production system by simulating a customer service conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcdf343b-8b40-4d79-aa13-92ffefe31cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Memory System Demo\n",
      "============================================================\n",
      "\n",
      "Processing conversations:\n",
      "\n",
      "User: I'm looking for a laptop for video editing\n",
      "Agent: I can help you find the perfect laptop for video editing. What's your budget?\n",
      "Skipped (trivial): - The user is looking for a laptop for video editing.\n",
      "Stored [long_term_semantic]: - The agent is offering to help the user find a suitable laptop.\n",
      "Stored [long_term_semantic]: - The agent is inquiring about the user's budget.\n",
      "\n",
      "User: My budget is around $1500\n",
      "Agent: Great! For video editing at $1500, I'd recommend our Laptop Pro X1.\n",
      "Stored [long_term_semantic]: - User's budget is around $1500.\n",
      "Stored [long_term_semantic]: - Agent recommends the Laptop Pro X1 for video editing at $1500.\n",
      "\n",
      "User: I prefer Windows over Mac\n",
      "Agent: Perfect! The Laptop Pro X1 runs Windows 11 Pro.\n",
      "Stored [long_term_semantic]: - User prefers Windows over Mac.\n",
      "Stored [long_term_semantic]: - The Laptop Pro X1 runs Windows 11 Pro.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create production memory system\n",
    "memory_system = ProductionMemorySystem(llm, max_memories=50)\n",
    "\n",
    "# Simulate a realistic customer service conversation\n",
    "conversations = [\n",
    "    (\"I'm looking for a laptop for video editing\", \n",
    "     \"I can help you find the perfect laptop for video editing. What's your budget?\"),\n",
    "    (\"My budget is around $1500\", \n",
    "     \"Great! For video editing at $1500, I'd recommend our Laptop Pro X1.\"),\n",
    "    (\"I prefer Windows over Mac\", \n",
    "     \"Perfect! The Laptop Pro X1 runs Windows 11 Pro.\"),\n",
    "]\n",
    "\n",
    "print(\"Production Memory System Demo\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nProcessing conversations:\\n\")\n",
    "\n",
    "# Process each conversation turn\n",
    "for user_msg, agent_msg in conversations:\n",
    "    print(f\"User: {user_msg}\")\n",
    "    print(f\"Agent: {agent_msg}\")\n",
    "    # Extract and store facts from this interaction\n",
    "    memory_system.process_interaction(user_msg, agent_msg)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15b2fc31-bf30-4209-8367-8ec65fcd0d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Memory System Statistics:\n",
      "Total memories: 6\n",
      "Average importance: 0.68\n",
      "Total unique tags: 15\n",
      "\n",
      "Memories by type:\n",
      "  long_term_semantic: 6\n"
     ]
    }
   ],
   "source": [
    "# Get and display system statistics\n",
    "stats = memory_system.get_statistics()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMemory System Statistics:\")\n",
    "print(f\"Total memories: {stats['total_memories']}\")\n",
    "print(f\"Average importance: {stats['avg_importance']:.2f}\")\n",
    "print(f\"Total unique tags: {stats['total_tags']}\")\n",
    "\n",
    "# Show breakdown by memory type\n",
    "print(\"\\nMemories by type:\")\n",
    "for mem_type, count in stats['by_type'].items():\n",
    "    if count > 0:\n",
    "        print(f\"  {mem_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47724629-7aaa-4ab6-bbdc-1dc490a27a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "Testing context retrieval:\n",
      "\n",
      "Query: What laptop should I buy?\n",
      "\n",
      "Relevant context from memory:\n",
      "- - The Laptop Pro X1 runs Windows 11 Pro. [long_term_semantic]\n",
      "- - The agent is offering to help the user find a suitable laptop. [long_term_semantic]\n",
      "- - The agent is inquiring about the user's budget. [long_term_semantic]\n",
      "- - User's budget is around $1500. [long_term_semantic]\n",
      "- - User prefers Windows over Mac. [long_term_semantic]\n"
     ]
    }
   ],
   "source": [
    "# Test context retrieval for a new query\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTesting context retrieval:\\n\")\n",
    "\n",
    "# Simulate a follow-up query\n",
    "query = \"What laptop should I buy?\"\n",
    "\n",
    "# Get relevant context from memory\n",
    "context = memory_system.get_context_for_query(query)\n",
    "\n",
    "# Display the query and retrieved context\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\n{context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "The system can be extended with: memory verification (fact-checking against knowledge bases), memory merging across sessions (recognizing returning users), memory export/import (for data portability) and memory analytics (understanding what users talk about most)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
