{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "\n",
    "Context engineering is the practice of crafting the information that flows into and out of AI agents. Among the various \"write strategies\" for producing high-quality context, prompt engineering stands as one of the most fundamental. Well-designed prompts serve as the blueprint for agent behavior, guiding not just what the agent says, but how it reasons, what information it prioritizes, and how it structures its responses.\n",
    "\n",
    "The challenge is finding the right level of detail and organization. Prompts that are too vague leave agents directionless, producing inconsistent or off-target outputs. Prompts that are too prescriptive become brittle and token-inefficient, micromanaging every detail at the cost of flexibility. The solution lies in treating prompts as composable, reusable modules that can be mixed and matched based on task requirements, enhanced with strategic examples, and structured with clear delimiters to maximize both clarity and efficiency.\n",
    "\n",
    "In this notebook, we explore systematic techniques for engineering prompts that produce consistent, high-quality outputs while minimizing token waste. We will examine how to structure system prompts as composable modules, use few-shot examples strategically to guide behavior, find the appropriate \"altitude\" for instructions, and leverage structured delimiters to organize information clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by initializing the language model that will process our prompts. Using a consistent model with temperature 0 ensures deterministic, reproducible outputs across our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(),\n",
    "    temperature=0  # Set to 0 for more deterministic outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Composable prompt modules\n",
    "\n",
    "The traditional approach to prompt engineering creates monolithic prompts—single, large blocks of text that attempt to handle every possible scenario. While this might seem comprehensive, it quickly becomes problematic as systems grow. These monolithic prompts load all information into every request regardless of relevance, wasting tokens and diluting focus. They become maintenance nightmares as different sections need updates at different times and they cannot be reused across different agents or tasks.\n",
    "\n",
    "The solution is composability: breaking prompts into focused, reusable modules that can be mixed and matched based on task requirements. Each module handles a specific concern - identity, communication style, product information, policies, security rules - and can be independently maintained and tested. When a request comes in, we select only the relevant modules, dramatically reducing token usage while improving clarity and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much is the Laptop Pro X1?\n",
      "\n",
      "Response with monolithic prompt:\n",
      "The Laptop Pro X1 is priced at $1299. If you have any other questions or need assistance, feel free to ask!\n",
      "\n",
      "Prompt length: 743 characters\n"
     ]
    }
   ],
   "source": [
    "# Example of a monolithic prompt that tries to handle everything at once\n",
    "monolithic_prompt = \"\"\"You are a customer service agent for TechStore.\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "- Answer product questions\n",
    "- Process orders and returns\n",
    "- Handle technical support\n",
    "- Manage account issues\n",
    "- Provide shipping information\n",
    "\n",
    "GUIDELINES:\n",
    "- Be professional and courteous\n",
    "- Keep responses under 3 sentences\n",
    "- Always verify customer identity for account changes\n",
    "- Escalate billing issues to finance team\n",
    "- Never share customer data with third parties\n",
    "\n",
    "PRODUCTS:\n",
    "- Laptop Pro X1: $1299, 16GB RAM, 512GB SSD\n",
    "- Tablet Mini: $499, 10-inch screen, 128GB\n",
    "- Wireless Earbuds: $199, noise cancelling\n",
    "\n",
    "POLICIES:\n",
    "- 30-day return policy\n",
    "- Free shipping over $50\n",
    "- 2-year warranty on electronics\n",
    "- Price matching within 14 days\n",
    "\n",
    "Customer question: {question}\"\"\"\n",
    "\n",
    "# Test with a simple question\n",
    "question = \"How much is the Laptop Pro X1?\"\n",
    "response = llm.invoke(monolithic_prompt.format(question=question))\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"\\nResponse with monolithic prompt:\")\n",
    "print(response.content)\n",
    "print(f\"\\nPrompt length: {len(monolithic_prompt.format(question=question))} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the monolithic approach in action:\n",
    "1. Defines a single large prompt containing all possible information including responsibilities, guidelines, product catalog, and policies.\n",
    "2. Formats the prompt with a simple product pricing question.\n",
    "3. Sends the entire prompt to the model even though most sections are irrelevant to this specific query.\n",
    "4. Displays both the response and the total character count, highlighting the inefficiency.\n",
    "\n",
    "The prompt contains 500+ characters of information when only about 100 characters (product catalog section) are actually needed to answer the question. This wastes tokens and makes the model process irrelevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modular components created successfully!\n",
      "\n",
      "Total modules: 5\n",
      "Identity module: 47 chars\n",
      "Communication module: 126 chars\n",
      "Product module: 150 chars\n",
      "Policy module: 128 chars\n",
      "Security module: 168 chars\n"
     ]
    }
   ],
   "source": [
    "# Module 1: Core identity - defines the agent's basic role\n",
    "IDENTITY_MODULE = \"\"\"You are a customer service agent for TechStore.\"\"\"\n",
    "\n",
    "# Module 2: Communication guidelines - defines interaction style\n",
    "COMMUNICATION_MODULE = \"\"\"Communication style:\n",
    "- Be professional and courteous\n",
    "- Keep responses concise and focused\n",
    "- Show empathy for customer concerns\"\"\"\n",
    "\n",
    "# Module 3: Product catalog - for product-related questions\n",
    "PRODUCT_CATALOG_MODULE = \"\"\"Product information:\n",
    "- Laptop Pro X1: $1299, 16GB RAM, 512GB SSD\n",
    "- Tablet Mini: $499, 10-inch screen, 128GB\n",
    "- Wireless Earbuds: $199, noise cancelling\"\"\"\n",
    "\n",
    "# Module 4: Store policies - for returns, shipping, warranty questions\n",
    "POLICY_MODULE = \"\"\"Store policies:\n",
    "- 30-day return policy\n",
    "- Free shipping over $50\n",
    "- 2-year warranty on electronics\n",
    "- Price matching within 14 days\"\"\"\n",
    "\n",
    "# Module 5: Security requirements - for account operations\n",
    "SECURITY_MODULE = \"\"\"Security requirements:\n",
    "- Always verify customer identity before account changes\n",
    "- Never share customer data with third parties\n",
    "- Escalate billing issues to finance team\"\"\"\n",
    "\n",
    "print(\"Modular components created successfully!\")\n",
    "print(f\"\\nTotal modules: 5\")\n",
    "print(f\"Identity module: {len(IDENTITY_MODULE)} chars\")\n",
    "print(f\"Communication module: {len(COMMUNICATION_MODULE)} chars\")\n",
    "print(f\"Product module: {len(PRODUCT_CATALOG_MODULE)} chars\")\n",
    "print(f\"Policy module: {len(POLICY_MODULE)} chars\")\n",
    "print(f\"Security module: {len(SECURITY_MODULE)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having broken down the monolithic prompt into modules, we can now compose task-specific prompts by selecting only the relevant pieces. This function demonstrates dynamic prompt composition based on task type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Task: product_question\n",
      "Question: How much is the Laptop Pro X1?\n",
      "============================================================\n",
      "Composed prompt (378 chars):\n",
      "You are a customer service agent for TechStore.\n",
      "\n",
      "Communication style:\n",
      "- Be professional and courteous\n",
      "- Keep responses concise and focused\n",
      "- Show empathy for customer concerns\n",
      "\n",
      "Product information:\n",
      "- Laptop Pro X1: $1299, 16GB RAM, 512GB SSD\n",
      "- Tablet Mini: $499, 10-inch screen, 128GB\n",
      "- Wireless Earbuds: $199, noise cancelling\n",
      "\n",
      "Customer question: How much is the Laptop Pro X1?\n",
      "\n",
      "Response:\n",
      "The Laptop Pro X1 is priced at $1,299. If you have any other questions or need further assistance, feel free to ask!\n",
      "\n",
      "============================================================\n",
      "Task: policy_question\n",
      "Question: What's your return policy?\n",
      "============================================================\n",
      "Composed prompt (352 chars):\n",
      "You are a customer service agent for TechStore.\n",
      "\n",
      "Communication style:\n",
      "- Be professional and courteous\n",
      "- Keep responses concise and focused\n",
      "- Show empathy for customer concerns\n",
      "\n",
      "Store policies:\n",
      "- 30-day return policy\n",
      "- Free shipping over $50\n",
      "- 2-year warranty on electronics\n",
      "- Price matching within 14 days\n",
      "\n",
      "Customer question: What's your return policy?\n",
      "\n",
      "Response:\n",
      "Thank you for your inquiry! Our return policy allows you to return items within 30 days of purchase for a full refund or exchange. If you have any further questions or need assistance with a return, please let me know!\n"
     ]
    }
   ],
   "source": [
    "def compose_prompt(task_type: str, question: str) -> str:\n",
    "    \"\"\"Compose a prompt by selecting relevant modules based on task type.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (product_question, policy_question, etc.)\n",
    "        question: Customer's question\n",
    "        \n",
    "    Returns:\n",
    "        Composed prompt string with only relevant modules\n",
    "    \"\"\"\n",
    "    \n",
    "    # All tasks get identity and communication modules (always needed)\n",
    "    modules = [IDENTITY_MODULE, COMMUNICATION_MODULE]\n",
    "    \n",
    "    # Add task-specific modules based on what the customer needs\n",
    "    if task_type == \"product_question\":\n",
    "        modules.append(PRODUCT_CATALOG_MODULE)\n",
    "    elif task_type == \"policy_question\":\n",
    "        modules.append(POLICY_MODULE)\n",
    "    elif task_type == \"account_change\":\n",
    "        modules.extend([SECURITY_MODULE, POLICY_MODULE])\n",
    "    elif task_type == \"purchase\":\n",
    "        modules.extend([PRODUCT_CATALOG_MODULE, POLICY_MODULE])\n",
    "    \n",
    "    # Compose final prompt by joining selected modules\n",
    "    prompt = \"\\n\\n\".join(modules)\n",
    "    prompt += f\"\\n\\nCustomer question: {question}\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test with different task types to demonstrate selective module loading\n",
    "test_cases = [\n",
    "    (\"product_question\", \"How much is the Laptop Pro X1?\"),\n",
    "    (\"policy_question\", \"What's your return policy?\"),\n",
    "]\n",
    "\n",
    "for task_type, question in test_cases:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task: {task_type}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Compose prompt with only necessary modules\n",
    "    composed_prompt = compose_prompt(task_type, question)\n",
    "    print(f\"Composed prompt ({len(composed_prompt)} chars):\")\n",
    "    print(composed_prompt)\n",
    "    \n",
    "    # Get response from model\n",
    "    response = llm.invoke(composed_prompt)\n",
    "    print(f\"\\nResponse:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Strategic few-shot examples\n",
    "\n",
    "Few-shot examples are one of the most powerful tools in prompt engineering. By showing the model concrete examples of desired behavior, we can guide it toward specific output formats, reasoning patterns, and quality standards without lengthy explanations. The key is using examples strategically - providing just enough to establish the pattern without cluttering the context window. Well-chosen examples teach the model through demonstration rather than prescription, making prompts both clearer and more token-efficient.\n",
    "\n",
    "The effectiveness of few-shot learning lies in its ability to show rather than tell. Instead of describing in detail what we want, we provide 2-3 representative examples that demonstrate the desired behavior. This is particularly valuable for complex formatting requirements, domain-specific reasoning patterns and edge case handling where natural language descriptions would be verbose or ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot response:\n",
      "Product Name: Laptop Pro X1  \n",
      "Price: $1299  \n",
      "Category: Computers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot: No examples provided, relying on model's general capabilities\n",
    "zero_shot_prompt = \"\"\"Extract the product name, price, and category from this text.\n",
    "\n",
    "Text: The Laptop Pro X1 costs $1299 and is in the computers category.\n",
    "\n",
    "Output:\"\"\"\n",
    "\n",
    "# Invoke the model without any examples to guide it\n",
    "response = llm.invoke(zero_shot_prompt)\n",
    "print(\"Zero-shot response:\")\n",
    "print(response.content)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero-shot approach often produces reasonable results, but we have limited control over the output format. The model might choose any structure - natural language sentences, bullet points or some other format. For production systems requiring consistent, parseable outputs, this variability is problematic. Few-shot examples solve this by establishing a clear format through demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot response:\n",
      "{\"product\": \"Laptop Pro X1\", \"price\": 1299.00, \"category\": \"computers\"}\n"
     ]
    }
   ],
   "source": [
    "# Few-shot: Include examples of desired behavior to establish format\n",
    "few_shot_prompt = \"\"\"Extract the product name, price, and category from text. Format as JSON.\n",
    "\n",
    "Example 1:\n",
    "Text: The Wireless Mouse is priced at $29.99 and belongs to accessories.\n",
    "Output: {\"product\": \"Wireless Mouse\", \"price\": 29.99, \"category\": \"accessories\"}\n",
    "\n",
    "Example 2:\n",
    "Text: Get the 4K Monitor for only $399 in the displays section.\n",
    "Output: {\"product\": \"4K Monitor\", \"price\": 399.00, \"category\": \"displays\"}\n",
    "\n",
    "Now extract from this:\n",
    "Text: The Laptop Pro X1 costs $1299 and is in the computers category.\n",
    "\n",
    "Output:\"\"\"\n",
    "\n",
    "# Invoke with examples guiding the output format\n",
    "response = llm.invoke(few_shot_prompt)\n",
    "print(\"Few-shot response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model consistently returns well-formatted JSON that can be parsed programmatically. This demonstrates how few-shot examples guide not just the content but also the structure of outputs. Two examples were sufficient to establish the pattern - showing the value of quality over quantity in few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning with few-shot examples:\n",
      "To determine if the customer is eligible for a refund based on the 30-day policy, we need to calculate the number of days elapsed between the purchase date and the refund request date.\n",
      "\n",
      "**Purchase date:** 2024-01-20  \n",
      "**Refund request date:** 2024-02-25  \n",
      "\n",
      "Now, let's calculate the days elapsed:\n",
      "\n",
      "1. From January 20 to January 31: 11 days (31 - 20 = 11)\n",
      "2. From February 1 to February 25: 25 days\n",
      "\n",
      "Now, we add these two periods together:\n",
      "\n",
      "11 days (January) + 25 days (February) = 36 days elapsed.\n",
      "\n",
      "Since 36 days exceeds the 30-day refund policy, the decision is:\n",
      "\n",
      "**Decision: DENIED - Exceeds 30-day policy**\n"
     ]
    }
   ],
   "source": [
    "# Few-shot for reasoning patterns - show how to approach the problem\n",
    "reasoning_prompt = \"\"\"Determine if a customer is eligible for a refund based on our 30-day policy.\n",
    "\n",
    "Example 1:\n",
    "Purchase date: 2024-01-15\n",
    "Refund request date: 2024-02-01\n",
    "Days elapsed: 17 days\n",
    "Decision: APPROVED - Within 30-day window\n",
    "\n",
    "Example 2:\n",
    "Purchase date: 2024-01-01  \n",
    "Refund request date: 2024-02-15\n",
    "Days elapsed: 45 days\n",
    "Decision: DENIED - Exceeds 30-day policy\n",
    "\n",
    "Example 3:\n",
    "Purchase date: 2024-02-05\n",
    "Refund request date: 2024-02-29\n",
    "Days elapsed: 24 days\n",
    "Decision: APPROVED - Within 30-day window\n",
    "\n",
    "Now evaluate this case:\n",
    "Purchase date: 2024-01-20\n",
    "Refund request date: 2024-02-25\n",
    "\n",
    "Analysis:\"\"\"\n",
    "\n",
    "# Invoke with examples demonstrating the reasoning process\n",
    "response = llm.invoke(reasoning_prompt)\n",
    "print(\"Reasoning with few-shot examples:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices for few-shot examples\n",
    "\n",
    "The quality and selection of few-shot examples significantly impact their effectiveness. Rather than providing many examples, focus on 2-3 high-quality ones that cover the most important patterns. Include edge cases that might cause confusion, maintain consistent formatting across all examples, and ensure examples represent the diversity of scenarios the agent will encounter. This strategic approach maximizes learning while minimizing token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis with few-shot examples:\n",
      "Feedback: \"The customer service was amazing but the product quality is disappointing.\"\n",
      "Sentiment: Mixed\n",
      "Category: Customer Service and Product Quality\n",
      "Action: Address product quality issues and acknowledge positive customer service feedback\n"
     ]
    }
   ],
   "source": [
    "# Creating a few-shot module for sentiment analysis with action recommendations\n",
    "FEW_SHOT_SENTIMENT = \"\"\"Analyze customer feedback sentiment and categorize the issue.\n",
    "\n",
    "Example 1:\n",
    "Feedback: \"The laptop arrived quickly and works great! Very happy with my purchase.\"\n",
    "Sentiment: Positive\n",
    "Category: Product Quality\n",
    "Action: None needed\n",
    "\n",
    "Example 2:\n",
    "Feedback: \"Shipping took forever and the box was damaged. The product is fine though.\"\n",
    "Sentiment: Mixed\n",
    "Category: Shipping\n",
    "Action: Review shipping partner\n",
    "\n",
    "Example 3:\n",
    "Feedback: \"This is the worst purchase I've ever made. It broke after 2 days!\"\n",
    "Sentiment: Negative\n",
    "Category: Product Defect\n",
    "Action: Immediate replacement + apology\n",
    "\"\"\"\n",
    "\n",
    "# Test the few-shot pattern with new feedback\n",
    "test_feedback = \"The customer service was amazing but the product quality is disappointing.\"\n",
    "prompt = FEW_SHOT_SENTIMENT + f\"\\nNew feedback: {test_feedback}\\n\\nAnalysis:\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(\"Sentiment analysis with few-shot examples:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Altitude-appropriate prompts\n",
    "\n",
    "Prompt \"altitude\" refers to the level of detail and specificity in our instructions. Finding the right altitude is crucial: too high (vague) and the agent lacks direction, producing inconsistent outputs; too low (prescriptive) and the agent becomes rigid, unable to adapt to varying contexts. The optimal altitude depends on task complexity, the model's capabilities, the acceptable level of output variance, and the consequences of errors.\n",
    "\n",
    "The goal is to provide enough structure to ensure reliable behavior without over-constraining the agent. High-stakes operations like security verification require lower altitude with detailed step-by-step instructions. Creative tasks or domains where the model has strong knowledge benefit from higher altitude, allowing natural adaptation to context. Most production systems need balanced altitude—clear guidance on what matters while preserving flexibility in execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too vague prompt response:\n",
      "Of course! I can help you with that. Could you please provide me with your order number and the reason for the return? Additionally, let me know if the laptop is still in its original packaging and if you have all the accessories that came with it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Too vague - gives model excessive freedom without clear direction\n",
    "vague_prompt = \"Help the customer.\"\n",
    "\n",
    "question = \"I want to return my laptop.\"\n",
    "response = llm.invoke(f\"{vague_prompt}\\n\\nCustomer: {question}\\n\\nAgent:\")\n",
    "print(\"Too vague prompt response:\")\n",
    "print(response.content)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the vague prompt might produce helpful responses, we sacrifice control over critical elements like tone consistency, which information to request, and which policies to mention. Different runs might produce widely varying responses, making the system unpredictable and difficult to test or maintain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too prescriptive prompt response:\n",
      "I'd be happy to help with your return. Could you please provide your order number?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Too prescriptive - micromanages every detail and interaction step\n",
    "prescriptive_prompt = \"\"\"You are a returns specialist. When a customer wants to return something:\n",
    "1. First, greet them warmly using exactly these words: \"I'd be happy to help with your return.\"\n",
    "2. Then ask for their order number using this exact phrasing: \"Could you please provide your order number?\"\n",
    "3. After they provide it, say: \"Thank you for providing that information.\"\n",
    "4. Then ask: \"May I know the reason for the return?\"\n",
    "5. After they answer, explain the 30-day policy using these words: \"Our return policy allows returns within 30 days of purchase.\"\n",
    "6. Then ask: \"When did you receive the item?\"\n",
    "7. Calculate the days and inform them if eligible.\n",
    "8. End with: \"Is there anything else I can help you with today?\"\n",
    "\n",
    "Follow these steps exactly in this order.\"\"\"\n",
    "\n",
    "question = \"I want to return my laptop.\"\n",
    "response = llm.invoke(f\"{prescriptive_prompt}\\n\\nCustomer: {question}\\n\\nAgent:\")\n",
    "print(\"Too prescriptive prompt response:\")\n",
    "print(response.content)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This level of prescription creates robotic, inflexible interactions. The agent cannot adapt to context, handle interruptions gracefully, or provide a natural conversational experience. It's also token-inefficient, using many words to specify details that could be left to the model's judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appropriate altitude prompt response:\n",
      "Hello! Thank you for reaching out to us about your return request. I’m here to help you with that. Could you please provide me with your order number and the purchase date of your laptop? \n",
      "\n",
      "Just to let you know, we have a 30-day return policy, so I’ll verify your eligibility based on the purchase date once I have that information. Looking forward to assisting you further!\n"
     ]
    }
   ],
   "source": [
    "# Appropriate altitude - clear guidance with flexibility for natural adaptation\n",
    "balanced_prompt = \"\"\"You are a returns specialist for TechStore.\n",
    "\n",
    "When handling return requests:\n",
    "- Acknowledge the request warmly\n",
    "- Ask for order number and purchase date\n",
    "- Explain our 30-day return policy\n",
    "- Verify eligibility based on purchase date\n",
    "- Provide next steps if approved, or alternatives if denied\n",
    "\n",
    "Maintain a helpful, professional tone throughout.\"\"\"\n",
    "\n",
    "question = \"I want to return my laptop.\"\n",
    "response = llm.invoke(f\"{balanced_prompt}\\n\\nCustomer: {question}\\n\\nAgent:\")\n",
    "print(\"Appropriate altitude prompt response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines for choosing the right altitude\n",
    "\n",
    "The decision between high and low altitude prompts depends on specific requirements and constraints. Use lower altitude with more prescriptive detail when output format is critical for API integration or structured data processing, when compliance or legal requirements must be met exactly, when errors carry high consequences, or when the agent is operating in an unfamiliar domain. Conversely, use higher altitude with less prescriptive detail when creative or varied responses add value, when the agent needs to adapt dynamically to diverse contexts, when the domain is well within the model's training, or when variety in tone and approach is desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low altitude (prescriptive) for security:\n",
      "You are a security verification agent.\n",
      "\n",
      "Before any account changes, you MUST:\n",
      "1. Verify identity using exactly two factors:\n",
      "   - Last 4 digits of registered phone number\n",
      "   - Account holder's date of birth\n",
      "2. Ask each question separately, wait for response\n",
      "3. Do NOT proceed unless both match our records\n",
      "4. If verification fails, immediately escalate to security team\n",
      "5. Never share what information we have on file\n",
      "\n",
      "Do not deviate from this protocol.\n",
      "\n",
      "============================================================\n",
      "\n",
      "High altitude (flexible) for general chat:\n",
      "You are a friendly shopping assistant.\n",
      "\n",
      "Help customers discover products they'll love by:\n",
      "- Asking about their needs and preferences\n",
      "- Suggesting relevant products\n",
      "- Answering questions enthusiastically\n",
      "\n",
      "Be conversational and adapt to their shopping style.\n"
     ]
    }
   ],
   "source": [
    "# Example: Choosing altitude based on task requirements and risk profile\n",
    "\n",
    "# High-stakes task: Lower altitude (more prescriptive) for security operations\n",
    "security_prompt = \"\"\"You are a security verification agent.\n",
    "\n",
    "Before any account changes, you MUST:\n",
    "1. Verify identity using exactly two factors:\n",
    "   - Last 4 digits of registered phone number\n",
    "   - Account holder's date of birth\n",
    "2. Ask each question separately, wait for response\n",
    "3. Do NOT proceed unless both match our records\n",
    "4. If verification fails, immediately escalate to security team\n",
    "5. Never share what information we have on file\n",
    "\n",
    "Do not deviate from this protocol.\"\"\"\n",
    "\n",
    "print(\"Low altitude (prescriptive) for security:\")\n",
    "print(security_prompt)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Low-stakes task: Higher altitude (more flexible) for general assistance\n",
    "chat_prompt = \"\"\"You are a friendly shopping assistant.\n",
    "\n",
    "Help customers discover products they'll love by:\n",
    "- Asking about their needs and preferences\n",
    "- Suggesting relevant products\n",
    "- Answering questions enthusiastically\n",
    "\n",
    "Be conversational and adapt to their shopping style.\"\"\"\n",
    "\n",
    "print(\"High altitude (flexible) for general chat:\")\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Structured delimiters\n",
    "\n",
    "Structured delimiters organize information within prompts, making complex content easier for both models and humans to parse. Without clear delimiters, prompts become walls of text where critical information blends into surrounding context. Delimiters create visual and logical boundaries that help models identify sections, prioritize information, and understand relationships between different parts of the prompt.\n",
    "\n",
    "Common delimiter approaches include section headers using markdown or symbols, XML-style tags that create clear hierarchical structure, and simple labels that mark distinct information types. The choice of delimiter style should match the complexity of your content - simple labels work well for flat structures, markdown excels at human readability, and XML-style tags shine for complex nested information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without delimiters:\n",
      "I'm sorry to hear that your laptop isn't starting. Let's go through some steps to troubleshoot the issue. \n",
      "\n",
      "First, can you please check if your laptop is plugged in? Make sure the power adapter is connected to both the laptop and a working electrical outlet. What do you see?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unstructured prompt - difficult to parse and locate specific information\n",
    "unstructured = \"\"\"You are a tech support agent. The customer has a laptop that won't turn on. \n",
    "First check if it's plugged in. Then check the battery. Then check for indicator lights. \n",
    "Our laptops have LED indicators: green means charging, amber means battery low, red means \n",
    "hardware issue, no light means no power. Always ask one question at a time. Be patient and \n",
    "clear. Never assume the customer has technical knowledge.\n",
    "\n",
    "Customer says: My laptop won't start\"\"\"\n",
    "\n",
    "response = llm.invoke(unstructured)\n",
    "print(\"Response without delimiters:\")\n",
    "print(response.content)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unstructured prompt contains all necessary information but presents it as a continuous block of text. This makes it difficult to scan quickly, hard to identify where one concept ends and another begins, and challenging to update specific sections without accidentally affecting others. Structured delimiters solve these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with clear delimiters:\n",
      "I'm sorry to hear that your laptop won't start. Let's try to troubleshoot the issue together. \n",
      "\n",
      "First, can you please check if your laptop is plugged into a power source?\n"
     ]
    }
   ],
   "source": [
    "# Well-structured prompt with clear section delimiters\n",
    "structured = \"\"\"### ROLE\n",
    "You are a tech support agent helping customers with laptop issues.\n",
    "\n",
    "### TROUBLESHOOTING PROCEDURE\n",
    "1. Check if laptop is plugged into power\n",
    "2. Check battery status\n",
    "3. Check LED indicator lights\n",
    "\n",
    "### LED INDICATORS\n",
    "- Green: Charging normally\n",
    "- Amber: Battery low\n",
    "- Red: Hardware issue detected\n",
    "- No light: No power received\n",
    "\n",
    "### INTERACTION GUIDELINES\n",
    "- Ask one question at a time\n",
    "- Be patient and clear\n",
    "- Don't assume technical knowledge\n",
    "\n",
    "### CUSTOMER ISSUE\n",
    "\"My laptop won't start\"\n",
    "\n",
    "### YOUR RESPONSE\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(structured)\n",
    "print(\"Response with clear delimiters:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structured version organizes identical information into clearly labeled sections. Models can quickly locate relevant information, humans can update specific sections without errors, and the logical flow from role to procedure to guidelines becomes immediately apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML-style delimiters:\n",
      "<role>\n",
      "Customer service agent for TechStore\n",
      "</role>\n",
      "\n",
      "<context>\n",
      "Customer purchased Laptop Pro X1 30 days ago for $1299\n",
      "</context>\n",
      "\n",
      "<instructions>\n",
      "- Determine refund eligibility\n",
      "- Explain decision clearly\n",
      "- Offer alternatives if denied\n",
      "</instructions>\n",
      "\n",
      "<customer_message>\n",
      "I'd like to return this laptop.\n",
      "</customer_message>\n",
      "\n",
      "<response>\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Markdown-style delimiters:\n",
      "# Role\n",
      "Customer service agent for TechStore\n",
      "\n",
      "## Context\n",
      "Customer purchased Laptop Pro X1 30 days ago for $1299\n",
      "\n",
      "## Instructions\n",
      "- Determine refund eligibility\n",
      "- Explain decision clearly  \n",
      "- Offer alternatives if denied\n",
      "\n",
      "## Customer Message\n",
      "\"I'd like to return this laptop.\"\n",
      "\n",
      "## Your Response\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Label-style delimiters:\n",
      "ROLE: Customer service agent for TechStore\n",
      "\n",
      "CONTEXT: Customer purchased Laptop Pro X1 30 days ago for $1299\n",
      "\n",
      "INSTRUCTIONS:\n",
      "- Determine refund eligibility\n",
      "- Explain decision clearly\n",
      "- Offer alternatives if denied\n",
      "\n",
      "CUSTOMER: \"I'd like to return this laptop.\"\n",
      "\n",
      "AGENT:\n"
     ]
    }
   ],
   "source": [
    "# Style 1: XML-style tags provide explicit structure\n",
    "xml_style = \"\"\"<role>\n",
    "Customer service agent for TechStore\n",
    "</role>\n",
    "\n",
    "<context>\n",
    "Customer purchased Laptop Pro X1 30 days ago for $1299\n",
    "</context>\n",
    "\n",
    "<instructions>\n",
    "- Determine refund eligibility\n",
    "- Explain decision clearly\n",
    "- Offer alternatives if denied\n",
    "</instructions>\n",
    "\n",
    "<customer_message>\n",
    "I'd like to return this laptop.\n",
    "</customer_message>\n",
    "\n",
    "<response>\n",
    "\"\"\"\n",
    "\n",
    "print(\"XML-style delimiters:\")\n",
    "print(xml_style)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Style 2: Markdown headers for readability\n",
    "markdown_style = \"\"\"# Role\n",
    "Customer service agent for TechStore\n",
    "\n",
    "## Context\n",
    "Customer purchased Laptop Pro X1 30 days ago for $1299\n",
    "\n",
    "## Instructions\n",
    "- Determine refund eligibility\n",
    "- Explain decision clearly  \n",
    "- Offer alternatives if denied\n",
    "\n",
    "## Customer Message\n",
    "\"I'd like to return this laptop.\"\n",
    "\n",
    "## Your Response\n",
    "\"\"\"\n",
    "\n",
    "print(\"Markdown-style delimiters:\")\n",
    "print(markdown_style)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Style 3: Simple labels for straightforward content\n",
    "label_style = \"\"\"ROLE: Customer service agent for TechStore\n",
    "\n",
    "CONTEXT: Customer purchased Laptop Pro X1 30 days ago for $1299\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Determine refund eligibility\n",
    "- Explain decision clearly\n",
    "- Offer alternatives if denied\n",
    "\n",
    "CUSTOMER: \"I'd like to return this laptop.\"\n",
    "\n",
    "AGENT:\"\"\"\n",
    "\n",
    "print(\"Label-style delimiters:\")\n",
    "print(label_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three styles effectively organize the same information:\n",
    "1. XML tags work best for complex nested structures where hierarchy matters.\n",
    "2. Markdown headers excel at human readability and are familiar to most developers.\n",
    "3. Simple labels provide the most concise approach for flat, straightforward structures.\n",
    "\n",
    "Choose based on your content complexity and team preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive structured prompt response:\n",
      "\"Your order #11111 has been successfully delivered! It was signed for by John Doe on February 18, 2024. Thank you for shopping with us!\"\n"
     ]
    }
   ],
   "source": [
    "# Well-structured prompt combining delimiters with few-shot examples\n",
    "comprehensive_prompt = \"\"\"### ROLE\n",
    "Order status assistant for TechStore\n",
    "\n",
    "### TASK\n",
    "Parse order status updates and provide customer-friendly summaries.\n",
    "\n",
    "### EXAMPLES\n",
    "\n",
    "Example 1:\n",
    "---\n",
    "Input: {\"order_id\": \"12345\", \"status\": \"shipped\", \"tracking\": \"ABC123\", \"eta\": \"2024-02-20\"}\n",
    "Output: \"Great news! Your order #12345 has shipped and is on its way. Track it with ABC123. Expected delivery: February 20, 2024.\"\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "---\n",
    "Input: {\"order_id\": \"67890\", \"status\": \"delayed\", \"reason\": \"weather\", \"new_eta\": \"2024-02-25\"}\n",
    "Output: \"We apologize for the delay on order #67890. Due to weather conditions, your new expected delivery is February 25, 2024.\"\n",
    "---\n",
    "\n",
    "### NEW INPUT\n",
    "{\"order_id\": \"11111\", \"status\": \"delivered\", \"signed_by\": \"John Doe\", \"date\": \"2024-02-18\"}\n",
    "\n",
    "### YOUR OUTPUT\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(comprehensive_prompt)\n",
    "print(\"Comprehensive structured prompt response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Production prompts require combining multiple best practices into a cohesive system. Rather than choosing a single technique, effective prompt engineering layers composable modules, strategic examples, appropriate altitude and structured delimiters to create prompts that are both powerful and maintainable. This integrated approach ensures prompts remain clear as complexity grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      "============================================================\n",
      "You are a customer service agent for TechStore.\n",
      "\n",
      "### COMMUNICATION STYLE\n",
      "- Professional and courteous\n",
      "- Clear and concise\n",
      "- Empathetic to customer needs\n",
      "\n",
      "### POLICIES\n",
      "- 30-day return policy\n",
      "- Free shipping over $50\n",
      "- 2-year warranty on electronics\n",
      "\n",
      "### EXAMPLES\n",
      "\n",
      "Example 1:\n",
      "Customer: \"I bought this 20 days ago and want a refund.\"\n",
      "Agent: \"I'd be happy to help with your refund. Since you're within our 30-day window, you're eligible. I'll need your order number to process this.\"\n",
      "\n",
      "Example 2:\n",
      "Customer: \"I need to return something I bought 40 days ago.\"\n",
      "Agent: \"I understand you'd like a return. Unfortunately, this falls outside our 30-day policy. However, I can check if the warranty covers your issue or offer store credit.\"\n",
      "\n",
      "\n",
      "### CUSTOMER MESSAGE\n",
      "I bought a laptop 25 days ago and want to return it.\n",
      "\n",
      "### YOUR RESPONSE\n",
      "============================================================\n",
      "\n",
      "Agent Response:\n",
      "I'd be happy to assist you with your return. Since you're within our 30-day return policy, you're eligible for a refund. Please provide your order number, and I'll get the process started for you.\n"
     ]
    }
   ],
   "source": [
    "class PromptBuilder:\n",
    "    \"\"\"Build production-ready prompts using best practices.\n",
    "    \n",
    "    This class demonstrates how to combine multiple prompt engineering techniques:\n",
    "    - Composable modules for token efficiency\n",
    "    - Few-shot examples for format guidance\n",
    "    - Altitude-appropriate instructions\n",
    "    - Structured delimiters for clarity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Core reusable modules\n",
    "        self.modules = {\n",
    "            \"identity\": \"You are a customer service agent for TechStore.\",\n",
    "            \"communication\": \"\"\"### COMMUNICATION STYLE\n",
    "- Professional and courteous\n",
    "- Clear and concise\n",
    "- Empathetic to customer needs\"\"\",\n",
    "            \"products\": \"\"\"### PRODUCTS\n",
    "- Laptop Pro X1: $1299, 16GB RAM, 512GB SSD\n",
    "- Tablet Mini: $499, 10-inch screen, 128GB\n",
    "- Wireless Earbuds: $199, noise cancelling\"\"\",\n",
    "            \"policies\": \"\"\"### POLICIES\n",
    "- 30-day return policy\n",
    "- Free shipping over $50\n",
    "- 2-year warranty on electronics\"\"\"\n",
    "        }\n",
    "        \n",
    "        # Few-shot examples organized by task type\n",
    "        self.examples = {\n",
    "            \"refund\": \"\"\"### EXAMPLES\n",
    "\n",
    "Example 1:\n",
    "Customer: \"I bought this 20 days ago and want a refund.\"\n",
    "Agent: \"I'd be happy to help with your refund. Since you're within our 30-day window, you're eligible. I'll need your order number to process this.\"\n",
    "\n",
    "Example 2:\n",
    "Customer: \"I need to return something I bought 40 days ago.\"\n",
    "Agent: \"I understand you'd like a return. Unfortunately, this falls outside our 30-day policy. However, I can check if the warranty covers your issue or offer store credit.\"\n",
    "\"\"\",\n",
    "            \"product_info\": \"\"\"### EXAMPLES\n",
    "\n",
    "Example:\n",
    "Customer: \"Tell me about the Laptop Pro X1.\"\n",
    "Agent: \"The Laptop Pro X1 is $1299 and features 16GB RAM with 512GB SSD storage. It includes our 2-year warranty and qualifies for free shipping.\"\n",
    "\"\"\"\n",
    "        }\n",
    "    \n",
    "    def build_prompt(self, task_type: str, customer_message: str, \n",
    "                    include_examples: bool = True, altitude: str = \"balanced\") -> str:\n",
    "        \"\"\"Build a prompt using composable modules and best practices.\n",
    "        \n",
    "        Args:\n",
    "            task_type: Type of task (refund, product_info, etc.)\n",
    "            customer_message: The customer's message to respond to\n",
    "            include_examples: Whether to include few-shot examples\n",
    "            altitude: Instruction detail level (prescriptive, balanced, flexible)\n",
    "            \n",
    "        Returns:\n",
    "            Complete prompt string ready for the model\n",
    "        \"\"\"\n",
    "        \n",
    "        sections = []\n",
    "        \n",
    "        # 1. Identity (always included)\n",
    "        sections.append(self.modules[\"identity\"])\n",
    "        \n",
    "        # 2. Communication style (always included)\n",
    "        sections.append(self.modules[\"communication\"])\n",
    "        \n",
    "        # 3. Task-specific modules\n",
    "        if task_type in [\"product_info\", \"purchase\"]:\n",
    "            sections.append(self.modules[\"products\"])\n",
    "        \n",
    "        if task_type in [\"refund\", \"policy\", \"purchase\"]:\n",
    "            sections.append(self.modules[\"policies\"])\n",
    "        \n",
    "        # 4. Few-shot examples (if requested and available)\n",
    "        if include_examples and task_type in self.examples:\n",
    "            sections.append(self.examples[task_type])\n",
    "        \n",
    "        # 5. Instructions (altitude-appropriate)\n",
    "        if altitude == \"prescriptive\":\n",
    "            instructions = \"\"\"### INSTRUCTIONS\n",
    "1. Read the customer message carefully\n",
    "2. Identify the specific need\n",
    "3. Check relevant policies or product info\n",
    "4. Formulate a response that addresses the need\n",
    "5. Keep response under 3 sentences\"\"\"\n",
    "            sections.append(instructions)\n",
    "        elif altitude == \"flexible\":\n",
    "            instructions = \"\"\"### INSTRUCTIONS\n",
    "Address the customer's needs appropriately.\"\"\"\n",
    "            sections.append(instructions)\n",
    "        # balanced altitude adds no extra instructions\n",
    "        \n",
    "        # 6. Customer message with clear delimiter\n",
    "        sections.append(f\"\"\"### CUSTOMER MESSAGE\n",
    "{customer_message}\n",
    "\n",
    "### YOUR RESPONSE\"\"\")\n",
    "        \n",
    "        return \"\\n\\n\".join(sections)\n",
    "\n",
    "# Test the prompt builder with a real scenario\n",
    "builder = PromptBuilder()\n",
    "\n",
    "# Test case: Refund request\n",
    "prompt1 = builder.build_prompt(\n",
    "    task_type=\"refund\",\n",
    "    customer_message=\"I bought a laptop 25 days ago and want to return it.\",\n",
    "    include_examples=True,\n",
    "    altitude=\"balanced\"\n",
    ")\n",
    "\n",
    "print(\"Generated Prompt:\")\n",
    "print(\"=\"*60)\n",
    "print(prompt1)\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = llm.invoke(prompt1)\n",
    "print(\"\\nAgent Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying these techniques systematically, we create prompts that are token-efficient, guide models effectively toward desired outputs, remain maintainable as systems grow in complexity and produce consistent high-quality results. This foundation is essential for production AI systems where prompt quality directly impacts agent performance and reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
