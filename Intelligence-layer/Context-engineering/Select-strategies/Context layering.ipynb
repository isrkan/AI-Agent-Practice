{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context layering\n",
    "\n",
    "Context windows are precious and finite, yet not all information deserves equal priority. Some content is absolutely critical - system instructions that define agent behavior, current task objectives, user authentication state. Other content is highly valuable but not strictly essential - recent conversation history, relevant memories, helpful background knowledge. Still other content is nice-to-have but expendable - older conversation turns, tangentially related documentation, supplementary examples. The challenge is organizing this hierarchy of importance so that when context space runs low, we preserve what matters most.\n",
    "\n",
    "Context layering addresses this challenge by organizing information into priority-based tiers, each with different treatment under space constraints. Layer 0 contains immutable critical content that must always be present. Layer 1 holds important information that should be included when possible but can be summarized if needed. Layer 2 contains supplementary content that is included opportunistically when space permits but dropped entirely under tight constraints. This stratification ensures graceful degradation - as context budgets tighten, we shed lower-priority layers while preserving the foundation.\n",
    "\n",
    "In this notebook, we explore how to implement context layering as a sophisticated select strategy for context engineering. We will examine how to define layer priorities and assign content appropriately, how to implement layer-aware context building with space constraints, how to compress or drop lower layers when budgets are tight, how to ensure critical information always survives, and how to build production-ready layered context systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by initializing the language model that will work with our layered context. Consistent model configuration ensures reproducible behavior across different layering strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model for generating responses\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(),\n",
    "    temperature=0  # Set to 0 for more deterministic outputs\n",
    ")\n",
    "\n",
    "print(\"Language model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining context layers and priorities\n",
    "\n",
    "Effective context layering begins with a clear taxonomy of layer priorities and guidelines for what content belongs in each tier. The layer structure should reflect our system's non-negotiable requirements versus nice-to-have enhancements. Layer 0 typically contains system instructions and current task context that define the agent's behavior and purpose. Layer 1 holds important supporting information like recent conversation history and key user preferences. Layer 2 contains supplementary material like background knowledge and older interactions.\n",
    "\n",
    "We will define a LayerPriority enumeration and a ContextLayer class that encapsulates content with its assigned priority, token budget, and compression options. This structure enables systematic layer-aware context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Layer Structure\n",
      "======================================================================\n",
      "\n",
      "CRITICAL (Priority 0):\n",
      "  Layers: 2\n",
      "  Total tokens: ~56\n",
      "  Content:\n",
      "    - System Instructions (~41 tokens, fixed)\n",
      "    - Current Task (~15 tokens, fixed)\n",
      "\n",
      "IMPORTANT (Priority 1):\n",
      "  Layers: 2\n",
      "  Total tokens: ~69\n",
      "  Content:\n",
      "    - Recent Conversation (~49 tokens, compressible)\n",
      "    - Key Preferences (~20 tokens, compressible)\n",
      "\n",
      "SUPPLEMENTARY (Priority 2):\n",
      "  Layers: 3\n",
      "  Total tokens: ~131\n",
      "  Content:\n",
      "    - Product Documentation (~60 tokens, compressible)\n",
      "    - Return Policy (~38 tokens, compressible)\n",
      "    - Earlier Conversation (~33 tokens, compressible)\n",
      "\n",
      "Total context size: ~256 tokens\n"
     ]
    }
   ],
   "source": [
    "class LayerPriority(Enum):\n",
    "    \"\"\"Priority levels for context layers.\"\"\"\n",
    "    CRITICAL = 0  # Must always be included, never compressed\n",
    "    IMPORTANT = 1  # Should be included, can be summarized if needed\n",
    "    SUPPLEMENTARY = 2  # Nice to have, dropped first under constraints\n",
    "\n",
    "@dataclass\n",
    "class ContextLayer:\n",
    "    \"\"\"A layer of context with priority and content.\"\"\"\n",
    "    \n",
    "    priority: LayerPriority  # Layer's priority level (CRITICAL, IMPORTANT, or SUPPLEMENTARY)\n",
    "    name: str  # Name for this layer (e.g., \"System Instructions\")\n",
    "    content: str  # The actual text content stored in this layer\n",
    "    compressible: bool = False  # Can this layer be summarized?\n",
    "    \n",
    "    def estimate_tokens(self) -> int:\n",
    "        \"\"\"Estimate token count for this layer's content.\n",
    "        \n",
    "        Returns:\n",
    "            Estimated tokens (rough approximation)\n",
    "        \"\"\"\n",
    "        # Split by whitespace to count words\n",
    "        word_count = len(self.content.split())\n",
    "        # Rough estimate: ~0.75 tokens per word\n",
    "        return int(word_count * 0.75)\n",
    "    \n",
    "    def compress(self, target_length: int) -> str:\n",
    "        \"\"\"Create compressed version of content.\n",
    "        \n",
    "        Args:\n",
    "            target_length: Target character length\n",
    "            \n",
    "        Returns:\n",
    "            Compressed content string\n",
    "        \"\"\"\n",
    "        # Don't compress if layer is marked as non-compressible or already shorter than target\n",
    "        if not self.compressible or len(self.content) <= target_length:\n",
    "            return self.content\n",
    "        \n",
    "        # Simple truncation with ellipsis (production would use smarter summarization)\n",
    "        return self.content[:target_length-3] + \"...\"\n",
    "\n",
    "# Define example context layers for a customer service agent\n",
    "def create_sample_layers() -> List[ContextLayer]:\n",
    "    \"\"\"Create sample context layers demonstrating the hierarchy.\n",
    "    \n",
    "    Returns:\n",
    "        List of context layers with different priorities\n",
    "    \"\"\"\n",
    "    # This creates a realistic example of how a customer service chatbot would organize its context into different priority layers\n",
    "    return [\n",
    "        # Layer 0: Critical - System instructions\n",
    "        ContextLayer(\n",
    "            priority=LayerPriority.CRITICAL,\n",
    "            name=\"System Instructions\",\n",
    "            content=\"\"\"You are a helpful customer service agent for TechStore.\n",
    "\n",
    "Core responsibilities:\n",
    "- Answer product questions accurately\n",
    "- Help with orders and returns\n",
    "- Provide technical support\n",
    "- Maintain professional, friendly tone\n",
    "\n",
    "Critical rules:\n",
    "- Never share customer data with unauthorized parties\n",
    "- Always verify identity before account changes\n",
    "- Escalate refunds over $500 to manager\"\"\",\n",
    "            compressible=False\n",
    "        ),\n",
    "        \n",
    "        # Layer 0: Critical - Current task\n",
    "        ContextLayer(\n",
    "            priority=LayerPriority.CRITICAL,\n",
    "            name=\"Current Task\",\n",
    "            content=\"\"\"Current customer request: Help with laptop overheating issue\n",
    "Customer: Premium member, ID #12345\n",
    "Product: Laptop Pro X1 purchased 45 days ago\"\"\",\n",
    "            compressible=False\n",
    "        ),\n",
    "        \n",
    "        # Layer 1: Important - Recent conversation\n",
    "        ContextLayer(\n",
    "            priority=LayerPriority.IMPORTANT,\n",
    "            name=\"Recent Conversation\",\n",
    "            content=\"\"\"Last 3 exchanges:\n",
    "\n",
    "Customer: My laptop gets really hot when I'm gaming.\n",
    "Agent: I understand that's frustrating. Let me help you troubleshoot this.\n",
    "\n",
    "Customer: It's been happening for about a week now.\n",
    "Agent: Thank you for that detail. Has anything changed recently—new software, updates, or room temperature?\n",
    "\n",
    "Customer: I did install some new games last week.\n",
    "Agent: That could be related. Let's check a few things.\"\"\",\n",
    "            compressible=True\n",
    "        ),\n",
    "        \n",
    "        # Layer 1: Important - Key user preferences\n",
    "        ContextLayer(\n",
    "            priority=LayerPriority.IMPORTANT,\n",
    "            name=\"Key Preferences\",\n",
    "            content=\"\"\"User preferences:\n",
    "- Prefers detailed technical explanations\n",
    "- Wants to try DIY solutions before returns\n",
    "- Previous positive experience with cooling pad recommendation\n",
    "- Values quick resolution\"\"\",\n",
    "            compressible=True\n",
    "        ),\n",
    "        \n",
    "        # Layer 2: Supplementary - Product documentation\n",
    "        ContextLayer(\n",
    "            priority=LayerPriority.SUPPLEMENTARY,\n",
    "            name=\"Product Documentation\",\n",
    "            content=\"\"\"Laptop Pro X1 Technical Specs:\n",
    "- Intel Core i7-12700H processor (14 cores, up to 4.7 GHz)\n",
    "- NVIDIA RTX 3060 graphics (6GB GDDR6)\n",
    "- 16GB DDR5 RAM\n",
    "- Dual fan cooling system with heat pipes\n",
    "- Recommended operating temperature: 0-35°C ambient\n",
    "- Maximum thermal design power: 140W\n",
    "\n",
    "Common thermal management tips:\n",
    "- Ensure ventilation openings are clear\n",
    "- Use on hard, flat surfaces (not soft bedding)\n",
    "- Update thermal drivers to latest version\n",
    "- Consider cooling pad for intensive workloads\"\"\",\n",
    "            compressible=True\n",
    "        ),\n",
    "        \n",
    "        # Layer 2: Supplementary - Policy information\n",
    "        ContextLayer(\n",
    "            priority=LayerPriority.SUPPLEMENTARY,\n",
    "            name=\"Return Policy\",\n",
    "            content=\"\"\"TechStore Return Policy:\n",
    "- 30-day return window from purchase date\n",
    "- Products must be in original packaging\n",
    "- Refunds processed in 5-7 business days\n",
    "- Free return shipping for defective items\n",
    "- Customer pays return shipping for buyer's remorse\n",
    "- 2-year manufacturer warranty covers defects\n",
    "- Extended warranty available for purchase\"\"\",\n",
    "            compressible=True\n",
    "        ),\n",
    "        \n",
    "        # Layer 2: Supplementary - Older conversation history\n",
    "        ContextLayer(\n",
    "            priority=LayerPriority.SUPPLEMENTARY,\n",
    "            name=\"Earlier Conversation\",\n",
    "            content=\"\"\"Earlier in conversation:\n",
    "\n",
    "Customer: Hi, I need some help.\n",
    "Agent: Hello! I'm happy to help. What can I assist you with today?\n",
    "\n",
    "Customer: I'm having an issue with my laptop.\n",
    "Agent: I'm sorry to hear that. Can you tell me more about what's happening?\"\"\",\n",
    "            compressible=True\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "# Create sample layers and analyze them\n",
    "layers = create_sample_layers()\n",
    "\n",
    "print(\"Context Layer Structure\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Group layers by priority level and calculate statistics\n",
    "# This helps us understand the token distribution across priority tiers\n",
    "for priority in LayerPriority:\n",
    "    # Filter to get only layers matching this priority level\n",
    "    priority_layers = [l for l in layers if l.priority == priority]\n",
    "    # Calculate total tokens for this priority level\n",
    "    total_tokens = sum(l.estimate_tokens() for l in priority_layers)\n",
    "\n",
    "    # Display summary for this priority level\n",
    "    print(f\"\\n{priority.name} (Priority {priority.value}):\")\n",
    "    print(f\"  Layers: {len(priority_layers)}\")\n",
    "    print(f\"  Total tokens: ~{total_tokens}\")\n",
    "    print(f\"  Content:\")\n",
    "\n",
    "    # Show details for each layer in this priority\n",
    "    for layer in priority_layers:\n",
    "        compressible = \"compressible\" if layer.compressible else \"fixed\"\n",
    "        print(f\"    - {layer.name} (~{layer.estimate_tokens()} tokens, {compressible})\")\n",
    "\n",
    "# Calculate and display overall statistics\n",
    "total_tokens = sum(l.estimate_tokens() for l in layers)\n",
    "print(f\"\\nTotal context size: ~{total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer structure demonstrates a clear priority hierarchy:\n",
    "1. Defines three priority tiers (Critical, Important, Supplementary) that encode different treatment under space constraints.\n",
    "2. Creates a ContextLayer class with token estimation, compression capabilities and priority assignment.\n",
    "3. Builds realistic example layers showing system instructions and current task as critical, recent conversation and preferences as important and documentation as supplementary.\n",
    "4. Analyzes token distribution across priorities revealing that critical layers consume modest space while supplementary layers contain the bulk of optional content.\n",
    "\n",
    "This foundation enables intelligent context management under varying token budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Layer-aware context assembly\n",
    "\n",
    "With layers defined and prioritized, we can now implement context assembly that respects priority ordering and token budgets. The assembly algorithm should always include critical layers in full, include important layers when space permits with compression as a fallback, and include supplementary layers only when abundant space is available. This ensures graceful degradation as budgets tighten.\n",
    "\n",
    "The algorithm operates in priority order, allocating budget to higher-priority layers first. Critical layers consume their required space unconditionally. Important layers attempt to fit in full, compressing if needed. Supplementary layers fill remaining space opportunistically. This systematic approach guarantees that essential information always survives resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_layered_context(layers: List[ContextLayer],\n",
    "                            token_budget: int,\n",
    "                            compression_ratio: float = 0.5) -> Dict:\n",
    "    \"\"\"Assemble context from layers respecting priorities and budget.\n",
    "    \n",
    "    Args:\n",
    "        layers: All available context layers\n",
    "        token_budget: Maximum tokens to use\n",
    "        compression_ratio: Target size ratio when compressing (0-1, default 0.5)\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing assembled context and metadata\n",
    "    \"\"\"\n",
    "    # Sort layers by priority (critical first). This ensures we process higher priority layers before lower priority ones\n",
    "    sorted_layers = sorted(layers, key=lambda l: l.priority.value)\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    included_layers = []  # Store the actual content to include\n",
    "    tokens_used = 0  # Track how many tokens we have consumed so far\n",
    "    compression_applied = []  # Track which layers got compressed\n",
    "    dropped_layers = []  # Track which layers we could not include\n",
    "\n",
    "    # Process each layer in priority order\n",
    "    for layer in sorted_layers:\n",
    "        # Calculate how many tokens this layer would use\n",
    "        layer_tokens = layer.estimate_tokens()\n",
    "        \n",
    "        # Critical layers: Always include in full\n",
    "        if layer.priority == LayerPriority.CRITICAL:\n",
    "            # Add to context without checking budget\n",
    "            included_layers.append(layer.content)  # Critical layers are non-negotiable - system can't function without them\n",
    "            tokens_used += layer_tokens\n",
    "            continue  # Move to next layer\n",
    "        \n",
    "        # Check remaining budget\n",
    "        remaining_budget = token_budget - tokens_used\n",
    "        \n",
    "        # Important layers: Include if possible, compress if needed\n",
    "        if layer.priority == LayerPriority.IMPORTANT:\n",
    "            # Check if layer fits in full within remaining budget\n",
    "            if layer_tokens <= remaining_budget:\n",
    "                # Fits in full. Add to context\n",
    "                included_layers.append(layer.content)\n",
    "                tokens_used += layer_tokens\n",
    "            # If it does not fit but is compressible and we have some space\n",
    "            elif layer.compressible and remaining_budget > 0:\n",
    "                # Calculate target character length (rough approximation)\n",
    "                target_chars = int(len(layer.content) * compression_ratio)\n",
    "                # Compress to fit budget\n",
    "                compressed = layer.compress(target_chars)\n",
    "                compressed_tokens = int(layer_tokens * compression_ratio)\n",
    "\n",
    "                # Check if compressed version fits\n",
    "                if compressed_tokens <= remaining_budget:\n",
    "                    # Include the compressed version\n",
    "                    included_layers.append(compressed)\n",
    "                    tokens_used += compressed_tokens\n",
    "                    compression_applied.append(layer.name)\n",
    "                else:\n",
    "                    # Even compressed, it doesn't fit - have to drop it\n",
    "                    dropped_layers.append(layer.name)\n",
    "            else:\n",
    "                # Can't compress or no space left - drop this layer\n",
    "                dropped_layers.append(layer.name)\n",
    "            continue # Move to next layer\n",
    "        \n",
    "        # Supplementary layers: Include only if budget allows\n",
    "        if layer.priority == LayerPriority.SUPPLEMENTARY:\n",
    "            # Check if layer fits in full\n",
    "            if layer_tokens <= remaining_budget:\n",
    "                # We have enough space - include it\n",
    "                included_layers.append(layer.content)\n",
    "                tokens_used += layer_tokens\n",
    "            # Try compressing if we have at least 30% of the tokens needed\n",
    "            elif layer.compressible and remaining_budget > layer_tokens * 0.3:\n",
    "                # Try compressing if it would fit\n",
    "                target_chars = int(len(layer.content) * compression_ratio)\n",
    "                compressed = layer.compress(target_chars)\n",
    "                compressed_tokens = int(layer_tokens * compression_ratio)\n",
    "\n",
    "                # Check if compressed version fits\n",
    "                if compressed_tokens <= remaining_budget:\n",
    "                    included_layers.append(compressed)\n",
    "                    tokens_used += compressed_tokens\n",
    "                    compression_applied.append(layer.name)\n",
    "                else:\n",
    "                    # Doesn't fit even compressed\n",
    "                    dropped_layers.append(layer.name)\n",
    "            else:\n",
    "                # Can't compress or no space left - drop this layer\n",
    "                dropped_layers.append(layer.name)\n",
    "    \n",
    "    # Assemble final context by joining all included layers with double newlines\n",
    "    context = \"\\n\\n\".join(included_layers)\n",
    "\n",
    "    # Return comprehensive metadata about the assembly process\n",
    "    return {\n",
    "        'context': context,  # The assembled context string\n",
    "        'tokens_used': tokens_used,  # Actual tokens consumed\n",
    "        'tokens_budget': token_budget,  # Budget we were given\n",
    "        'budget_utilization': (tokens_used / token_budget * 100) if token_budget > 0 else 0,  # Percentage used\n",
    "        'layers_included': len(included_layers),  # How many layers made it in\n",
    "        'layers_total': len(layers),  # Total layers we started with\n",
    "        'compressed_layers': compression_applied,  # Which layers were compressed\n",
    "        'dropped_layers': dropped_layers  # Which layers were dropped\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function implements the core layering algorithm:\n",
    "1. CRITICAL layers are ALWAYS included in full (no compression, no dropping)\n",
    "2. IMPORTANT layers are included if space permits, compressed if needed\n",
    "3. SUPPLEMENTARY layers are included only when abundant space is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-Aware Context Assembly\n",
      "======================================================================\n",
      "\n",
      "Token Budget: 100\n",
      "----------------------------------------------------------------------\n",
      "Used: 100/100 tokens (100.0%)\n",
      "Included: 4/7 layers\n",
      "Compressed: Recent Conversation\n",
      "Dropped: Product Documentation, Return Policy, Earlier Conversation\n",
      "\n",
      "Context preview (first 200 chars):\n",
      "You are a helpful customer service agent for TechStore.\n",
      "\n",
      "Core responsibilities:\n",
      "- Answer product questions accurately\n",
      "- Help with orders and returns\n",
      "- Provide technical support\n",
      "- Maintain professional...\n",
      "\n",
      "\n",
      "Token Budget: 200\n",
      "----------------------------------------------------------------------\n",
      "Used: 185/200 tokens (92.5%)\n",
      "Included: 5/7 layers\n",
      "Dropped: Return Policy, Earlier Conversation\n",
      "\n",
      "Context preview (first 200 chars):\n",
      "You are a helpful customer service agent for TechStore.\n",
      "\n",
      "Core responsibilities:\n",
      "- Answer product questions accurately\n",
      "- Help with orders and returns\n",
      "- Provide technical support\n",
      "- Maintain professional...\n",
      "\n",
      "\n",
      "Token Budget: 400\n",
      "----------------------------------------------------------------------\n",
      "Used: 256/400 tokens (64.0%)\n",
      "Included: 7/7 layers\n",
      "\n",
      "Context preview (first 200 chars):\n",
      "You are a helpful customer service agent for TechStore.\n",
      "\n",
      "Core responsibilities:\n",
      "- Answer product questions accurately\n",
      "- Help with orders and returns\n",
      "- Provide technical support\n",
      "- Maintain professional...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with different token budgets\n",
    "budgets = [100, 200, 400]\n",
    "\n",
    "print(\"Layer-Aware Context Assembly\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Try assembling context under different budget constraints\n",
    "for budget in budgets:\n",
    "    # Call the assembly function with this budget\n",
    "    result = assemble_layered_context(layers, token_budget=budget)\n",
    "    \n",
    "    print(f\"\\nToken Budget: {budget}\")\n",
    "    print(\"-\" * 70)\n",
    "    # Show how much of the budget was used\n",
    "    print(f\"Used: {result['tokens_used']}/{result['tokens_budget']} tokens ({result['budget_utilization']:.1f}%)\")\n",
    "    # Show how many layers fit\n",
    "    print(f\"Included: {result['layers_included']}/{result['layers_total']} layers\")\n",
    "\n",
    "    # Show which layers were compressed (if any)\n",
    "    if result['compressed_layers']:\n",
    "        print(f\"Compressed: {', '.join(result['compressed_layers'])}\")\n",
    "\n",
    "    # Show which layers were dropped (if any)\n",
    "    if result['dropped_layers']:\n",
    "        print(f\"Dropped: {', '.join(result['dropped_layers'])}\")\n",
    "\n",
    "    # Show a preview of the assembled context\n",
    "    print(f\"\\nContext preview (first 200 chars):\")\n",
    "    print(f\"{result['context'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer-aware assembly demonstrates graceful degradation:\n",
    "1. Implements priority-ordered assembly that processes critical layers first, ensuring they always consume budget unconditionally.\n",
    "2. Applies intelligent strategies for each priority tier - critical layers included in full, important layers compressed if needed, supplementary layers opportunistic.\n",
    "3. Tests multiple budget levels showing how context adapts from minimal (only critical) to generous (all layers included).\n",
    "4. Tracks compression and dropping decisions providing visibility into which layers were affected by budget constraints.\n",
    "\n",
    "This ensures agents maintain essential capabilities even under severe token limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Intelligent compression strategies\n",
    "\n",
    "Simple truncation is crude and lossy. Production systems should employ smarter compression strategies that preserve the most important information while reducing token count. This might involve extractive summarization that selects key sentences, abstractive summarization using language models to paraphrase concisely, or structured reduction that removes examples while keeping core principles.\n",
    "\n",
    "We will implement a more sophisticated compression approach that analyzes content structure and selectively reduces less critical portions while preserving essential information. For conversation history, this means keeping the most recent exchanges. For documentation, it means extracting key facts and dropping verbose examples. For policies, it means retaining rules while removing explanatory text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_compress_conversation(content: str, target_length: int) -> str:\n",
    "    \"\"\"Intelligently compress conversation history.\n",
    "    \n",
    "    Keeps most recent exchanges, drops older ones.\n",
    "    \n",
    "    Args:\n",
    "        content: Conversation history\n",
    "        target_length: Target character count\n",
    "        \n",
    "    Returns:\n",
    "        Compressed conversation\n",
    "    \"\"\"\n",
    "    # Split content into individual lines\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Check if first line is a header (e.g., \"Last 5 exchanges:\")\n",
    "    header = lines[0] if lines[0].endswith(':') else \"\"\n",
    "    # Get the actual exchange lines (excluding header)\n",
    "    exchanges = lines[1:] if header else lines\n",
    "    \n",
    "    # Take most recent exchanges until we hit target\n",
    "    result_lines = [header] if header else []  # Start building result with header if present\n",
    "    current_length = len(header)\n",
    "    \n",
    "    # Work backwards from most recent exchange. This ensures we keep the most recent conversation turns\n",
    "    for line in reversed(exchanges):\n",
    "        # Check if adding this line would exceed target\n",
    "        if current_length + len(line) + 1 <= target_length:\n",
    "            # Insert at beginning (after header) since we are working backwards\n",
    "            result_lines.insert(1 if header else 0, line)\n",
    "            current_length += len(line) + 1\n",
    "        else:\n",
    "            # No more space - stop adding older exchanges\n",
    "            break\n",
    "    \n",
    "    return '\\n'.join(result_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function we keep most recent exchanges and drop older ones. This preserves the most relevant context (recent conversation) while reducing token count by removing older exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_compress_documentation(content: str, target_length: int) -> str:\n",
    "    \"\"\"Intelligently compress technical documentation.\n",
    "    \n",
    "    Keeps specs and key points, drops verbose explanations.\n",
    "    \n",
    "    Args:\n",
    "        content: Documentation text\n",
    "        target_length: Target character count\n",
    "        \n",
    "    Returns:\n",
    "        Compressed documentation\n",
    "    \"\"\"\n",
    "    # Split content into individual lines\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Categorize lines by importance\n",
    "    priority_lines = []  # High-value technical information\n",
    "    other_lines = []  # General explanatory text\n",
    "    \n",
    "    for line in lines:\n",
    "        # High priority indicators: specs contain numbers, units, bullets or key-value pairs\n",
    "        if any(c in line for c in ['-', '•', ':', '°C', 'GB', 'GHz', 'W']):  # Look for: dashes (bullets), colons (key-value), technical units, numbers\n",
    "            priority_lines.append(line)\n",
    "        else:\n",
    "            # Everything else (headers, explanations) is lower priority\n",
    "            other_lines.append(line)\n",
    "    \n",
    "    # Build result with priority content first\n",
    "    result = '\\n'.join(priority_lines)  # Technical specs are most important, so include them first\n",
    "    \n",
    "    # Add other content if space permits. This ensures we keep specs even if we have to drop explanations\n",
    "    for line in other_lines:\n",
    "        # Check if we have room for this line\n",
    "        if len(result) + len(line) + 1 <= target_length:\n",
    "            result += '\\n' + line\n",
    "        else:\n",
    "            # Out of space - stop adding\n",
    "            break\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we keep specs and key points and drop verbose explanations. Technical specs (numbers, measurements) are most valuable, while explanatory prose can often be safely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_compress_policy(content: str, target_length: int) -> str:\n",
    "    \"\"\"Intelligently compress policy text.\n",
    "    \n",
    "    Keeps rules and key terms, drops explanations.\n",
    "    \n",
    "    Args:\n",
    "        content: Policy text\n",
    "        target_length: Target character count\n",
    "        \n",
    "    Returns:\n",
    "        Compressed policy\n",
    "    \"\"\"\n",
    "    # Split content into lines\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Build result focusing on structured content\n",
    "    result_lines = []\n",
    "    current_length = 0\n",
    "\n",
    "    # Keep title and bullet points\n",
    "    for line in lines:\n",
    "        # Prioritize title and bullets\n",
    "        # Lines with colons (typically \"Policy Name:\" or \"Key: Value\")\n",
    "        # Lines starting with dashes (bullet points with rules)\n",
    "        if ':' in line or line.strip().startswith('-'):\n",
    "            # Check if we have room\n",
    "            if current_length + len(line) + 1 <= target_length:\n",
    "                result_lines.append(line)\n",
    "                current_length += len(line) + 1\n",
    "        # Else: skip explanatory prose that doesn't match the pattern\n",
    "\n",
    "    # Join preserved lines\n",
    "    return '\\n'.join(result_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we keep rules and key terms and drop explanations. Policy documents have structure - titles and bullet points contain the essential rules, while surrounding text is often explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart Compression Demonstration\n",
      "======================================================================\n",
      "\n",
      "Original (407 chars):\n",
      "Last 5 exchanges:\n",
      "\n",
      "Customer: Hi, I need help.\n",
      "Agent: Hello! How can I assist you?\n",
      "\n",
      "Customer: My laptop overheats.\n",
      "Agent: I understand. When did this start?\n",
      "\n",
      "Customer: About a week ago.\n",
      "Agent: Has anything changed recently?\n",
      "\n",
      "Customer: I installed new games.\n",
      "Agent: That could be related. Let's check some things.\n",
      "\n",
      "Customer: The fans are very loud too.\n",
      "Agent: That suggests the cooling system is working hard.\n",
      "\n",
      "Compressed to 70% (269 chars):\n",
      "Last 5 exchanges:\n",
      "\n",
      "Customer: About a week ago.\n",
      "Agent: Has anything changed recently?\n",
      "\n",
      "Customer: I installed new games.\n",
      "Agent: That could be related. Let's check some things.\n",
      "\n",
      "Customer: The fans are very loud too.\n",
      "Agent: That suggests the cooling system is working hard.\n",
      "\n",
      "Compressed to 50% (202 chars):\n",
      "Last 5 exchanges:\n",
      "\n",
      "Customer: I installed new games.\n",
      "Agent: That could be related. Let's check some things.\n",
      "\n",
      "Customer: The fans are very loud too.\n",
      "Agent: That suggests the cooling system is working hard.\n",
      "\n",
      "Compressed to 30% (113 chars):\n",
      "Last 5 exchanges:\n",
      "\n",
      "Customer: The fans are very loud too.\n",
      "Agent: That suggests the cooling system is working hard.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced layer class with smart compression\n",
    "# Extends ContextLayer with content-aware compression that applies different strategies based on the type of content (conversation, documentation, policy, etc.)\n",
    "@dataclass\n",
    "class SmartContextLayer(ContextLayer):\n",
    "    \"\"\"Context layer with intelligent compression.\n",
    "    \n",
    "    Attributes:\n",
    "        layer_type: Type of content for smart compression (\"conversation\", \"documentation\", \"policy\" or \"general\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Additional attribute beyond base ContextLayer\n",
    "    layer_type: str = \"general\"  # conversation, documentation, policy, general. Default to general compression\n",
    "    \n",
    "    def smart_compress(self, target_length: int) -> str:\n",
    "        \"\"\"Apply content-aware compression.\n",
    "        \n",
    "        Args:\n",
    "            target_length: Target character length\n",
    "            \n",
    "        Returns:\n",
    "            Intelligently compressed content\n",
    "        \"\"\"\n",
    "        # Don't compress if not marked as compressible or already short enough\n",
    "        if not self.compressible or len(self.content) <= target_length:\n",
    "            return self.content\n",
    "        \n",
    "        # Apply type-specific compression\n",
    "        if self.layer_type == \"conversation\":\n",
    "            # Use conversation-aware compression\n",
    "            return smart_compress_conversation(self.content, target_length)\n",
    "        elif self.layer_type == \"documentation\":\n",
    "            # Use documentation-aware compression\n",
    "            return smart_compress_documentation(self.content, target_length)\n",
    "        elif self.layer_type == \"policy\":\n",
    "            # Use policy-aware compression\n",
    "            return smart_compress_policy(self.content, target_length)\n",
    "        else:\n",
    "            # Fallback to simple compression from base class\n",
    "            return self.compress(target_length)\n",
    "\n",
    "# Test smart compression on sample layer\n",
    "conversation_layer = SmartContextLayer(\n",
    "    priority=LayerPriority.IMPORTANT,\n",
    "    name=\"Recent Conversation\",\n",
    "    layer_type=\"conversation\",\n",
    "    content=\"\"\"Last 5 exchanges:\n",
    "\n",
    "Customer: Hi, I need help.\n",
    "Agent: Hello! How can I assist you?\n",
    "\n",
    "Customer: My laptop overheats.\n",
    "Agent: I understand. When did this start?\n",
    "\n",
    "Customer: About a week ago.\n",
    "Agent: Has anything changed recently?\n",
    "\n",
    "Customer: I installed new games.\n",
    "Agent: That could be related. Let's check some things.\n",
    "\n",
    "Customer: The fans are very loud too.\n",
    "Agent: That suggests the cooling system is working hard.\"\"\",\n",
    "    compressible=True\n",
    ")\n",
    "\n",
    "print(\"Smart Compression Demonstration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nOriginal ({len(conversation_layer.content)} chars):\")\n",
    "print(conversation_layer.content)\n",
    "\n",
    "# Test different compression levels\n",
    "for ratio in [0.7, 0.5, 0.3]:\n",
    "    # Calculate target character length\n",
    "    target = int(len(conversation_layer.content) * ratio)\n",
    "    # Apply smart compression\n",
    "    compressed = conversation_layer.smart_compress(target)\n",
    "    \n",
    "    print(f\"\\nCompressed to {ratio*100:.0f}% ({len(compressed)} chars):\")\n",
    "    print(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smart compression preserves essential information:\n",
    "1. Implements content-aware compression strategies tailored to different information types (conversation, documentation, policy).\n",
    "2. Prioritizes recent exchanges in conversation history, ensuring the most relevant context is retained even under compression.\n",
    "3. Extracts key specifications and facts from documentation while dropping verbose explanations and examples.\n",
    "4. Retains policy rules and structure while removing explanatory prose that adds length without critical information.\n",
    "\n",
    "These intelligent strategies ensure compression maintains usefulness rather than just reducing size arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dynamic layer prioritization\n",
    "\n",
    "While layers have default priorities, context-aware systems should adjust priorities dynamically based on the current task. When handling a refund request, policy layers become more important than product documentation. When troubleshooting technical issues, documentation rises in priority while general conversation history becomes less critical. Dynamic prioritization ensures that layer selection adapts to immediate needs.\n",
    "\n",
    "We implement this through priority boosting that temporarily elevates specific layers based on query analysis or task classification. The boosting is contextual and reversible, ensuring that priorities remain appropriate for each unique interaction while maintaining the underlying hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Layer Prioritization\n",
      "======================================================================\n",
      "\n",
      "Task: technical_support\n",
      "Context: Customer has overheating issue\n",
      "----------------------------------------------------------------------\n",
      "Budget: 200 tokens\n",
      "Used: 185 tokens (92.5%)\n",
      "Included: 5/7 layers\n",
      "\n",
      "Priority adjustments for this task:\n",
      "  • Product Documentation: SUPPLEMENTARY → IMPORTANT\n",
      "\n",
      "Dropped layers: Return Policy, Earlier Conversation\n",
      "\n",
      "\n",
      "Task: refund_request\n",
      "Context: Customer wants to return product\n",
      "----------------------------------------------------------------------\n",
      "Budget: 200 tokens\n",
      "Used: 193 tokens (96.5%)\n",
      "Included: 6/7 layers\n",
      "\n",
      "Priority adjustments for this task:\n",
      "  • Return Policy: SUPPLEMENTARY → IMPORTANT\n",
      "\n",
      "Dropped layers: Earlier Conversation\n",
      "\n",
      "\n",
      "Task: product_inquiry\n",
      "Context: Customer asking about specifications\n",
      "----------------------------------------------------------------------\n",
      "Budget: 200 tokens\n",
      "Used: 185 tokens (92.5%)\n",
      "Included: 5/7 layers\n",
      "\n",
      "Priority adjustments for this task:\n",
      "  • Product Documentation: SUPPLEMENTARY → IMPORTANT\n",
      "\n",
      "Dropped layers: Return Policy, Earlier Conversation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TaskType(Enum):\n",
    "    \"\"\"Different task types that influence layer priorities.\"\"\"\n",
    "    TECHNICAL_SUPPORT = \"technical_support\"  # Troubleshooting hardware/software\n",
    "    REFUND_REQUEST = \"refund_request\"  # Processing returns/refunds\n",
    "    PRODUCT_INQUIRY = \"product_inquiry\"  # Questions about product features\n",
    "    GENERAL_CHAT = \"general_chat\"  # Generic conversation\n",
    "\n",
    "# This function creates a NEW list of layers with adjusted priorities, leaving the original layers unchanged\n",
    "def adjust_layer_priorities(layers: List[ContextLayer],\n",
    "                           task_type: TaskType) -> List[ContextLayer]:\n",
    "    \"\"\"Dynamically adjust layer priorities based on task type.\n",
    "    \n",
    "    Args:\n",
    "        layers: Original context layers\n",
    "        task_type: Current task being performed\n",
    "        \n",
    "    Returns:\n",
    "        Layers with adjusted priorities (new list, original unchanged)\n",
    "    \"\"\"\n",
    "    adjusted = []\n",
    "\n",
    "    # Process each layer\n",
    "    for layer in layers:\n",
    "        # Copy layer to avoid modifying original. This is important so we can reuse the original layers for different tasks\n",
    "        new_layer = ContextLayer(\n",
    "            priority=layer.priority,  # Start with original priority\n",
    "            name=layer.name,\n",
    "            content=layer.content,\n",
    "            compressible=layer.compressible\n",
    "        )\n",
    "        \n",
    "        # Adjust priorities based on task type\n",
    "        # TECHNICAL SUPPORT: Boost documentation, prioritize conversation\n",
    "        if task_type == TaskType.TECHNICAL_SUPPORT:\n",
    "            # Product documentation becomes very important for troubleshooting\n",
    "            if \"Documentation\" in layer.name:\n",
    "                # Elevate from SUPPLEMENTARY to IMPORTANT\n",
    "                new_layer.priority = LayerPriority.IMPORTANT\n",
    "            # Policy information is less relevant for technical issues\n",
    "            elif \"Policy\" in layer.name:\n",
    "                # Keep as SUPPLEMENTARY or potentially downgrade\n",
    "                new_layer.priority = LayerPriority.SUPPLEMENTARY\n",
    "\n",
    "        # REFUND REQUEST: Boost policy information\n",
    "        elif task_type == TaskType.REFUND_REQUEST:\n",
    "            # Policy information (return windows, refund rules) is important\n",
    "            if \"Policy\" in layer.name:\n",
    "                # Elevate from SUPPLEMENTARY to IMPORTANT\n",
    "                new_layer.priority = LayerPriority.IMPORTANT\n",
    "            # Technical specs are less relevant for refund discussions\n",
    "            elif \"Documentation\" in layer.name:\n",
    "                # Keep as SUPPLEMENTARY - not critical for refunds\n",
    "                new_layer.priority = LayerPriority.SUPPLEMENTARY\n",
    "\n",
    "        # PRODUCT INQUIRY: Boost product documentation\n",
    "        elif task_type == TaskType.PRODUCT_INQUIRY:\n",
    "            # Product specs and features are essential for answering questions\n",
    "            if \"Documentation\" in layer.name:\n",
    "                # Elevate from SUPPLEMENTARY to IMPORTANT\n",
    "                new_layer.priority = LayerPriority.IMPORTANT\n",
    "            # Older conversation is less relevant for product questions\n",
    "            elif \"Earlier Conversation\" in layer.name:\n",
    "                # Keep as SUPPLEMENTARY - current task is more important\n",
    "                new_layer.priority = LayerPriority.SUPPLEMENTARY\n",
    "\n",
    "        # For GENERAL_CHAT or other task types, keep original priorities\n",
    "\n",
    "        # Add the adjusted layer to our result list\n",
    "        adjusted.append(new_layer)\n",
    "    \n",
    "    return adjusted\n",
    "\n",
    "# Test dynamic prioritization with different scenarios\n",
    "print(\"Dynamic Layer Prioritization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define test scenarios with different task types\n",
    "test_tasks = [\n",
    "    (TaskType.TECHNICAL_SUPPORT, \"Customer has overheating issue\"),\n",
    "    (TaskType.REFUND_REQUEST, \"Customer wants to return product\"),\n",
    "    (TaskType.PRODUCT_INQUIRY, \"Customer asking about specifications\"),\n",
    "]\n",
    "\n",
    "# Use a moderate token budget for all tests to see prioritization effects\n",
    "token_budget = 200\n",
    "\n",
    "# Test each scenario\n",
    "for task_type, description in test_tasks:\n",
    "    print(f\"\\nTask: {task_type.value}\")\n",
    "    print(f\"Context: {description}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Adjust priorities for this task\n",
    "    adjusted_layers = adjust_layer_priorities(layers, task_type)\n",
    "    \n",
    "    # Assemble context with adjusted priorities\n",
    "    result = assemble_layered_context(adjusted_layers, token_budget=token_budget)\n",
    "    \n",
    "    print(f\"Budget: {result['tokens_budget']} tokens\")\n",
    "    print(f\"Used: {result['tokens_used']} tokens ({result['budget_utilization']:.1f}%)\")\n",
    "    print(f\"Included: {result['layers_included']}/{result['layers_total']} layers\")\n",
    "    \n",
    "    # Show priority adjustments\n",
    "    print(f\"\\nPriority adjustments for this task:\")\n",
    "    for orig, adj in zip(layers, adjusted_layers):\n",
    "        if orig.priority != adj.priority:\n",
    "            print(f\"  • {adj.name}: {orig.priority.name} → {adj.priority.name}\")\n",
    "    \n",
    "    if result['dropped_layers']:\n",
    "        print(f\"\\nDropped layers: {', '.join(result['dropped_layers'])}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic prioritization adapts context to task requirements:\n",
    "1. Implements task-aware priority adjustment that boosts layers relevant to the current operation.\n",
    "2. Demonstrates how technical support tasks elevate documentation priority while refund requests elevate policy information.\n",
    "3. Shows that under the same token budget, different tasks select different layer combinations based on adjusted priorities.\n",
    "4. Maintains the critical layer foundation while flexibly reallocating important and supplementary designations.\n",
    "\n",
    "This ensures context composition matches the specific needs of each interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Production layered context system\n",
    "\n",
    "Integrating all the techniques we have explored, we can now build a production-ready layered context system. This system should provide a clean interface for context assembly, automatically adjust priorities based on task classification, apply intelligent compression when needed, respect token budgets while maximizing information value, and provide comprehensive metadata for monitoring and debugging.\n",
    "\n",
    "The production system combines layer definition, priority management, smart compression, token budgeting, and assembly orchestration into a cohesive architecture that agents can use seamlessly. This enables sophisticated context engineering that maintains quality and relevance across varying resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Layered Context System\n",
      "======================================================================\n",
      "\n",
      "Scenario: Tight budget, technical support\n",
      "----------------------------------------------------------------------\n",
      "Task: technical_support\n",
      "Budget: 150 tokens\n",
      "Used: 144 tokens (96.0%)\n",
      "\n",
      "Layers: 5 included, 1 compressed, 2 dropped\n",
      "\n",
      "Layer breakdown:\n",
      "  ✓ System Instructions: included_full\n",
      "  ✓ Current Task: included_full\n",
      "  ✓ Recent Conversation: included_full\n",
      "  ✓ Key Preferences: included_full\n",
      "  ✗ Product Documentation: dropped_no_space\n",
      "  ✓ Return Policy: included_compressed (50%)\n",
      "  ✗ Earlier Conversation: dropped_no_space\n",
      "\n",
      "\n",
      "Scenario: Moderate budget, refund request\n",
      "----------------------------------------------------------------------\n",
      "Task: refund_request\n",
      "Budget: 250 tokens\n",
      "Used: 239 tokens (95.6%)\n",
      "\n",
      "Layers: 7 included, 1 compressed, 0 dropped\n",
      "\n",
      "Layer breakdown:\n",
      "  ✓ System Instructions: included_full\n",
      "  ✓ Current Task: included_full\n",
      "  ✓ Recent Conversation: included_full\n",
      "  ✓ Key Preferences: included_full\n",
      "  ✓ Return Policy: included_full\n",
      "  ✓ Product Documentation: included_full\n",
      "  ✓ Earlier Conversation: included_compressed (48%)\n",
      "\n",
      "\n",
      "Scenario: Generous budget, general chat\n",
      "----------------------------------------------------------------------\n",
      "Task: general_chat\n",
      "Budget: 400 tokens\n",
      "Used: 256 tokens (64.0%)\n",
      "\n",
      "Layers: 7 included, 0 compressed, 0 dropped\n",
      "\n",
      "Layer breakdown:\n",
      "  ✓ System Instructions: included_full\n",
      "  ✓ Current Task: included_full\n",
      "  ✓ Recent Conversation: included_full\n",
      "  ✓ Key Preferences: included_full\n",
      "  ✓ Product Documentation: included_full\n",
      "  ✓ Return Policy: included_full\n",
      "  ✓ Earlier Conversation: included_full\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayeredContextManager:\n",
    "    \"\"\"Production layered context management system.\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio: float = 0.5):\n",
    "        \"\"\"Initialize context manager.\n",
    "        \n",
    "        Args:\n",
    "            compression_ratio: Default compression ratio for layers (0-1)\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.layers = []  # Storage for all available layers\n",
    "\n",
    "    # Layers are stored and can be used in future context assembly calls. The same pool of layers can be used for different tasks with different priority adjustments\n",
    "    def add_layer(self, layer: ContextLayer) -> None:\n",
    "        \"\"\"Add a layer to the context.\n",
    "        \n",
    "        Args:\n",
    "            layer: Context layer to add\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def build_context(self,\n",
    "                     token_budget: int,\n",
    "                     task_type: Optional[TaskType] = None,\n",
    "                     use_smart_compression: bool = True) -> Dict:\n",
    "        \"\"\"Build context from layers with budget constraints.\n",
    "        \n",
    "        Args:\n",
    "            token_budget: Maximum tokens to use\n",
    "            task_type: Optional task type for priority adjustment\n",
    "            use_smart_compression: Whether to use intelligent compression\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing context and detailed metadata\n",
    "        \"\"\"\n",
    "        # Apply task-based priority adjustment if a task type is specified\n",
    "        working_layers = self.layers\n",
    "        if task_type:\n",
    "            # Get adjusted priorities for this task\n",
    "            working_layers = adjust_layer_priorities(self.layers, task_type)\n",
    "        \n",
    "         # Sort layers by priority (CRITICAL=0 first, SUPPLEMENTARY=2 last)\n",
    "        sorted_layers = sorted(working_layers, key=lambda l: l.priority.value)\n",
    "        \n",
    "        # Initialize tracking for assembly process\n",
    "        included_content = []  # Actual text content to include\n",
    "        tokens_used = 0  # Running total of tokens consumed\n",
    "        layer_stats = []  # Detailed statistics for each layer\n",
    "\n",
    "        # Process each layer in priority order\n",
    "        for layer in sorted_layers:\n",
    "            # Estimate tokens for this layer\n",
    "            layer_tokens = layer.estimate_tokens()\n",
    "            # Calculate remaining budget\n",
    "            remaining = token_budget - tokens_used\n",
    "            \n",
    "            # Initialize statistics tracking for this layer\n",
    "            layer_stat = {\n",
    "                'name': layer.name,\n",
    "                'priority': layer.priority.name,\n",
    "                'original_tokens': layer_tokens,\n",
    "                'status': 'pending'  # Will be updated below\n",
    "            }\n",
    "            \n",
    "            # Critical: always include\n",
    "            if layer.priority == LayerPriority.CRITICAL:\n",
    "                # Include entire content without checking budget\n",
    "                included_content.append(layer.content)\n",
    "                tokens_used += layer_tokens\n",
    "                # Update status tracking\n",
    "                layer_stat['status'] = 'included_full'\n",
    "                layer_stat['tokens_used'] = layer_tokens\n",
    "            \n",
    "            # Important/Supplementary: fit or compress\n",
    "            # Check if layer fits in full within remaining budget\n",
    "            elif layer_tokens <= remaining:\n",
    "                # Include the whole layer\n",
    "                included_content.append(layer.content)\n",
    "                tokens_used += layer_tokens\n",
    "                layer_stat['status'] = 'included_full'\n",
    "                layer_stat['tokens_used'] = layer_tokens\n",
    "\n",
    "            # Layer doesn't fit - try compression if enabled\n",
    "            elif layer.compressible and remaining > 0:\n",
    "                # Calculate target character length for compression\n",
    "                target_chars = int(len(layer.content) * self.compression_ratio)\n",
    "\n",
    "                # Apply appropriate compression method\n",
    "                if use_smart_compression and isinstance(layer, SmartContextLayer):\n",
    "                    # Use intelligent content-aware compression\n",
    "                    compressed = layer.smart_compress(target_chars)\n",
    "                else:\n",
    "                    # Use simple truncation compression\n",
    "                    compressed = layer.compress(target_chars)\n",
    "\n",
    "                # Estimate tokens for compressed version\n",
    "                compressed_tokens = int(layer_tokens * self.compression_ratio)\n",
    "\n",
    "                # Check if compressed version fits\n",
    "                if compressed_tokens <= remaining:\n",
    "                    # Include compressed version\n",
    "                    included_content.append(compressed)\n",
    "                    tokens_used += compressed_tokens\n",
    "                    layer_stat['status'] = 'included_compressed'\n",
    "                    layer_stat['tokens_used'] = compressed_tokens\n",
    "                    layer_stat['compression_ratio'] = compressed_tokens / layer_tokens\n",
    "                else:\n",
    "                    # Even compressed, doesn't fit\n",
    "                    layer_stat['status'] = 'dropped_no_space'\n",
    "            else:\n",
    "                # Can't compress or no space left\n",
    "                layer_stat['status'] = 'dropped_no_space'\n",
    "\n",
    "            # Add this layer's statistics to our tracking\n",
    "            layer_stats.append(layer_stat)\n",
    "        \n",
    "        # Assemble final context\n",
    "        context = \"\\n\\n\".join(included_content)\n",
    "\n",
    "        # Return comprehensive results including context and metadata\n",
    "        return {\n",
    "            'context': context,\n",
    "            'tokens_used': tokens_used,  # Actual tokens consumed\n",
    "            'token_budget': token_budget,\n",
    "            'budget_utilization': (tokens_used / token_budget * 100),  # Percentage\n",
    "            'layers_total': len(self.layers),  # Total layers available\n",
    "            'layers_included': sum(1 for s in layer_stats if 'included' in s['status']),  # Count of included layers\n",
    "            'layers_compressed': sum(1 for s in layer_stats if 'compressed' in s['status']),  # Count of compressed layers\n",
    "            'layers_dropped': sum(1 for s in layer_stats if 'dropped' in s['status']),  # Count of dropped layers\n",
    "            'layer_details': layer_stats,  # Detailed per-layer statistics\n",
    "            'task_type': task_type.value if task_type else None  # Task type used\n",
    "        }\n",
    "\n",
    "# Create and test production system. Initialize manager with 50% compression ratio\n",
    "manager = LayeredContextManager(compression_ratio=0.5)\n",
    "\n",
    "# Add all sample layers to the manager\n",
    "for layer in layers:\n",
    "    manager.add_layer(layer)\n",
    "\n",
    "print(\"Production Layered Context System\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define test scenarios covering different budgets and task types\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'Tight budget, technical support',\n",
    "        'token_budget': 150,\n",
    "        'task_type': TaskType.TECHNICAL_SUPPORT\n",
    "    },\n",
    "    {\n",
    "        'name': 'Moderate budget, refund request',\n",
    "        'token_budget': 250,\n",
    "        'task_type': TaskType.REFUND_REQUEST\n",
    "    },\n",
    "    {\n",
    "        'name': 'Generous budget, general chat',\n",
    "        'token_budget': 400,\n",
    "        'task_type': TaskType.GENERAL_CHAT\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run each test scenario\n",
    "for scenario in scenarios:\n",
    "    name = scenario.pop('name')  # Extract scenario name for display\n",
    "    result = manager.build_context(**scenario)  # Build context for this scenario\n",
    "    \n",
    "    print(f\"\\nScenario: {name}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Task: {result['task_type']}\")\n",
    "    print(f\"Budget: {result['token_budget']} tokens\")\n",
    "    print(f\"Used: {result['tokens_used']} tokens ({result['budget_utilization']:.1f}%)\")\n",
    "\n",
    "    # Show summary statistics\n",
    "    print(f\"\\nLayers: {result['layers_included']} included, \"\n",
    "          f\"{result['layers_compressed']} compressed, \"\n",
    "          f\"{result['layers_dropped']} dropped\")\n",
    "\n",
    "    # Show detailed breakdown for each layer\n",
    "    print(f\"\\nLayer breakdown:\")\n",
    "    for detail in result['layer_details']:\n",
    "        status_icon = \"✓\" if \"included\" in detail['status'] else \"✗\"  # Use checkmark for included, X for dropped\n",
    "        compression = f\" ({detail.get('compression_ratio', 0)*100:.0f}%)\" if 'compressed' in detail['status'] else \"\"\n",
    "        print(f\"  {status_icon} {detail['name']}: {detail['status']}{compression}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The production system demonstrates complete layered context management:\n",
    "1. Implements a unified manager class that orchestrates layer addition, priority adjustment, compression and assembly.\n",
    "2. Supports multiple assembly strategies including task-aware priority boosting and intelligent compression selection.\n",
    "3. Provides detailed metadata about every layer's treatment (included full, compressed, or dropped) enabling observability and debugging.\n",
    "4. Tests realistic scenarios showing how the same layer collection adapts to different token budgets and task types.\n",
    "5. Demonstrates graceful degradation where critical information is always preserved while supplementary content scales with available resources.\n",
    "\n",
    "This architecture supports sophisticated production agents with complex context requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
