{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RAG system design\n",
    "\n",
    "RAG systems extend agent capabilities by grounding responses in external knowledge. Rather than relying solely on training data, RAG-enabled agents can query document stores, databases, and knowledge bases to retrieve relevant information before generating responses. This approach enables agents to access up-to-date information, cite sources, and provide answers grounded in specific organizational knowledge. However, poorly designed RAG systems introduce their own problems: noisy retrieval that clutters context with irrelevant information, missing critical details due to inadequate chunking, and wasted tokens on redundant or low-quality content.\n",
    "\n",
    "RAG system design is fundamentally about maximizing signal-to-noise ratio in retrieved context. Every decision - how documents are chunked, how chunks are indexed, what metadata is attached, how results are ranked, and how many results to return - affects the quality of context provided to the agent. Well-designed RAG systems deliver precisely the information needed to answer queries accurately while minimizing tokens spent on irrelevant details. This requires thoughtful strategies for chunking that preserve semantic coherence, rich metadata that enables filtering, provenance markers that support verification, and relevance scoring that prioritizes quality over quantity.\n",
    "\n",
    "In this notebook, we explore systematic techniques for designing RAG systems that maximize retrieval quality and context efficiency. We will examine effective chunking strategies that respect document structure, semantic indexing with metadata enrichment, retrieval pipelines with provenance markers, relevance indicators and confidence scores, source attribution for verifiability, and techniques for optimizing signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "- **`langchain_openai`**: Provides the integration with OpenAI's models (Chat and Embeddings).\n",
    "- **`langchain_text_splitters`**: Tools to split long text into smaller, manageable chunks.\n",
    "- **`langchain_community.vectorstores`**: Contains the FAISS vector store implementation for efficient similarity search.\n",
    "- **`pydantic`**: Used for data validation and defining structured data models.\n",
    "\n",
    "### Initialize the language model and embeddings\n",
    "We initialize the OpenAI chat model and embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model and embeddings\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(),\n",
    "    temperature=0  # Set to 0 for more deterministic outputs\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce080e75-5366-49d2-a1f5-6fad692b1bca",
   "metadata": {},
   "source": [
    "- **`temperature=0`**: Ensures the model returns the most likely response, making it consistent and reproducible.\n",
    "- **`OpenAIEmbeddings`**: Converts text into vector representations that capture semantic meaning.\n",
    "\n",
    "### Sample document\n",
    "We define a sample document (a product return policy) to use for demonstrating the RAG concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba55c57c-b0f2-4b13-a95d-651c075d770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document length: 972 characters\n",
      "Original document:\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the ...\n"
     ]
    }
   ],
   "source": [
    "# Sample document\n",
    "sample_document = \"\"\"Product Return Policy\n",
    "\n",
    "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
    "\n",
    "Return Window: You have 30 days from the date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
    "\n",
    "Condition Requirements: Items must be in original condition with all accessories, manuals, and packaging. Opened software cannot be returned due to licensing restrictions.\n",
    "\n",
    "Refund Process: Once we receive your return, we'll inspect it within 2 business days. Approved refunds are processed to your original payment method within 5-7 business days.\n",
    "\n",
    "Shipping Costs: Return shipping is free for defective items. For other returns, a $9.99 shipping fee will be deducted from your refund unless you use our prepaid return label.\n",
    "\n",
    "Exceptions: Final sale items, gift cards, and personalized products cannot be returned. Contact customer service for assistance with these items.\"\"\"\n",
    "\n",
    "print(f\"Original document length: {len(sample_document)} characters\")\n",
    "print(f\"Original document:\\n{sample_document[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Effective chunking strategies\n",
    "\n",
    "Chunking transforms long documents into retrievable units, but this seemingly simple operation profoundly affects retrieval quality. Chunk too large and we retrieve irrelevant information alongside relevant content, wasting tokens and diluting signal. Chunk too small and we fragment context, losing the connections and coherence needed to understand the information. The chunking strategy determines whether our RAG system can find and return exactly what's needed or buries valuable information in noise.\n",
    "\n",
    "Different chunking strategies suit different content types and retrieval needs. Fixed-size chunking with overlap provides consistency and preserves some context across boundaries. Recursive chunking respects document structure by splitting on meaningful delimiters like paragraphs and sentences. Semantic chunking groups content by topic, creating chunks where every sentence contributes to answering a specific type of query. Understanding these approaches and when to apply each is essential for effective RAG system design.\n",
    "\n",
    "### Bad approach: Naive fixed-size chunking without overlap\n",
    "This approach simply splits the text into fixed-size chunks without considering any document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Naive chunking results:\n",
      "Number of chunks: 5\n",
      "\n",
      "Chunk 1:\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the \n",
      "---\n",
      "Chunk 2:\n",
      "date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
      "\n",
      "Condition Requirements: Items must be in original condition with all accessories, manuals, and pack\n",
      "---\n",
      "Chunk 3:\n",
      "aging. Opened software cannot be returned due to licensing restrictions.\n",
      "\n",
      "Refund Process: Once we receive your return, we'll inspect it within 2 business days. Approved refunds are processed to your o\n",
      "---\n",
      "\n",
      "⚠️ Problems:\n",
      "  - Splits mid-sentence\n",
      "  - Separates related information\n",
      "  - No context about what comes before/after\n",
      "  - Impossible to understand chunk 3 alone\n"
     ]
    }
   ],
   "source": [
    "# Naive chunking - splits at arbitrary character positions\n",
    "def naive_chunk(text: str, chunk_size: int = 200) -> List[str]:\n",
    "    \"\"\"Split text into fixed-size chunks without considering content.\"\"\"\n",
    "    # Use list comprehension to slice the text at fixed intervals\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Apply naive chunking to the sample document\n",
    "naive_chunks = naive_chunk(sample_document, chunk_size=200)\n",
    "\n",
    "print(\"❌ Naive chunking results:\")\n",
    "print(f\"Number of chunks: {len(naive_chunks)}\\n\")\n",
    "# Iterate and print each chunk to observe the splits\n",
    "for i, chunk in enumerate(naive_chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunk)\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\n⚠️ Problems:\")\n",
    "print(\"  - Splits mid-sentence\")\n",
    "print(\"  - Separates related information\")\n",
    "print(\"  - No context about what comes before/after\")\n",
    "print(\"  - Impossible to understand chunk 3 alone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Strategy 1: Fixed-size chunking with overlap\n",
    "We use `CharacterTextSplitter` to split text while maintaining some overlap between chunks to preserve context across boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed-size chunking with overlap:\n",
      "Number of chunks: 5\n",
      "\n",
      "Chunk 1 (299 chars):\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
      "---\n",
      "Chunk 2 (171 chars):\n",
      "Condition Requirements: Items must be in original condition with all accessories, manuals, and packaging. Opened software cannot be returned due to licensing restrictions.\n",
      "---\n",
      "Chunk 3 (174 chars):\n",
      "Refund Process: Once we receive your return, we'll inspect it within 2 business days. Approved refunds are processed to your original payment method within 5-7 business days.\n",
      "---\n",
      "\n",
      "✅ Improvements:\n",
      "  - Respects paragraph boundaries\n",
      "  - Overlap provides context\n",
      "  - Each chunk is more coherent\n"
     ]
    }
   ],
   "source": [
    "# Initialize CharacterTextSplitter with specific parameters\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Split on paragraphs\n",
    "    chunk_size=300,  # Target size for each chunk in characters\n",
    "    chunk_overlap=50,  # 50 character overlap preserves context\n",
    "    length_function=len,  # Function to measure text length (standard len())\n",
    ")\n",
    "\n",
    "# Split the sample document using the configured splitter\n",
    "fixed_chunks = text_splitter.split_text(sample_document)\n",
    "\n",
    "print(\"✅ Fixed-size chunking with overlap:\")\n",
    "print(f\"Number of chunks: {len(fixed_chunks)}\\n\")\n",
    "for i, chunk in enumerate(fixed_chunks[:3]):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\n✅ Improvements:\")\n",
    "print(\"  - Respects paragraph boundaries\")\n",
    "print(\"  - Overlap provides context\")\n",
    "print(\"  - Each chunk is more coherent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Strategy 2: Recursive chunking (respects document structure)\n",
    "We use `RecursiveCharacterTextSplitter` to split text hierarchically, trying to keep related text together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recursive chunking (structure-aware):\n",
      "Number of chunks: 5\n",
      "\n",
      "Chunk 1 (299 chars):\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
      "---\n",
      "Chunk 2 (171 chars):\n",
      "Condition Requirements: Items must be in original condition with all accessories, manuals, and packaging. Opened software cannot be returned due to licensing restrictions.\n",
      "---\n",
      "Chunk 3 (174 chars):\n",
      "Refund Process: Once we receive your return, we'll inspect it within 2 business days. Approved refunds are processed to your original payment method within 5-7 business days.\n",
      "---\n",
      "\n",
      "✅ Benefits:\n",
      "  - Respects document hierarchy\n",
      "  - Falls back gracefully (paragraph -> sentence -> word)\n",
      "  - Preserves semantic units\n"
     ]
    }
   ],
   "source": [
    "# Recursive chunking - tries multiple separators in order\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    # List of separators to try in order: paragraphs, lines, sentences, words, chars\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try these in order\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the document recursively\n",
    "recursive_chunks = recursive_splitter.split_text(sample_document)\n",
    "\n",
    "print(\"✅ Recursive chunking (structure-aware):\")\n",
    "print(f\"Number of chunks: {len(recursive_chunks)}\\n\")\n",
    "for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\n✅ Benefits:\")\n",
    "print(\"  - Respects document hierarchy\")\n",
    "print(\"  - Falls back gracefully (paragraph -> sentence -> word)\")\n",
    "print(\"  - Preserves semantic units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "- **`separators`**: The splitter tries these in order. It first tries to split by paragraphs (`\\n\\n`), then lines (`\\n`), then sentences (`. `), and so on. This respects the natural structure of the document better than fixed-size splitting.\n",
    "\n",
    "### Strategy 3: Semantic chunking (content-aware)\n",
    "We define a custom function to chunk text based on semantic boundaries (keywords) rather than just length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semantic chunking (topic-based):\n",
      "Number of chunks: 6\n",
      "\n",
      "Topic 1 (95 chars):\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase.\n",
      "---\n",
      "Topic 2 (142 chars):\n",
      "Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the date of delivery to return most items.\n",
      "---\n",
      "Topic 3 (167 chars):\n",
      "Electronics have a 14-day return window due to their nature.\n",
      "\n",
      "Condition Requirements: Items must be in original condition with all accessories, manuals, and packaging.\n",
      "---\n",
      "Topic 4 (152 chars):\n",
      "Opened software cannot be returned due to licensing restrictions.\n",
      "\n",
      "Refund Process: Once we receive your return, we'll inspect it within 2 business days.\n",
      "---\n",
      "\n",
      "✅ Benefits:\n",
      "  - Each chunk covers one topic\n",
      "  - Natural semantic boundaries\n",
      "  - More relevant retrieval results\n"
     ]
    }
   ],
   "source": [
    "def semantic_chunk(text: str, embeddings_model) -> List[str]:\n",
    "    \"\"\"Chunk text based on semantic similarity between sentences.\"\"\"\n",
    "    # Split text into sentences based on '. '\n",
    "    sentences = [s.strip() for s in text.split('. ') if s.strip()]\n",
    "    \n",
    "    # For small documents, group by topic manually\n",
    "    # In production, use embeddings to detect topic shifts\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        \n",
    "        # Group by topic markers (in production, use embedding similarity)\n",
    "        # Check if the sentence contains any topic shift keywords\n",
    "        if any(keyword in sentence.lower() for keyword in ['window:', 'requirements:', 'process:', 'costs:', 'exceptions:']):\n",
    "            # If a keyword is found and we have accumulated sentences, finalize the current chunk\n",
    "            if len(current_chunk) > 1:\n",
    "                # Join sentences to form the chunk, excluding the current sentence (start of new topic)\n",
    "                chunks.append('. '.join(current_chunk[:-1]) + '.')\n",
    "                # Start the new chunk with the current sentence\n",
    "                current_chunk = [current_chunk[-1]]\n",
    "\n",
    "    # Append any remaining sentences as the final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append('. '.join(current_chunk) + '.')\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Apply semantic chunking\n",
    "semantic_chunks = semantic_chunk(sample_document, embeddings)\n",
    "\n",
    "print(\"✅ Semantic chunking (topic-based):\")\n",
    "print(f\"Number of chunks: {len(semantic_chunks)}\\n\")\n",
    "for i, chunk in enumerate(semantic_chunks[:4]):\n",
    "    print(f\"Topic {i+1} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\n✅ Benefits:\")\n",
    "print(\"  - Each chunk covers one topic\")\n",
    "print(\"  - Natural semantic boundaries\")\n",
    "print(\"  - More relevant retrieval results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "This custom function iterates through sentences and starts a new chunk whenever it detects a keyword indicating a new topic. This ensures that each chunk is semantically self-contained.\n",
    "\n",
    "### Choosing the right chunking strategy\n",
    "\n",
    "| Strategy | Best For | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| Fixed-size | Consistent chunk lengths, simple documents | Fast, predictable | Ignores structure |\n",
    "| Recursive | Structured documents (markdown, code) | Respects hierarchy | May create uneven chunks |\n",
    "| Semantic | Topic-based documents, FAQs | Best retrieval accuracy | Slower, more complex |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Part 2: Semantic indexing with metadata\n",
    "\n",
    "Metadata transforms chunks from isolated text fragments into rich, queryable information objects. Without metadata, chunks float in isolation with no context about their source, recency, category, or reliability. With well-designed metadata, agents can filter by document type, prioritize recent information, verify sources, and understand the provenance of every fact. This enables precise retrieval where only relevant, trustworthy information makes it into the agent's context.\n",
    "\n",
    "The key is choosing metadata that supports filtering and ranking without adding excessive overhead. Core metadata like source and document type enable basic filtering. Structural metadata like section and chunk position help agents understand context. Temporal metadata like creation and update timestamps support freshness-based ranking. Categorical metadata like tags and keywords enable semantic filtering. Together, these metadata fields transform simple text retrieval into intelligent information discovery.\n",
    "\n",
    "##### Chunks without metadata\n",
    "First, let's see what chunks look like without metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Chunks without metadata:\n",
      "\n",
      "Chunk 1:\n",
      "  Content: Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our ...\n",
      "  Metadata: {}\n",
      "\n",
      "Chunk 2:\n",
      "  Content: Condition Requirements: Items must be in original condition with all accessories, manuals, and packa...\n",
      "  Metadata: {}\n",
      "\n",
      "⚠️ Problems:\n",
      "  - No source attribution\n",
      "  - Can't filter by document type\n",
      "  - No timestamp for freshness\n",
      "  - Missing context about document structure\n"
     ]
    }
   ],
   "source": [
    "# Create Document objects from raw text chunks without adding metadata\n",
    "chunks_without_metadata = [\n",
    "    Document(page_content=chunk) \n",
    "    for chunk in recursive_chunks\n",
    "]\n",
    "\n",
    "print(\"❌ Chunks without metadata:\")\n",
    "for i, doc in enumerate(chunks_without_metadata[:2]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "\n",
    "print(\"\\n⚠️ Problems:\")\n",
    "print(\"  - No source attribution\")\n",
    "print(\"  - Can't filter by document type\")\n",
    "print(\"  - No timestamp for freshness\")\n",
    "print(\"  - Missing context about document structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "Without metadata, the `metadata` dictionary is empty. This limits our ability to filter or track the source of the information.\n",
    "\n",
    "### Rich metadata for better retrieval\n",
    "Now we will create a function to automatically attach rich metadata to each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunks with rich metadata:\n",
      "\n",
      "Chunk 1:\n",
      "  Content: Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our ...\n",
      "  Metadata:\n",
      "    - source: return_policy_v2.md\n",
      "    - document_type: policy\n",
      "    - section: Standard Returns\n",
      "    - chunk_index: 0\n",
      "    - total_chunks: 5\n",
      "    - created_at: 2025-11-28T18:52:52.840134\n",
      "    - last_updated: 2025-11-28T18:52:52.840143\n",
      "    - keywords: ['policy', 'returns', 'standard returns']\n",
      "    - category: customer_service\n",
      "\n",
      "Chunk 2:\n",
      "  Content: Condition Requirements: Items must be in original condition with all accessories, manuals, and packa...\n",
      "  Metadata:\n",
      "    - source: return_policy_v2.md\n",
      "    - document_type: policy\n",
      "    - section: Standard Returns\n",
      "    - chunk_index: 1\n",
      "    - total_chunks: 5\n",
      "    - created_at: 2025-11-28T18:52:52.840174\n",
      "    - last_updated: 2025-11-28T18:52:52.840176\n",
      "    - keywords: ['policy', 'returns', 'standard returns']\n",
      "    - category: customer_service\n"
     ]
    }
   ],
   "source": [
    "# With comprehensive metadata\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Metadata schema for document chunks.\"\"\"\n",
    "    source: str  # Source document\n",
    "    document_type: str  # policy, faq, product_info, etc.\n",
    "    section: str  # Section within document\n",
    "    chunk_index: int  # Position in document\n",
    "    total_chunks: int  # Total chunks in document\n",
    "    created_at: str  # When indexed\n",
    "    last_updated: str  # Last modification\n",
    "    keywords: List[str]  # Extracted keywords\n",
    "    category: str  # High-level category\n",
    "\n",
    "def create_enriched_chunks(text: str, source: str, document_type: str) -> List[Document]:\n",
    "    \"\"\"Create chunks with rich metadata.\"\"\"\n",
    "    # Use recursive splitter for better content preservation\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Heuristic to determine section based on keywords in the chunk\n",
    "        section = \"General\"\n",
    "        if \"shipping\" in chunk.lower(): section = \"Shipping Costs\"\n",
    "        elif \"refund\" in chunk.lower(): section = \"Refund Process\"\n",
    "        elif \"return\" in chunk.lower(): section = \"Standard Returns\"\n",
    "        \n",
    "        # Create metadata object with all relevant fields\n",
    "        metadata = ChunkMetadata(\n",
    "            source=source,\n",
    "            document_type=document_type,\n",
    "            section=section,\n",
    "            chunk_index=i,\n",
    "            total_chunks=len(chunks),\n",
    "            created_at=datetime.now().isoformat(),\n",
    "            last_updated=datetime.now().isoformat(),\n",
    "            keywords=[\"policy\", \"returns\", section.lower()],\n",
    "            category=\"customer_service\"\n",
    "        )\n",
    "        \n",
    "        # Create Document object with content and metadata dictionary\n",
    "        documents.append(Document(page_content=chunk, metadata=metadata.__dict__))\n",
    "    return documents\n",
    "\n",
    "# Generate enriched chunks from the sample document\n",
    "enriched_chunks = create_enriched_chunks(\n",
    "    sample_document, \n",
    "    source=\"return_policy_v2.md\",\n",
    "    document_type=\"policy\"\n",
    ")\n",
    "\n",
    "print(\"✅ Chunks with rich metadata:\")\n",
    "for i, doc in enumerate(enriched_chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"  Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"    - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "- **`section`**: We use a simple heuristic to guess the section based on keywords. In a real system, this might come from the document structure.\n",
    "- **`chunk_index`**: Helps in ordering chunks if we need to reconstruct the document.\n",
    "- **`keywords`**: Allows for keyword-based filtering in addition to semantic search.\n",
    "\n",
    "### Using metadata for filtered retrieval\n",
    "We will create a vector store and perform a filtered search. This allows us to narrow down the search space before performing similarity matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic retrieval (no metadata filtering):\n",
      "\n",
      "Result 1:\n",
      "  Section: Shipping Costs\n",
      "  Content: Shipping Costs: Return shipping is free for defective items. For other returns, a $9.99 shipping fee will be deducted from your refund unless you use ...\n",
      "\n",
      "Result 2:\n",
      "  Section: Standard Returns\n",
      "  Content: Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightf...\n",
      "\n",
      "============================================================\n",
      "Filtered retrieval (section: Shipping Costs):\n",
      "\n",
      "Result 1:\n",
      "  Section: Shipping Costs\n",
      "  Content: Shipping Costs: Return shipping is free for defective items. For other returns, a $9.99 shipping fee will be deducted from your refund unless you use ...\n",
      "\n",
      "✅ Benefits of metadata filtering:\n",
      "  - More precise results\n",
      "  - Can filter by date, type, category\n",
      "  - Reduces noise in retrieved context\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store with metadata\n",
    "vectorstore = FAISS.from_documents(enriched_chunks, embeddings)\n",
    "\n",
    "query = \"What's the shipping cost for returns?\"\n",
    "\n",
    "# Retrieval without metadata filtering\n",
    "basic_results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(\"Basic retrieval (no metadata filtering):\")\n",
    "for i, doc in enumerate(basic_results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Section: {doc.metadata['section']}\")\n",
    "    print(f\"  Content: {doc.page_content[:150]}...\")\n",
    "\n",
    "\n",
    "# Retrieval with metadata filtering\n",
    "filtered_results = vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=2,\n",
    "    filter={\"section\": \"Shipping Costs\"}  # Only search specific section\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Filtered retrieval (section: Shipping Costs):\")\n",
    "for i, doc in enumerate(filtered_results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Section: {doc.metadata['section']}\")\n",
    "    print(f\"  Content: {doc.page_content[:150]}...\")\n",
    "\n",
    "print(\"\\n✅ Benefits of metadata filtering:\")\n",
    "print(\"  - More precise results\")\n",
    "print(\"  - Can filter by date, type, category\")\n",
    "print(\"  - Reduces noise in retrieved context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "- **`FAISS.from_documents`**: Creates the vector index from our enriched chunks.\n",
    "- **`filter={\"section\": \"Shipping Costs\"}`**: This argument tells the retriever to only consider chunks where the `section` metadata field matches \"Shipping Costs\". This guarantees that the results are from the relevant section, even if other sections have similar keywords.\n",
    "\n",
    "## Part 3: Retrieval pipelines with provenance markers\n",
    "Provenance markers identify where information came from, enabling verification and building trust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Context without provenance:\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
      "\n",
      "Condition Requirements: Items must be in original condition with all accessories, manuals, and packaging. Opened software cannot be returned due to licensing restrictions.\n",
      "\n",
      "⚠️ Problems:\n",
      "  - No way to verify information\n",
      "  - Can't trace back to source\n",
      "  - User can't fact-check\n",
      "  - Difficult to debug retrieval issues\n"
     ]
    }
   ],
   "source": [
    "# Without provenance - can't verify sources\n",
    "def basic_retrieval(query: str) -> str:\n",
    "    \"\"\"Basic retrieval without provenance.\"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=2)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    return context\n",
    "\n",
    "query = \"Can I return electronics?\"\n",
    "basic_context = basic_retrieval(query)\n",
    "\n",
    "print(\"❌ Context without provenance:\")\n",
    "print(basic_context)\n",
    "print(\"\\n⚠️ Problems:\")\n",
    "print(\"  - No way to verify information\")\n",
    "print(\"  - Can't trace back to source\")\n",
    "print(\"  - User can't fact-check\")\n",
    "print(\"  - Difficult to debug retrieval issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Adding provenance markers\n",
    "We will implement retrieval with source tracking. It is crucial for the agent to know *where* information came from so it can cite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Context with provenance markers:\n",
      "[Source 1: return_policy_v2.md - Standard Returns]\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
      "\n",
      "[Source 2: return_policy_v2.md - Standard Returns]\n",
      "Condition Requirements: Items must be in original condition with all accessories, manuals, and packaging. Opened software cannot be returned due to licensing restrictions.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Source metadata:\n",
      "\n",
      "Source 1:\n",
      "  source: return_policy_v2.md\n",
      "  section: Standard Returns\n",
      "  document_type: policy\n",
      "  last_updated: 2025-11-28T18:52:52.840143\n",
      "\n",
      "Source 2:\n",
      "  source: return_policy_v2.md\n",
      "  section: Standard Returns\n",
      "  document_type: policy\n",
      "  last_updated: 2025-11-28T18:52:52.840176\n"
     ]
    }
   ],
   "source": [
    "def retrieval_with_provenance(query: str) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"Retrieves documents and formats them with source markers.\"\"\"\n",
    "    # Retrieve top 2 documents\n",
    "    results = vectorstore.similarity_search(query, k=2)\n",
    "    \n",
    "    # Build context with provenance markers\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        # Add provenance marker for the LLM (e.g., [Source 1: filename - section])\n",
    "        marker = f\"[Source {i}: {doc.metadata['source']} - {doc.metadata['section']}]\"\n",
    "        # Append the marker and content to the context list\n",
    "        context_parts.append(f\"{marker}\\n{doc.page_content}\")\n",
    "        \n",
    "        # Collect source information\n",
    "        sources.append({\n",
    "            \"id\": i,\n",
    "            \"source\": doc.metadata['source'],\n",
    "            \"section\": doc.metadata['section'],\n",
    "            \"document_type\": doc.metadata['document_type'],\n",
    "            \"last_updated\": doc.metadata['last_updated'],\n",
    "        })\n",
    "\n",
    "    # Join all context parts with double newlines\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    return context, sources\n",
    "\n",
    "# Test the function\n",
    "context_with_provenance, sources = retrieval_with_provenance(query)\n",
    "\n",
    "print(\"✅ Context with provenance markers:\")\n",
    "print(context_with_provenance)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nSource metadata:\")\n",
    "for source in sources:\n",
    "    print(f\"\\nSource {source['id']}:\")\n",
    "    for key, value in source.items():\n",
    "        if key != 'id':\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "- **`[Source i: ...]`**: We prepend this string to each chunk. The LLM sees this and can use it to reference the information.\n",
    "- **`sources` list**: We return the structured metadata separately so the application can display citations to the user.\n",
    "\n",
    "### Using provenance in agent responses\n",
    "We instruct the agent to use these markers in its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response with citations:\n",
      "Yes, you can return electronics, but please note that they have a 14-day return window from the date of delivery due to their nature. Make sure the items are in original condition with all accessories, manuals, and packaging included [1].\n",
      "\n",
      "============================================================\n",
      "\n",
      "Sources:\n",
      "[1] return_policy_v2.md - Standard Returns (updated: 2025-11-28T18:52:52.840143)\n",
      "[2] return_policy_v2.md - Standard Returns (updated: 2025-11-28T18:52:52.840176)\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template that instructs the model to use citations\n",
    "provenance_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a customer service agent. Answer questions using the provided context.\n",
    "    \n",
    "IMPORTANT: When citing information, reference the source number in brackets, e.g., [1] or [2].\n",
    "\n",
    "Context with sources:\n",
    "{context}\n",
    "\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Format the messages with the retrieved context and user question\n",
    "messages = provenance_prompt.format_messages(\n",
    "    context=context_with_provenance,\n",
    "    question=query\n",
    ")\n",
    "\n",
    "# Invoke the LLM to generate a response\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"Agent response with citations:\")\n",
    "print(response.content)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nSources:\")\n",
    "for source in sources:\n",
    "    print(f\"[{source['id']}] {source['source']} - {source['section']} (updated: {source['last_updated']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "The system prompt explicitly tells the model to use the `[1]` format. Because the context contains `[Source 1: ...]`, the model can easily map the information to the source number.\n",
    "\n",
    "## Part 4: Relevance indicators and confidence scores\n",
    "Not all retrieved chunks are equally relevant. Adding relevance scores helps the agent weight information appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Retrieval without relevance scores:\n",
      "\n",
      "Result 1:\n",
      "  Content: Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our ...\n",
      "  Relevance: Unknown\n",
      "\n",
      "Result 2:\n",
      "  Content: Condition Requirements: Items must be in original condition with all accessories, manuals, and packa...\n",
      "  Relevance: Unknown\n",
      "\n",
      "Result 3:\n",
      "  Content: Shipping Costs: Return shipping is free for defective items. For other returns, a $9.99 shipping fee...\n",
      "  Relevance: Unknown\n",
      "\n",
      "⚠️ Problems:\n",
      "  - Agent can't prioritize information\n",
      "  - No indication of match quality\n",
      "  - May use less relevant chunks equally\n"
     ]
    }
   ],
   "source": [
    "# Retrieval without relevance scores\n",
    "def retrieve_without_scores(query: str, k: int = 3) -> List[Document]:\n",
    "    \"\"\"Retrieve without confidence indicators.\"\"\"\n",
    "    return vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "results_no_scores = retrieve_without_scores(\"return policy for laptops\")\n",
    "\n",
    "print(\"❌ Retrieval without relevance scores:\")\n",
    "for i, doc in enumerate(results_no_scores):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"  Relevance: Unknown\")\n",
    "\n",
    "print(\"\\n⚠️ Problems:\")\n",
    "print(\"  - Agent can't prioritize information\")\n",
    "print(\"  - No indication of match quality\")\n",
    "print(\"  - May use less relevant chunks equally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Adding relevance scores and confidence levels\n",
    "We will add confidence scores to retrieval results. This helps us filter out irrelevant results that might hallucinate the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieval with confidence scores:\n",
      "\n",
      "Result 1:\n",
      "  Source: return_policy_v2.md - Standard Returns\n",
      "  Similarity: 0.743\n",
      "  Confidence: MEDIUM\n",
      "  Reason: Moderate semantic match\n",
      "  Content: Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our ...\n",
      "\n",
      "Result 2:\n",
      "  Source: return_policy_v2.md - Standard Returns\n",
      "  Similarity: 0.708\n",
      "  Confidence: MEDIUM\n",
      "  Reason: Moderate semantic match\n",
      "  Content: Condition Requirements: Items must be in original condition with all accessories, manuals, and packa...\n",
      "\n",
      "Result 3:\n",
      "  Source: return_policy_v2.md - Shipping Costs\n",
      "  Similarity: 0.694\n",
      "  Confidence: MEDIUM\n",
      "  Reason: Moderate semantic match\n",
      "  Content: Shipping Costs: Return shipping is free for defective items. For other returns, a $9.99 shipping fee...\n"
     ]
    }
   ],
   "source": [
    "# Define a Pydantic model for structured retrieval results\n",
    "class RetrievalResult(BaseModel):\n",
    "    \"\"\"Retrieval result with confidence scoring.\"\"\"\n",
    "    content: str\n",
    "    source: str\n",
    "    section: str\n",
    "    # Ensure similarity score is between 0 and 1\n",
    "    similarity_score: float = Field(ge=0.0, le=1.0)\n",
    "    confidence_level: str  # high, medium, low\n",
    "    relevance_reason: str\n",
    "\n",
    "def retrieve_with_scores(query: str, k: int = 3) -> List[RetrievalResult]:\n",
    "    \"\"\"Retrieve with confidence scores and relevance indicators.\"\"\"\n",
    "    # Get results with scores (lower is better) from FAISS\n",
    "    results_with_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    retrieval_results = []\n",
    "    \n",
    "    for doc, score in results_with_scores:\n",
    "        # Convert FAISS distance to similarity (lower distance = higher similarity)\n",
    "        # Normalize to 0-1 range (this is approximate)\n",
    "        similarity = 1 / (1 + score)\n",
    "        \n",
    "        # Determine confidence level\n",
    "        if similarity > 0.8:\n",
    "            confidence = \"high\"\n",
    "            reason = \"Strong semantic match\"\n",
    "        elif similarity > 0.6:\n",
    "            confidence = \"medium\"\n",
    "            reason = \"Moderate semantic match\"\n",
    "        else:\n",
    "            confidence = \"low\"\n",
    "            reason = \"Weak semantic match - verify relevance\"\n",
    "\n",
    "        # Create a structured result object\n",
    "        retrieval_results.append(RetrievalResult(\n",
    "            content=doc.page_content,\n",
    "            source=doc.metadata['source'],\n",
    "            section=doc.metadata['section'],\n",
    "            similarity_score=round(similarity, 3),\n",
    "            confidence_level=confidence,\n",
    "            relevance_reason=reason\n",
    "        ))\n",
    "    \n",
    "    return retrieval_results\n",
    "\n",
    "# Test with a query\n",
    "scored_results = retrieve_with_scores(\"return policy for laptops\", k=3)\n",
    "\n",
    "print(\"✅ Retrieval with confidence scores:\")\n",
    "for i, result in enumerate(scored_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Source: {result.source} - {result.section}\")\n",
    "    print(f\"  Similarity: {result.similarity_score:.3f}\")\n",
    "    print(f\"  Confidence: {result.confidence_level.upper()}\")\n",
    "    print(f\"  Reason: {result.relevance_reason}\")\n",
    "    print(f\"  Content: {result.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "- **`similarity_search_with_score`**: Returns the distance metric along with the document. For FAISS L2 index, this is the Euclidean distance.\n",
    "- **`1 / (1 + score)`**: Converts the unbounded distance (where 0 is identical) to a normalized 0-1 score (where 1 is identical). This makes it easier to set thresholds.\n",
    "- **Thresholds**: We arbitrarily define >0.8 as \"high\" and >0.6 as \"medium\". These would need tuning in a real application.\n",
    "\n",
    "### Using confidence scores in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context with confidence scores:\n",
      "[Source 1 - MEDIUM confidence (0.74)]\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
      "\n",
      "[Source 2 - MEDIUM confidence (0.71)]\n",
      "Condition Requirements: Items must be in original condition with all accessories, manuals, and packaging. Opened software cannot be returned due to licensing restrictions.\n",
      "\n",
      "[Source 3 - MEDIUM confidence (0.69)]\n",
      "Shipping Costs: Return shipping is free for defective items. For other returns, a $9.99 shipping fee will be deducted from your refund unless you use our prepaid return label.\n",
      "\n",
      "============================================================\n",
      "\n",
      "✅ Benefits:\n",
      "  - Agent can weight high-confidence information more\n",
      "  - Low-confidence results flagged for verification\n",
      "  - Users understand information quality\n",
      "  - Enables confidence-based filtering\n"
     ]
    }
   ],
   "source": [
    "def build_scored_context(query: str) -> str:\n",
    "    \"\"\"Build context with confidence indicators.\"\"\"\n",
    "    results = retrieve_with_scores(query, k=3)\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Add confidence indicator\n",
    "        confidence_marker = f\"[Source {i} - {result.confidence_level.upper()} confidence ({result.similarity_score:.2f})]\"\n",
    "        context_parts.append(f\"{confidence_marker}\\n{result.content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "scored_context = build_scored_context(\"return policy for laptops\")\n",
    "\n",
    "print(\"Context with confidence scores:\")\n",
    "print(scored_context)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n✅ Benefits:\")\n",
    "print(\"  - Agent can weight high-confidence information more\")\n",
    "print(\"  - Low-confidence results flagged for verification\")\n",
    "print(\"  - Users understand information quality\")\n",
    "print(\"  - Enables confidence-based filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Part 5: Source attribution and signal-to-noise ratio\n",
    "\n",
    "Maximizing signal-to-noise ratio means retrieving highly relevant information while excluding noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ High-noise retrieval (k=5, no filtering):\n",
      "Context length: 972 characters\n",
      "Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
      "...\n",
      "\n",
      "⚠️ Problems:\n",
      "  - Too much irrelevant information\n",
      "  - Agent must sort through noise\n",
      "  - Wastes tokens\n",
      "  - May confuse the agent\n"
     ]
    }
   ],
   "source": [
    "# High noise example - retrieving too much\n",
    "def noisy_retrieval(query: str) -> str:\n",
    "    \"\"\"Retrieve many results without filtering.\"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=5)  # Too many\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "noisy_context = noisy_retrieval(\"laptop return window\")\n",
    "\n",
    "print(\"❌ High-noise retrieval (k=5, no filtering):\")\n",
    "print(f\"Context length: {len(noisy_context)} characters\")\n",
    "print(noisy_context[:300] + \"...\")\n",
    "print(\"\\n⚠️ Problems:\")\n",
    "print(\"  - Too much irrelevant information\")\n",
    "print(\"  - Agent must sort through noise\")\n",
    "print(\"  - Wastes tokens\")\n",
    "print(\"  - May confuse the agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### Optimizing signal-to-noise ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optimized retrieval (confidence-filtered):\n",
      "Context length: 740 characters\n",
      "[1. Standard Returns (medium)] Product Return Policy\n",
      "\n",
      "At TechStore, we want you to be completely satisfied with your purchase. Our return policy is designed to be fair and straightforward.\n",
      "\n",
      "Return Window: You have 30 days from the date of delivery to return most items. Electronics have a 14-day return window due to their nature.\n",
      "\n",
      "[2. Standard Returns (medium)] Condition Requirements: Items must be in original condition with all accessories, manuals, and packaging. Opened software cannot be returned due to licensing restrictions.\n",
      "\n",
      "[3. Shipping Costs (medium)] Shipping Costs: Return shipping is free for defective items. For other returns, a $9.99 shipping fee will be deducted from your refund unless you use our prepaid return label.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Retrieval metadata:\n",
      "  total_retrieved: 5\n",
      "  filtered_count: 3\n",
      "  avg_confidence: 0.734000007311503\n",
      "  min_confidence_threshold: 0.7\n",
      "\n",
      "✅ Improvements:\n",
      "  - Reduced noise: 232 chars saved\n",
      "  - Only high-confidence results included\n",
      "  - Concise and focused\n",
      "  - Better token efficiency\n"
     ]
    }
   ],
   "source": [
    "def optimized_retrieval(query: str, min_confidence: float = 0.7) -> Tuple[str, Dict]:\n",
    "    \"\"\"Retrieve with confidence filtering for maximum signal-to-noise.\"\"\"\n",
    "    # Get results with scores\n",
    "    results = retrieve_with_scores(query, k=5)\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    high_quality = [r for r in results if r.similarity_score >= min_confidence]\n",
    "    \n",
    "    # If too few high-quality results, include medium confidence\n",
    "    if len(high_quality) < 2:\n",
    "        high_quality = [r for r in results if r.similarity_score >= 0.5]\n",
    "    \n",
    "    # Build concise context with only relevant info\n",
    "    context_parts = []\n",
    "    for i, result in enumerate(high_quality[:3], 1):  # Max 3 results\n",
    "        marker = f\"[{i}. {result.section} ({result.confidence_level})]\"  \n",
    "        context_parts.append(f\"{marker} {result.content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Metadata about retrieval quality\n",
    "    metadata = {\n",
    "        \"total_retrieved\": len(results),\n",
    "        \"filtered_count\": len(high_quality),\n",
    "        \"avg_confidence\": np.mean([r.similarity_score for r in high_quality]),\n",
    "        \"min_confidence_threshold\": min_confidence,\n",
    "    }\n",
    "    \n",
    "    return context, metadata\n",
    "\n",
    "optimized_context, metadata = optimized_retrieval(\"laptop return window\", min_confidence=0.7)\n",
    "\n",
    "print(\"✅ Optimized retrieval (confidence-filtered):\")\n",
    "print(f\"Context length: {len(optimized_context)} characters\")\n",
    "print(optimized_context)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nRetrieval metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n✅ Improvements:\")\n",
    "print(f\"  - Reduced noise: {len(noisy_context) - len(optimized_context)} chars saved\")\n",
    "print(\"  - Only high-confidence results included\")\n",
    "print(\"  - Concise and focused\")\n",
    "print(\"  - Better token efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## Putting it all together: Production RAG system\n",
    "We will combine all these techniques into a single `ProductionRAG` class that handles the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 5 chunks from 1 documents\n",
      "\n",
      "============================================================\n",
      "Testing Production RAG System\n",
      "============================================================\n",
      "\n",
      "\n",
      "Query: Can I return a laptop after 20 days?\n",
      "------------------------------------------------------------\n",
      "\n",
      "Answer: According to our records, you can return a laptop within 14 days of delivery due to its nature as an electronic item. Since you mentioned that it has been 20 days, unfortunately, you would not be able to return the laptop [1].\n",
      "\n",
      "Confidence: 0.727\n",
      "Sources used: 3\n",
      "\n",
      "Source details:\n",
      "  [1] return_policy_v2.md - Standard Returns (confidence: medium)\n",
      "  [2] return_policy_v2.md - Standard Returns (confidence: medium)\n",
      "  [3] return_policy_v2.md - Refund Process (confidence: medium)\n",
      "\n",
      "\n",
      "Query: What's the shipping cost for returns?\n",
      "------------------------------------------------------------\n",
      "\n",
      "Answer: Return shipping is free for defective items. For other returns, a $9.99 shipping fee will be deducted from your refund unless you use our prepaid return label [1].\n",
      "\n",
      "Confidence: 0.761\n",
      "Sources used: 3\n",
      "\n",
      "Source details:\n",
      "  [1] return_policy_v2.md - Shipping Costs (confidence: high)\n",
      "  [2] return_policy_v2.md - Standard Returns (confidence: medium)\n",
      "  [3] return_policy_v2.md - Refund Process (confidence: medium)\n",
      "\n",
      "\n",
      "Query: Are gift cards returnable?\n",
      "------------------------------------------------------------\n",
      "\n",
      "Answer: According to our records, gift cards cannot be returned. They are considered a final sale item [1].\n",
      "\n",
      "Confidence: 0.735\n",
      "Sources used: 3\n",
      "\n",
      "Source details:\n",
      "  [1] return_policy_v2.md - Standard Returns (confidence: medium)\n",
      "  [2] return_policy_v2.md - Standard Returns (confidence: medium)\n",
      "  [3] return_policy_v2.md - Standard Returns (confidence: medium)\n"
     ]
    }
   ],
   "source": [
    "class ProductionRAG:\n",
    "    \"\"\"Production-ready RAG system with all best practices.\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_model, llm_model):\n",
    "        # Initialize embeddings and LLM\n",
    "        self.embeddings = embeddings_model\n",
    "        self.llm = llm_model\n",
    "        self.vectorstore = None\n",
    "        \n",
    "    def index_documents(self, documents: List[str], sources: List[str], \n",
    "                       doc_types: List[str], chunking_strategy: str = \"recursive\"):\n",
    "        \"\"\"Index documents with chosen chunking strategy and rich metadata.\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc, source, doc_type in zip(documents, sources, doc_types):\n",
    "            # Create chunks with metadata\n",
    "            chunks = create_enriched_chunks(doc, source, doc_type)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        # Create vector store\n",
    "        self.vectorstore = FAISS.from_documents(all_chunks, self.embeddings)\n",
    "        print(f\"✅ Indexed {len(all_chunks)} chunks from {len(documents)} documents\")\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3, min_confidence: float = 0.6,\n",
    "                filters: Optional[Dict] = None) -> Dict:\n",
    "        \"\"\"Retrieve with confidence scoring, provenance, and filtering.\"\"\"\n",
    "        # Get results\n",
    "        if filters:\n",
    "            results = self.vectorstore.similarity_search_with_score(\n",
    "                query, k=k*2, filter=filters  # Get extra for filtering\n",
    "            )\n",
    "        else:\n",
    "            results = self.vectorstore.similarity_search_with_score(query, k=k*2)\n",
    "        \n",
    "        # Score and filter\n",
    "        scored_results = []\n",
    "        for doc, score in results:\n",
    "            similarity = 1 / (1 + score)\n",
    "            if similarity >= min_confidence:\n",
    "                scored_results.append({\n",
    "                    'content': doc.page_content,\n",
    "                    'metadata': doc.metadata,\n",
    "                    'similarity': round(similarity, 3),\n",
    "                    'confidence': 'high' if similarity > 0.8 else 'medium'\n",
    "                })\n",
    "        \n",
    "        # Take top k\n",
    "        scored_results = scored_results[:k]\n",
    "        \n",
    "        # Build context with provenance\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(scored_results, 1):\n",
    "            provenance = f\"[Source {i}: {result['metadata']['source']} - {result['metadata']['section']} | Confidence: {result['confidence'].upper()} ({result['similarity']})]\"  \n",
    "            context_parts.append(f\"{provenance}\\n{result['content']}\")\n",
    "        \n",
    "        return {\n",
    "            'context': \"\\n\\n\".join(context_parts),\n",
    "            'results': scored_results,\n",
    "            'num_results': len(scored_results),\n",
    "            'avg_confidence': np.mean([r['similarity'] for r in scored_results]) if scored_results else 0\n",
    "        }\n",
    "    \n",
    "    def answer_query(self, query: str, min_confidence: float = 0.6) -> Dict:\n",
    "        \"\"\"Answer query with full RAG pipeline.\"\"\"\n",
    "        # Retrieve context\n",
    "        retrieval = self.retrieve(query, k=3, min_confidence=min_confidence)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful customer service agent. Answer using the provided context.\n",
    "\n",
    "IMPORTANT:\n",
    "- Cite sources using [1], [2], etc.\n",
    "- If confidence is MEDIUM, mention \"According to our records...\"\n",
    "- If no relevant context found, say \"I don't have that information\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        \n",
    "        # Generate response\n",
    "        messages = prompt.format_messages(\n",
    "            context=retrieval['context'],\n",
    "            question=query\n",
    "        )\n",
    "        response = self.llm.invoke(messages)\n",
    "        \n",
    "        return {\n",
    "            'answer': response.content,\n",
    "            'sources': retrieval['results'],\n",
    "            'confidence': retrieval['avg_confidence'],\n",
    "            'num_sources': retrieval['num_results']\n",
    "        }\n",
    "\n",
    "# Initialize production RAG\n",
    "rag = ProductionRAG(embeddings, llm)\n",
    "\n",
    "# Index documents\n",
    "rag.index_documents(\n",
    "    documents=[sample_document],\n",
    "    sources=[\"return_policy_v2.md\"],\n",
    "    doc_types=[\"policy\"]\n",
    ")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Can I return a laptop after 20 days?\",\n",
    "    \"What's the shipping cost for returns?\",\n",
    "    \"Are gift cards returnable?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Production RAG System\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n\\nQuery: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = rag.answer_query(query)\n",
    "    \n",
    "    print(f\"\\nAnswer: {result['answer']}\")\n",
    "    print(f\"\\nConfidence: {result['confidence']:.3f}\")\n",
    "    print(f\"Sources used: {result['num_sources']}\")\n",
    "    \n",
    "    print(\"\\nSource details:\")\n",
    "    for i, source in enumerate(result['sources'], 1):\n",
    "        print(f\"  [{i}] {source['metadata']['source']} - {source['metadata']['section']} (confidence: {source['confidence']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
